{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Capstone_NLP_models.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guptapawan227/Capstone_AIML/blob/Ashish/Capstone_NLP_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XycWjq1YtXnI"
      },
      "source": [
        "Mounting Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_AZpolni4uT"
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeHuMI62tbyY"
      },
      "source": [
        "Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDYfqgy9q1p_"
      },
      "source": [
        "import pandas as pd \n",
        "import numpy as np \n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import time, os, sys, itertools, re \n",
        "from PIL import Image\n",
        "import warnings, pickle, string\n",
        "from dateutil import parser\n",
        "%matplotlib inline\n",
        "\n",
        "# Data Visualization\n",
        "import cufflinks as cf\n",
        "import plotly as py\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import download_plotlyjs,init_notebook_mode,plot,iplot\n",
        "\n",
        "from ftfy import fix_text, badness\n",
        "\n",
        "# Traditional Modeling\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Tools & Evaluation metrics\n",
        "from sklearn.metrics import confusion_matrix, classification_report, auc\n",
        "from sklearn.metrics import roc_curve, accuracy_score, precision_recall_curve\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjfS1VF6teuM"
      },
      "source": [
        "Reading the data from excel "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5aLjCsPjai0"
      },
      "source": [
        "#data=pd.read_excel('/content/drive/MyDrive/Capstone/input_data.xlsx')\n",
        "data=pd.read_excel('input_data.xlsx')\n",
        "data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvAO8EEEtmny"
      },
      "source": [
        "## Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4HmTOOQqTg7"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXfghUHQjo95"
      },
      "source": [
        "assignment_group_count=data['Assignment group'].value_counts()\n",
        "assignment_group_count.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dd91Cg37qrN3"
      },
      "source": [
        "plt.subplots(figsize=(50,10))\n",
        "ax=sns.countplot(x='Assignment group', data=data)\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=30)\n",
        "plt.tight_layout\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1aYzasoBsjB"
      },
      "source": [
        "assignment_group_count.head(50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFeL_BzlBsjB"
      },
      "source": [
        "assignment_group_count.tail(24)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eAeh9jCj5xd"
      },
      "source": [
        "Check Missing Values in dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LHjYyAk9b1a"
      },
      "source": [
        "data.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9u7hQHExVL1"
      },
      "source": [
        "data[data[\"Short description\"].isnull()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyohpEPk9icQ"
      },
      "source": [
        "### Copy Short Description to Description if the Description value is NaN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jL2xmtIN9EE3"
      },
      "source": [
        "data[data[\"Description\"].isnull()]=data[\"Short description\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ea8mfm119PZZ"
      },
      "source": [
        "data[data[\"Description\"].isnull()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imMRGWAqeJQB"
      },
      "source": [
        "data['Short description'] = data['Short description'].replace(np.nan, '', regex=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCyUJGoMz2rT"
      },
      "source": [
        "data.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coPKRzP7BsjE"
      },
      "source": [
        "## Create a rule based engine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNgvxqASBsjE"
      },
      "source": [
        "#df_rules = pd.read_csv('/content/drive/MyDrive/Capstone/Rule_matrix.csv')\n",
        "df_rules = pd.read_csv(\"Rule_matrix.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6-JgP7vBsjF"
      },
      "source": [
        "def applyRules(datadf,rulesdf,Description,ShortDescription):\n",
        "    datadf['pred_group'] = np.nan\n",
        "    for i, row in rulesdf.iterrows():\n",
        "        for j, row in datadf.iterrows():\n",
        "            if pd.notna(datadf[ShortDescription][j]):\n",
        "                if (('erp' in datadf[ShortDescription][j]) and (('EU_tool' in datadf[ShortDescription][j]))):\n",
        "                        datadf['pred_group'][j] = 'GRP_25'\n",
        "        for j, row in datadf.iterrows():\n",
        "            if pd.notna(datadf[Description][j]):\n",
        "                if (datadf[Description][j] == 'the'):\n",
        "                        datadf['pred_group'][j] = 'GRP_17' \n",
        "                \n",
        "                if (('finance_app' in datadf[ShortDescription][j]) and ('HostName_1132' not in datadf[ShortDescription][j])):\n",
        "                    datadf['pred_group'][j] = 'GRP_55'\n",
        "                \n",
        "                if (('processor' in datadf[Description][j]) and ('engg' in datadf[Description][j])):\n",
        "                    datadf['pred_group'][j] = 'GRP_58'\n",
        "        \n",
        "        if rulesdf['Short Desc Rule'][i] == 'begins with' and rulesdf['Desc Rule'][i] == 'begins with' and pd.isna(rulesdf['User'][i]):\n",
        "            for j, row in datadf.iterrows():\n",
        "                if pd.notna(datadf[ShortDescription][j]) and pd.notna(datadf[Description][j]):\n",
        "                    if ((datadf[ShortDescription][j].startswith(rulesdf['Short Dec Keyword'][i])) and (datadf[Description][j].startswith(rulesdf['Dec keyword'][i]))):\n",
        "                        datadf['pred_group'][j] = rulesdf['Group'][i]\n",
        "        if pd.isna(rulesdf['Short Desc Rule'][i]) and rulesdf['Desc Rule'][i] == 'begins with' and pd.notna(rulesdf['User'][i]):\n",
        "            for j, row in datadf.iterrows():\n",
        "                if pd.notna(datadf[Description][j]) and pd.notna(datadf['Caller'][j]):\n",
        "                    if ((datadf[Description][j].startswith(rulesdf['Desc Rule'][i]) and (rulesdf['User'][i] == datadf['Caller'][j]))):\n",
        "                        datadf['pred_group'][j] = rulesdf['Group'][i]\n",
        "        if rulesdf['Short Desc Rule'][i] == 'contains' and pd.notna(rulesdf['User'][i]):\n",
        "            for j, row in datadf.iterrows():\n",
        "                if (pd.notna(datadf[ShortDescription][j]) and pd.notna(datadf['Caller'][j])):\n",
        "                     if ((rulesdf['Short Dec Keyword'][i] in datadf[ShortDescription][j]) and (rulesdf['User'][i] == datadf['Caller'][j])):\n",
        "                        datadf['pred_group'][j] = rulesdf['Group'][i]\n",
        "        if rulesdf['Short Desc Rule'][i] == 'contains' and pd.isna(rulesdf['Desc Rule'][i]) and pd.isna(rulesdf['User'][i]):\n",
        "            for j, row in datadf.iterrows():\n",
        "                #print(j)\n",
        "                if pd.notna(datadf[ShortDescription][j]):\n",
        "                    if (rulesdf['Short Dec Keyword'][i] in datadf[ShortDescription][j]):\n",
        "                        datadf['pred_group'][j] = rulesdf['Group'][i]\n",
        "        if pd.isna(rulesdf['Short Desc Rule'][i]) and rulesdf['Desc Rule'][i] == 'begins with' and pd.isna(rulesdf['User'][i]):\n",
        "            for j, row in datadf.iterrows():\n",
        "                if pd.notna(datadf[Description][j]):\n",
        "                    if (datadf[Description][j].startswith(rulesdf['Dec keyword'][i])):\n",
        "                        datadf['pred_group'][j] = rulesdf['Group'][i]\n",
        "        if pd.isna(rulesdf['Short Desc Rule'][i]) and rulesdf['Desc Rule'][i] == 'contains' and pd.isna(rulesdf['User'][i]):\n",
        "            for j, row in datadf.iterrows():\n",
        "                if pd.notna(datadf[Description][j]):\n",
        "                    if (rulesdf['Dec keyword'][i] in datadf[Description][j]):\n",
        "                        datadf['pred_group'][j] = rulesdf['Group'][i]\n",
        "       \n",
        "\n",
        "    return datadf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bb6Q0RQGBsjG"
      },
      "source": [
        "rules_applied_df = applyRules(data,df_rules,'Description','Short description')\n",
        "rules_applied_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9NJjimoBsjH"
      },
      "source": [
        "rules_applied_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlLZAXimBsjH"
      },
      "source": [
        "rules_applied_df = rules_applied_df[(rules_applied_df['pred_group'].isna())]\n",
        "rules_applied_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAka7ylMBsjH"
      },
      "source": [
        "assignment_group_count=rules_applied_df['Assignment group'].value_counts()\n",
        "assignment_group_count.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jsmb0Q7u90k2"
      },
      "source": [
        "### Concatenate Short Description and Description Column into New Description, drop the previous columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEfP-wfh-0Nb"
      },
      "source": [
        "#Concatenate Short Description and Description columns\n",
        "rules_applied_df['New Description'] = rules_applied_df['Description'] + ' ' +rules_applied_df['Short description']\n",
        "\n",
        "clean_data=rules_applied_df.drop(['Short description', 'Description', 'pred_group'], axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVB_Dzw7bz-S"
      },
      "source": [
        "clean_data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWWXT5EnBsjI"
      },
      "source": [
        "## Fixing Garbled Text/ Mojibake using ftfy library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9bghLidBsjJ"
      },
      "source": [
        "# Write a function to apply to the dataset to detect Mojibakes\n",
        "def is_mojibake_impacted(text):\n",
        "    if not badness.sequence_weirdness(text):\n",
        "        # nothing weird, should be okay\n",
        "        return True\n",
        "    try:\n",
        "        text.encode('sloppy-windows-1252')\n",
        "    except UnicodeEncodeError:\n",
        "        # Not CP-1252 encodable, probably fine\n",
        "        return True\n",
        "    else:\n",
        "        # Encodable as CP-1252, Mojibake alert level high\n",
        "        return False\n",
        "# Check the dataset for mojibake impact\n",
        "clean_data[~clean_data.iloc[:,:].applymap(is_mojibake_impacted).all(1)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEYRzxjDBsjJ"
      },
      "source": [
        "# Take an example of row# 8471 Short Desc and fix it\n",
        "print('Grabled text: \\033[1m%s\\033[0m\\nFixed text: \\033[1m%s\\033[0m' % (clean_data['New Description'][8471], \n",
        "                                                                        fix_text(clean_data['New Description'][8471])))\n",
        "\n",
        "# List all mojibakes defined in ftfy library\n",
        "print('\\nMojibake Symbol RegEx:\\n', badness.MOJIBAKE_SYMBOL_RE.pattern)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxVgsJGRBsjJ"
      },
      "source": [
        "# Sanitize the dataset from Mojibakes\n",
        "clean_data['New Description'] = clean_data['New Description'].apply(fix_text)\n",
        "\n",
        "# Visualize that row# 8471\n",
        "clean_data.loc[8471]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4INDyU8-_RoR"
      },
      "source": [
        "## Cleaning & Processing the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXY-QspE_9e5"
      },
      "source": [
        "def date_validity(date_str):\n",
        "    try:\n",
        "        parser.parse(date_str)\n",
        "        return True\n",
        "    except:\n",
        "        return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zOXTxcwZ3b2"
      },
      "source": [
        "\n",
        "def process(text_string):\n",
        "    text=text_string.lower()\n",
        "    text_string = ' '.join([w for w in text_string.split() if not date_validity(w)])\n",
        "    text_string = re.sub(r\"received from:\",'',text_string)\n",
        "    text_string = re.sub(r\"from:\",' ',text_string)\n",
        "    text_string = re.sub(r\"to:\",' ',text_string)\n",
        "    text_string = re.sub(r\"subject:\",' ',text_string)\n",
        "    text_string = re.sub(r\"sent:\",' ',text_string)\n",
        "    text_string = re.sub(r\"ic:\",' ',text_string)\n",
        "    text_string = re.sub(r\"cc:\",' ',text_string)\n",
        "    text_string = re.sub(r\"bcc:\",' ',text_string)\n",
        "    text_string = re.sub(r'\\S*@\\S*\\s?', '', text_string)\n",
        "    text_string = re.sub(r'\\d+','' ,text_string)\n",
        "    text_string = re.sub(r'\\n',' ',text_string)\n",
        "    text_string = re.sub(r'#','', text_string)\n",
        "    text_string = re.sub(r'&;?', 'and',text_string)\n",
        "    text_string = re.sub(r'\\&\\w*;', '', text_string)\n",
        "    text_string = re.sub(r'https?:\\/\\/.*\\/\\w*', '', text_string)  \n",
        "    text_string= ''.join(c for c in text_string if c <= '\\uFFFF') \n",
        "    text_string = text_string.strip()\n",
        "    text_string = ' '.join(re.sub(\"[^\\u0030-\\u0039\\u0041-\\u005a\\u0061-\\u007a]\", \" \", text_string).split())\n",
        "    text_string = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', text_string)\n",
        "    text_string = re.sub(' +', ' ', text_string)\n",
        "    text_string = text_string.strip()\n",
        "    return text_string\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UBm9uIvPr8f"
      },
      "source": [
        "clean_data[\"Clean_Description\"] = clean_data[\"New Description\"].apply(process)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CsFdd6BS1pw"
      },
      "source": [
        "clean_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBwzB1oFYRiQ"
      },
      "source": [
        "## Language Translation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4S-ah1_UfHjC"
      },
      "source": [
        "!pip install langdetect"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJBjoH3Taoah"
      },
      "source": [
        "from langdetect import detect\n",
        "    \n",
        "def fn_lang_detect(df):                                        \n",
        "   try:                                                          \n",
        "      return detect(df)                                      \n",
        "   except:                                                       \n",
        "      return 'no'                                                  \n",
        "\n",
        "clean_data['language'] = clean_data['Clean_Description'].apply(fn_lang_detect)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJo72j1taoah"
      },
      "source": [
        "x = clean_data[\"language\"].value_counts()\n",
        "x=x.sort_index()\n",
        "plt.figure(figsize=(10,6))\n",
        "ax= sns.barplot(x.index, x.values, alpha=0.8)\n",
        "plt.title(\"Distribution of text by language\")\n",
        "plt.ylabel('number of records')\n",
        "plt.xlabel('Language')\n",
        "rects = ax.patches\n",
        "labels = x.values\n",
        "for rect, label in zip(rects, labels):\n",
        "    height = rect.get_height()\n",
        "    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cn2MDmaKaoah"
      },
      "source": [
        "We can see that most of the tickets are in english, followed by tickets in German language. We need to translate these into english."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cG2YtyrtYS63"
      },
      "source": [
        "#german_data = pd.read_csv(\"/content/drive/MyDrive/Capstone/german.csv\")\n",
        "german_data = pd.read_csv('german.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWGhyshhZSZW"
      },
      "source": [
        "german_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fr8nlGTyYkJD"
      },
      "source": [
        "german_dictionary = german_data.to_dict(orient='records')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CD7tHLwMNcMZ"
      },
      "source": [
        "\n",
        "def translate_function(text):\n",
        "    translated_text = []\n",
        "    text_split = text.split()\n",
        "    for text in text_split:\n",
        "        word_found = False\n",
        "        for item in range(len(german_dictionary)):\n",
        "            if text == german_dictionary[item][\"German\"]:\n",
        "                translated_text.append(german_dictionary[item][\"English\"])\n",
        "                word_found = True\n",
        "        if word_found == False:\n",
        "            translated_text.append(text)\n",
        "    translate = ' '.join([word for word in translated_text])        \n",
        "    return translate\n",
        "\n",
        "clean_data[\"Translated Text\"] = clean_data[\"Clean_Description\"].apply(translate_function)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIOtS7WzqL7S"
      },
      "source": [
        "clean_data.tail(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkkDCx8kBsjO"
      },
      "source": [
        "clean_data[clean_data.language == 'de']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5W1Mf97BsjO"
      },
      "source": [
        "## Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_qdWy1yBsjO"
      },
      "source": [
        "!pip3 install nltk\n",
        "import nltk \n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import wordnet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gt5RRxDkBsjO"
      },
      "source": [
        "from collections import OrderedDict\n",
        "from nltk.tokenize import word_tokenize\n",
        "def find_synonyms(word):\n",
        "  synonyms = []\n",
        "  for synset in wordnet.synsets(word):\n",
        "    for syn in synset.lemma_names():\n",
        "      synonyms.append(syn)\n",
        "\n",
        "  # using this to drop duplicates while maintaining word order (closest synonyms comes first)\n",
        "  synonyms_without_duplicates = list(OrderedDict.fromkeys(synonyms))\n",
        "  return synonyms_without_duplicates"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkwvCo2YBsjP"
      },
      "source": [
        "def create_set_of_new_sentences(sentence, max_syn_per_word = 3):\n",
        "  new_sentences = []\n",
        "  for word in word_tokenize(sentence):\n",
        "    if len(word)<=3 : continue \n",
        "    for synonym in find_synonyms(word)[0:max_syn_per_word]:\n",
        "      synonym = synonym.replace('_', ' ') #restore space character\n",
        "      new_sentence = sentence.replace(word,synonym)\n",
        "      new_sentences.append(new_sentence)\n",
        "  return new_sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpdCaFhhBsjP"
      },
      "source": [
        "med_records=['GRP_8','GRP_3','GRP_12','GRP_2','GRP_13','GRP_19']\n",
        "\n",
        "low_records=['GRP_24','GRP_9','GRP_6','GRP_10','GRP_5','GRP_14','GRP_25','GRP_33','GRP_4','GRP_29','GRP_18','GRP_16','GRP_17','GRP_31','GRP_7','GRP_34','GRP_26','GRP_40','GRP_28','GRP_41'\n",
        ",'GRP_15','GRP_30','GRP_42','GRP_20','GRP_45','GRP_22','GRP_1','GRP_11']\n",
        "\n",
        "vlow_records =['GRP_21','GRP_47','GRP_23','GRP_62','GRP_48','GRP_60','GRP_39','GRP_27','GRP_37','GRP_44','GRP_36','GRP_50','GRP_53','GRP_65','GRP_53','GRP_52','GRP_55','GRP_51','GRP_59','GRP_49','GRP_46','GRP_43','GRP_66','GRP_32','GRP_63','GRP_58','GRP_56','GRP_38','GRP_68','GRP_69','GRP_57','GRP_72','GRP_71','GRP_54','GRP_35','GRP_64','GRP_70','GRP_61','GRP_67','GRP_73']\n",
        "\n",
        "clean_data1 = clean_data[clean_data[\"Assignment group\"].isin(med_records)]\n",
        "clean_data2 = clean_data[clean_data[\"Assignment group\"].isin(low_records)]\n",
        "clean_data3 = clean_data[clean_data[\"Assignment group\"] .isin(vlow_records)]\n",
        "\n",
        "clean_data4 = clean_data[clean_data[\"Assignment group\"] == 'GRP_0']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bhou5LYdBsjP"
      },
      "source": [
        "clean_data1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTPAApjrBsjP"
      },
      "source": [
        "clean_data2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPFm5iEjBsjQ"
      },
      "source": [
        "clean_data3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLjiFF9UBsjQ"
      },
      "source": [
        "clean_data4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEKEZ-wgBsjQ"
      },
      "source": [
        "maxsyn=1\n",
        "#clean_data1[\"Augmented_data\"] = clean_data1[\"Translated Text\"].apply(create_set_of_new_sentences)\n",
        "clean_data1[\"Augmented_data\"] = clean_data1.apply(lambda x: create_set_of_new_sentences(x['Translated Text'], maxsyn),axis=1)\n",
        "\n",
        "clean_data1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Sw-WgxIBsjQ"
      },
      "source": [
        "s = clean_data1.apply(lambda x: pd.Series(x['Augmented_data']), axis=1).stack().reset_index(level=1, drop=True)\n",
        "s.name = 'Final_Text'\n",
        "clean_data_aug1 = clean_data1.drop(['New Description','Augmented_data', 'Clean_Description', 'Translated Text'],axis=1).join(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dr94fyDFBsjQ"
      },
      "source": [
        "init_notebook_mode()\n",
        "cf.go_offline()\n",
        "\n",
        "# Assignment group distribution\n",
        "print('\\033[1mTotal assignment groups:\\033[0m', clean_data_aug1['Assignment group'].nunique())\n",
        "\n",
        "# Histogram\n",
        "clean_data_aug1['Assignment group'].iplot(\n",
        "    kind='hist',\n",
        "    xTitle='Assignment Group',\n",
        "    yTitle='count',\n",
        "    title='Assignment Group Distribution- Histogram (Fig-1)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieJJ8qh9BsjR"
      },
      "source": [
        "maxsyn=6\n",
        "clean_data2[\"Augmented_data\"] = clean_data2.apply(lambda x: create_set_of_new_sentences(x['Translated Text'], maxsyn),axis=1)\n",
        "\n",
        "clean_data2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsv_Z2M7BsjR"
      },
      "source": [
        "s = clean_data2.apply(lambda x: pd.Series(x['Augmented_data']), axis=1).stack().reset_index(level=1, drop=True)\n",
        "s.name = 'Final_Text'\n",
        "clean_data_aug2 = clean_data2.drop(['New Description','Augmented_data', 'Clean_Description', 'Translated Text'],axis=1).join(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnmS6sRVBsjR"
      },
      "source": [
        "# Assignment group distribution\n",
        "print('\\033[1mTotal assignment groups:\\033[0m', clean_data_aug2['Assignment group'].nunique())\n",
        "\n",
        "# Histogram\n",
        "clean_data_aug2['Assignment group'].iplot(\n",
        "    kind='hist',\n",
        "    xTitle='Assignment Group',\n",
        "    yTitle='count',\n",
        "    title='Assignment Group Distribution- Histogram (Fig-2)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOjhKDMhBsjR"
      },
      "source": [
        "maxsyn=10\n",
        "clean_data3[\"Augmented_data\"] = clean_data3.apply(lambda x: create_set_of_new_sentences(x['Translated Text'], maxsyn),axis=1)\n",
        "\n",
        "clean_data3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHadfd3vBsjS"
      },
      "source": [
        "s = clean_data3.apply(lambda x: pd.Series(x['Augmented_data']), axis=1).stack().reset_index(level=1, drop=True)\n",
        "s.name = 'Final_Text'\n",
        "clean_data_aug3 = clean_data3.drop(['New Description','Augmented_data', 'Clean_Description', 'Translated Text'],axis=1).join(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "fP3-sKnlBsjS"
      },
      "source": [
        "# Assignment group distribution\n",
        "print('\\033[1mTotal assignment groups:\\033[0m', clean_data_aug3['Assignment group'].nunique())\n",
        "\n",
        "# Histogram\n",
        "clean_data_aug3['Assignment group'].iplot(\n",
        "    kind='hist',\n",
        "    xTitle='Assignment Group',\n",
        "    yTitle='count',\n",
        "    title='Assignment Group Distribution- Histogram (Fig-4)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STc8Fk5DBsjS"
      },
      "source": [
        "maxsyn=1\n",
        "clean_data4[\"Augmented_data\"] = clean_data4.apply(lambda x: create_set_of_new_sentences(x['Translated Text'], maxsyn),axis=1)\n",
        "\n",
        "clean_data4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_jw8J77BsjS"
      },
      "source": [
        "s = clean_data4.apply(lambda x: pd.Series(x['Augmented_data']), axis=1).stack().reset_index(level=1, drop=True)\n",
        "s.name = 'Final_Text'\n",
        "clean_data_aug4 = clean_data4.drop(['New Description','Augmented_data', 'Clean_Description', 'Translated Text'],axis=1).join(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nDaLJoFBsjS"
      },
      "source": [
        "# Assignment group distribution\n",
        "print('\\033[1mTotal assignment groups:\\033[0m', clean_data_aug4['Assignment group'].nunique())\n",
        "\n",
        "# Histogram\n",
        "clean_data_aug4['Assignment group'].iplot(\n",
        "    kind='hist',\n",
        "    xTitle='Assignment Group',\n",
        "    yTitle='count',\n",
        "    title='Assignment Group Distribution- Histogram (Fig-5)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EScjYfrBsjT"
      },
      "source": [
        "clean_data_mod4 = clean_data4.drop(['New Description', 'Clean_Description'],axis=1)\n",
        "clean_data_mod4.rename(columns={'Translated Text': 'Final_Text'}, inplace=True)\n",
        "clean_data_mod4.head()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcQ3e0XSBsjT"
      },
      "source": [
        "dataframes=[clean_data_aug1,clean_data_aug2,clean_data_aug3,clean_data_aug4]\n",
        "clean_data_result= pd.concat(dataframes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVwHlgzVBsjT"
      },
      "source": [
        "# Assignment group distribution\n",
        "print('\\033[1mTotal assignment groups:\\033[0m', clean_data_result['Assignment group'].nunique())\n",
        "\n",
        "# Histogram\n",
        "clean_data_result['Assignment group'].iplot(\n",
        "    kind='hist',\n",
        "    xTitle='Assignment Group',\n",
        "    yTitle='count',\n",
        "    title='Assignment Group Distribution- Histogram (Fig-5)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzxDdWyMBsjT"
      },
      "source": [
        "## Stop words removal and Lemmatise text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45rNIDGvBsjU"
      },
      "source": [
        "#Stop words removal\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "sr = stopwords.words('english')\n",
        "for i,text in enumerate(clean_data_result['Final_Text']):\n",
        "    clean_data_result['Final_Text'][i]=\" \".join(word for word in text.split(' ') if word not in sr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xn4waA6MBsjU"
      },
      "source": [
        "clean_data_result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "EWJN-ekLBsjU"
      },
      "source": [
        "#Lemmatisation using spacy library\n",
        "!pip install spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeoMfBEBBsjU"
      },
      "source": [
        "!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAGj1FplBsjV"
      },
      "source": [
        "# Need to run \"python -m spacy download en\" in anaconda prompt to avoid 'en' not found issue."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Pxl8SyyBsjV"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
        "allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']\n",
        "def lemmatize_text(text):\n",
        "    doc = nlp(text)\n",
        "    return ' '.join([token.lemma_ for token in doc])\n",
        "\n",
        "clean_data_result['Final_Text'] = clean_data_result['Final_Text'].apply(lemmatize_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0YLyD-iBsjV"
      },
      "source": [
        "clean_data_result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hCn99wvBsjV"
      },
      "source": [
        "### Attempt to use Google Translate library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cg4GJAaFaoah"
      },
      "source": [
        "#!pip install goslate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kb0GSBoSaoah"
      },
      "source": [
        "'''# Define and construct the service urls\n",
        "domains = ['.com','.com.au','.com.ar','.co.kr','.co.in','.co.jp','.at','.de','.ru','.ch','.fr','.es','.ae']\n",
        "urls = ['http://translate.google' + domain for domain in domains]'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24dnr7cPaoah"
      },
      "source": [
        "\"\"\"from goslate import Goslate # Provided by Google\n",
        "import random\n",
        "\n",
        "# List of column data to consider for translation\n",
        "trans_cols = ['Clean_Description']\n",
        "\n",
        "for idx in range(clean_data.shape[0]):\n",
        "    # Instantiate Goslate class in each iteration\n",
        "    gs = Goslate(service_urls=random.choice(urls))\n",
        "    row_iter = gs.translate(clean_data.loc[idx, trans_cols].tolist(), \n",
        "                            target_language='en', \n",
        "                            source_language='auto')\n",
        "    clean_data.loc[idx, trans_cols] = list(row_iter)\n",
        "    time.sleep(30)\n",
        "    \n",
        "clean_data.tail()\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yJf4QCZaoah"
      },
      "source": [
        "# Serialize the translated dataset\n",
        "clean_data_result.to_csv('Final_data.csv', index=False, encoding='utf_8_sig')\n",
        "with open('Final_data.pkl','wb') as f:\n",
        "    pickle.dump(clean_data_result, f, pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-zXZRYDaoah"
      },
      "source": [
        "# Load the translated pickle file \n",
        "with open('final_data.pkl','rb') as f:\n",
        "    clean_data = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MufrYo4zaoah"
      },
      "source": [
        "### Univariate visualization\n",
        "Single-variable or univariate visualization is the simplest type of visualization which consists of observations on only a single characteristic or attribute. Univariate visualization includes histogram, bar plots and line charts.\n",
        "\n",
        "#### The distribution of Assignment groups\n",
        "Plots how the assignments groups are scattered across the dataset. The bar chart, histogram and pie chart tells the frequency of any ticket assigned to any group OR the tickets count for each group."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEzuZQTdaoah"
      },
      "source": [
        "# Assignment group distribution\n",
        "print('\\033[1mTotal assignment groups:\\033[0m', clean_data['Assignment group'].nunique())\n",
        "\n",
        "# Histogram\n",
        "clean_data['Assignment group'].iplot(\n",
        "    kind='hist',\n",
        "    xTitle='Assignment Group',\n",
        "    yTitle='count',\n",
        "    title='Assignment Group Distribution- Histogram (Fig-1)')\n",
        "\n",
        "# Pie chart\n",
        "assgn_grp = pd.DataFrame(clean_data.groupby('Assignment group').size(),columns = ['Count']).reset_index()\n",
        "assgn_grp.iplot(\n",
        "    kind='pie', \n",
        "    labels='Assignment group', \n",
        "    values='Count', \n",
        "    title='Assignment Group Distribution- Pie Chart (Fig-2)', \n",
        "    hoverinfo=\"label+percent+name\", hole=0.25)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Hn2Iwq_aoah"
      },
      "source": [
        "### Lets visualize the percentage of incidents per assignment group"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fw4H53FGaoah"
      },
      "source": [
        "# Plot to visualize the percentage data distribution across different groups\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.figure(figsize=(20,5))\n",
        "ax = sns.countplot(x=\"Assignment group\", data=clean_data, order=clean_data[\"Assignment group\"].value_counts().index)\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
        "for p in ax.patches:\n",
        "  ax.annotate(str(format(p.get_height()/len(clean_data.index)*100, '.2f')+\"%\"), (p.get_x() + p.get_width() / 2., p.get_height()), ha = 'center', va = 'bottom', rotation=90, xytext = (0, 10), textcoords = 'offset points')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HRA85rBaoah"
      },
      "source": [
        "top_20 = clean_data['Assignment group'].value_counts().nlargest(20).reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mk193_BAaoah"
      },
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "bars = plt.bar(top_20['index'],top_20['Assignment group'])\n",
        "plt.title('Top 20 Assignment groups with highest number of Tickets')\n",
        "plt.xlabel('Assignment Group')\n",
        "plt.xticks(rotation=90)\n",
        "plt.ylabel('Number of Tickets')\n",
        "\n",
        "for bar in bars:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x(), yval + .005, yval)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oL1odSqxaoah"
      },
      "source": [
        "bottom_20 = clean_data['Assignment group'].value_counts().nsmallest(20).reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKTbEME0aoah"
      },
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "bars = plt.bar(bottom_20['index'],bottom_20['Assignment group'])\n",
        "plt.title('Bottom 20 Assignment groups with small number of Tickets')\n",
        "plt.xlabel('Assignment Group')\n",
        "plt.xticks(rotation=90)\n",
        "plt.ylabel('Number of Tickets')\n",
        "for bar in bars:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x(), yval + .005, yval)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTEdYvB4aoah"
      },
      "source": [
        "#### The distribution of Callers\n",
        "Plots how the callers are associated with tickets and what are the assignment groups they most frequently raise tickets for."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DF-i2z1aoah"
      },
      "source": [
        "# Find out top 10 callers in terms of frequency of raising tickets in the entire dataset\n",
        "print('\\033[1mTotal caller count:\\033[0m', clean_data['Caller'].nunique())\n",
        "df = pd.DataFrame(clean_data.groupby(['Caller']).size().nlargest(10), columns=['Count']).reset_index()\n",
        "df.iplot(kind='pie',\n",
        "         labels='Caller', \n",
        "         values='Count', \n",
        "         title='Top 10 caller- Pie Chart (Fig-7)',\n",
        "         colorscale='-spectral',\n",
        "         pull=[0,0,0,0,0.05,0.1,0.15,0.2,0.25,0.3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTiANuJnaoah"
      },
      "source": [
        "# Top 5 callers in each assignment group\n",
        "top_n = 5\n",
        "s = clean_data['Caller'].groupby(clean_data['Assignment group']).value_counts()\n",
        "caller_grp = pd.DataFrame(s.groupby(level=0).nlargest(top_n).reset_index(level=0, drop=True))\n",
        "caller_grp.head(15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbHn-V3Maoah"
      },
      "source": [
        "#### The distribution of description lengths\n",
        "Plots the variation of length and word count of new description attribute"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdrJgg48aoai"
      },
      "source": [
        "clean_data.insert(1, 'desc_len', clean_data['Final_Text'].astype(str).apply(len))\n",
        "clean_data.insert(5, 'desc_word_count', clean_data['Final_Text'].apply(lambda x: len(str(x).split())))\n",
        "clean_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38p2Thiaaoai"
      },
      "source": [
        "# Description text length\n",
        "clean_data['desc_len'].iplot(\n",
        "    kind='bar',\n",
        "    xTitle='text length',\n",
        "    yTitle='count',\n",
        "    colorscale='-ylgn',\n",
        "    title='Description Text Length Distribution (Fig-11)')\n",
        "\n",
        "# Description word count\n",
        "clean_data['desc_word_count'].iplot(\n",
        "    kind='bar',\n",
        "    xTitle='word count',\n",
        "    linecolor='black',\n",
        "    yTitle='count',\n",
        "    colorscale='-bupu',\n",
        "    title='Description Word Count Distribution (Fig-12)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qTz5tP8aoai"
      },
      "source": [
        "### N-Grams\n",
        "N-gram is a contiguous sequence of N items from a given sample of text or speech, in the fields of computational linguistics and probability. The items can be phonemes, syllables, letters, words or base pairs according to the application. N-grams are used to describe the number of words used as observation points, e.g., unigram means singly-worded, bigram means 2-worded phrase, and trigram means 3-worded phrase. \n",
        "\n",
        "We'll be using scikit-learn’s CountVectorizer function to derive n-grams and compare them before and after removing stop words. Stop words are a set of commonly used words in any language. We'll be using english corpus stopwords and extend it to include some business specific common words considered to be stop words in our case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2c0ivkwEaoai"
      },
      "source": [
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Extend the English Stop Wordss\n",
        "STOP_WORDS = STOPWORDS.union({'yes','na','hi',\n",
        "                              'receive','hello',\n",
        "                              'regards','thanks',\n",
        "                              'from','greeting',\n",
        "                              'forward','reply',\n",
        "                              'will','please',\n",
        "                              'see','help','able'})\n",
        "\n",
        "# Generic function to derive top N n-grams from the corpus\n",
        "def get_top_n_ngrams(corpus, top_n=None, ngram_range=(1,1), stopwords=None):\n",
        "    vec = CountVectorizer(ngram_range=ngram_range, \n",
        "                          stop_words=stopwords).fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0) \n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "    return words_freq[:top_n]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BaGo4WSaoai"
      },
      "source": [
        "### Top Unigrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pejZdQiTaoai"
      },
      "source": [
        "# Top 50 Unigrams before removing stop words\n",
        "top_n = 50\n",
        "ngram_range = (1,1)\n",
        "uni_grams = get_top_n_ngrams(clean_data.Final_Text, top_n, ngram_range)\n",
        "\n",
        "df = pd.DataFrame(uni_grams, columns = ['Final_Text' , 'count'])\n",
        "df.groupby('Final_Text').sum()['count'].sort_values(ascending=False).iplot(\n",
        "    kind='bar', \n",
        "    yTitle='Count', \n",
        "    linecolor='black', \n",
        "    colorscale='piyg',\n",
        "    title=f'Top {top_n} Unigrams in Final_Text')\n",
        "\n",
        "# Top 50 Unigrams after removing stop words\n",
        "uni_grams_sw = get_top_n_ngrams(clean_data.Final_Text, top_n, ngram_range, stopwords=STOP_WORDS)\n",
        "\n",
        "df = pd.DataFrame(uni_grams_sw, columns = ['Final_Text' , 'count'])\n",
        "df.groupby('Final_Text').sum()['count'].sort_values(ascending=False).iplot(\n",
        "    kind='bar', \n",
        "    yTitle='Count', \n",
        "    linecolor='black',\n",
        "    colorscale='-piyg',\n",
        "    title=f'Top {top_n} Unigrams in Final_Text without stop words')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jb8iWkH-aoai"
      },
      "source": [
        "### Top Bigrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Qrr4qJNaoai"
      },
      "source": [
        "# Top 50 Bigrams before removing stop words\n",
        "top_n = 50\n",
        "ngram_range = (2,2)\n",
        "bi_grams = get_top_n_ngrams(clean_data.Final_Text, top_n, ngram_range)\n",
        "\n",
        "df = pd.DataFrame(bi_grams, columns = ['Final_Text' , 'count'])\n",
        "df.groupby('Final_Text').sum()['count'].sort_values(ascending=False).iplot(\n",
        "    kind='bar', \n",
        "    yTitle='Count', \n",
        "    linecolor='black', \n",
        "    colorscale='piyg',\n",
        "    title=f'Top {top_n} Bigrams in Final_Text')\n",
        "\n",
        "# Top 50 Bigrams after removing stop words\n",
        "bi_grams_sw = get_top_n_ngrams(clean_data.Final_Text, top_n, ngram_range, stopwords=STOP_WORDS)\n",
        "\n",
        "df = pd.DataFrame(bi_grams_sw, columns = ['Final_Text' , 'count'])\n",
        "df.groupby('Final_Text').sum()['count'].sort_values(ascending=False).iplot(\n",
        "    kind='bar', \n",
        "    yTitle='Count', \n",
        "    linecolor='black',\n",
        "    colorscale='-piyg',\n",
        "    title=f'Top {top_n} Bigrams in Final_Text without stop words')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xc62MwL5aoai"
      },
      "source": [
        "### Top Trigrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LE3n2E_zaoai"
      },
      "source": [
        "# Top 50 Trigrams before removing stop words\n",
        "top_n = 50\n",
        "ngram_range = (3,3)\n",
        "tri_grams = get_top_n_ngrams(clean_data.Final_Text, top_n, ngram_range)\n",
        "\n",
        "df = pd.DataFrame(tri_grams, columns = ['Final_Text' , 'count'])\n",
        "df.groupby('Final_Text').sum()['count'].sort_values(ascending=False).iplot(\n",
        "    kind='bar', \n",
        "    yTitle='Count', \n",
        "    linecolor='black', \n",
        "    colorscale='piyg',\n",
        "    title=f'Top {top_n} Trigrams in Final_Text')\n",
        "\n",
        "# Top 50 Trigrams after removing stop words\n",
        "tri_grams_sw = get_top_n_ngrams(clean_data.Final_Text, top_n, ngram_range, stopwords=STOP_WORDS)\n",
        "\n",
        "df = pd.DataFrame(tri_grams_sw, columns = ['Final_Text' , 'count'])\n",
        "df.groupby('Final_Text').sum()['count'].sort_values(ascending=False).iplot(\n",
        "    kind='bar', \n",
        "    yTitle='Count', \n",
        "    linecolor='black',\n",
        "    colorscale='-piyg',\n",
        "    title=f'Top {top_n} Trigrams in Final_Text without stop words')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4hSbsitaoai"
      },
      "source": [
        "### Word Cloud\n",
        "Let us attempt to visualize this as a word cloud for top three groups that has got maximum records. A word cloud enables us to visualize the data as cluster of words and each words displayed in different font size based on the number of occurences of that word . Basically; the bolder and bigger the word show up in the visualization, it implies its more often it’s mentioned within a given text compared to other words in the cloud and therefore would be more important for us.\n",
        "\n",
        "Let's write a generic method to generate Word Clouds for both Short and Long Description columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKttDbCQaoai"
      },
      "source": [
        "def generate_word_cloud(corpus):\n",
        "        # Instantiate the wordcloud object\n",
        "    wordcloud = WordCloud(width = 800, height = 800, \n",
        "                    background_color ='white', \n",
        "                    stopwords=STOP_WORDS,\n",
        "                    # mask=mask,\n",
        "                    min_font_size = 10).generate(corpus)\n",
        "\n",
        "    # plot the WordCloud image                        \n",
        "    plt.figure(figsize = (12, 12), facecolor = None) \n",
        "    plt.imshow(wordcloud) \n",
        "    plt.axis(\"off\") \n",
        "    plt.tight_layout(pad = 0) \n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKpk_Grwaoai"
      },
      "source": [
        "# Word Cloud for all tickets assigned to GRP_0\n",
        "generate_word_cloud(' '.join(clean_data[clean_data['Assignment group'] == 'GRP_0'].Final_Text.str.strip()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJkOwVhbaoai"
      },
      "source": [
        "# Word Cloud for all tickets assigned to GRP_8\n",
        "generate_word_cloud(' '.join(clean_data[clean_data['Assignment group'] == 'GRP_8'].Final_Text.str.strip()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLQhXr6Yaoai"
      },
      "source": [
        "# Word Cloud for all tickets assigned to GRP_25\n",
        "generate_word_cloud(' '.join(clean_data[clean_data['Assignment group'] == 'GRP_25'].Final_Text.str.strip()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IqWo_cPaoai",
        "scrolled": true
      },
      "source": [
        "# Generate wordcloud for Final_Text field\n",
        "generate_word_cloud(' '.join(clean_data.Final_Text.str.strip()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhCUgD6_Bsjn"
      },
      "source": [
        "## Prepping Dataframe for Model Building"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FW9b_WeCBsjn"
      },
      "source": [
        "# Import label encoder \n",
        "from sklearn import preprocessing \n",
        "  \n",
        "# label_encoder object knows how to understand word labels. \n",
        "label_encoder = preprocessing.LabelEncoder() \n",
        "  \n",
        "# Encode labels in column 'species'. \n",
        "clean_data['Assignment group LabelEncoded']= label_encoder.fit_transform(clean_data['Assignment group']) \n",
        "  \n",
        "clean_data['Assignment group LabelEncoded'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jsBDjGIBsjn"
      },
      "source": [
        "label_encoded_dict = dict(zip(clean_data['Assignment group'].unique(), clean_data['Assignment group LabelEncoded'].unique()))\n",
        "len(label_encoded_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ha-VhcWRBsjn"
      },
      "source": [
        "## Feature Extraction : Bag of Words using CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3UGzor_Bsjo"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "CV = CountVectorizer()\n",
        "\n",
        "X_BoW = CV.fit_transform(clean_data['Final_Text']).toarray()\n",
        "y = clean_data['Assignment group LabelEncoded']\n",
        "\n",
        "print(\"Shape of Input Feature :\",np.shape(X_BoW))\n",
        "print(\"Shape of Target Feature :\",np.shape(y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaaV3zyqBsjo"
      },
      "source": [
        "# Splitting Train Test \n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_BoW, y, test_size=0.3, random_state = 1)\n",
        "print('\\033[1mShape of the training set:\\033[0m', X_train.shape, X_test.shape)\n",
        "print('\\033[1mShape of the test set:\\033[0m', y_train.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIni9BjpBsjo"
      },
      "source": [
        "def run_classification(estimator, X_train, X_test, y_train, y_test, arch_name=None, pipelineRequired=True, isDeepModel=False):\n",
        "    # train the model\n",
        "    clf = estimator\n",
        "\n",
        "    if pipelineRequired :\n",
        "        clf = Pipeline([('tfidf', TfidfTransformer()),\n",
        "                     ('clf', estimator),\n",
        "                     ])\n",
        "      \n",
        "    if isDeepModel :\n",
        "        clf.fit(X_train, y_train, validation_data=(X_test, y_test),epochs=10, batch_size=128,verbose=1,callbacks=call_backs(arch_name))\n",
        "        # predict from the clasiffier\n",
        "        y_pred = clf.predict(X_test)\n",
        "        y_pred = np.argmax(y_pred, axis=1)\n",
        "        y_train_pred = clf.predict(X_train)\n",
        "        y_train_pred = np.argmax(y_train_pred, axis=1)\n",
        "    else :\n",
        "        clf.fit(X_train, y_train)\n",
        "        # predict from the clasiffier\n",
        "        y_pred = clf.predict(X_test)\n",
        "        y_train_pred = clf.predict(X_train)\n",
        "    \n",
        "    print('Estimator:', clf)\n",
        "    print('='*80)\n",
        "    print('Training accuracy: %.2f%%' % (accuracy_score(y_train,y_train_pred) * 100))\n",
        "    print('Testing accuracy: %.2f%%' % (accuracy_score(y_test, y_pred) * 100))\n",
        "    print('='*80)\n",
        "    print('Confusion matrix:\\n %s' % (confusion_matrix(y_test, y_pred)))\n",
        "    print('='*80)\n",
        "    print('Classification report:\\n %s' % (classification_report(y_test, y_pred)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxlH4wqyBsjo"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_RXTWZfBsjp"
      },
      "source": [
        "run_classification(LogisticRegression(), X_train, X_test, y_train, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7r6p2SGBsjp"
      },
      "source": [
        "## Naive Bayes Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hlgbYXyBsjp"
      },
      "source": [
        "run_classification(MultinomialNB(), X_train, X_test, y_train, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gs5ztA2kBsjp"
      },
      "source": [
        "## K-nearest Neighbor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4qwMKt7Bsjq"
      },
      "source": [
        "run_classification(KNeighborsClassifier(), X_train, X_test, y_train, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDMha2lxBsjq"
      },
      "source": [
        "## Support Vector Machine (SVM)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBc7yY11Bsjq"
      },
      "source": [
        "run_classification(LinearSVC(), X_train, X_test, y_train, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDPisp5SBsjq"
      },
      "source": [
        "## Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyvv2qn5Bsjq"
      },
      "source": [
        "run_classification(DecisionTreeClassifier(), X_train, X_test, y_train, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpJIipnsBsjr"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8b28KjuoBsjr"
      },
      "source": [
        "run_classification(RandomForestClassifier(n_estimators=100), X_train, X_test, y_train, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLvdhVW0Bsjr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJr4CJMBBsjr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}