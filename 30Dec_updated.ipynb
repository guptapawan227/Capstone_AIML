{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/guptapawan227/Capstone_AIML/blob/Ashish/Recreated_16thDec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XycWjq1YtXnI"
   },
   "source": [
    "Mounting Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-_AZpolni4uT",
    "outputId": "ca21137f-71a3-4375-9c72-43ae13e443a8"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IX79JvNILSQK",
    "outputId": "c8c108ac-8cf2-440a-fdf3-c45119f969b7"
   },
   "outputs": [],
   "source": [
    "!pip3 install ftfy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IeHuMI62tbyY"
   },
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "01_3Wr6nw-ee"
   },
   "outputs": [],
   "source": [
    "# Using TensorFlow 1.x only in colab as found a issue with 2.3 version used by colab while working with DNN model fit. Did not observe any issue with Tensor flow 2.1 version on local jupyter enviornment.\n",
    "%tensorflow_version 1.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "GDYfqgy9q1p_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time, os, sys, itertools, re \n",
    "from PIL import Image\n",
    "import warnings, pickle, string\n",
    "from dateutil import parser\n",
    "%matplotlib inline\n",
    "\n",
    "# Data Visualization\n",
    "import cufflinks as cf\n",
    "import plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs,init_notebook_mode,plot,iplot\n",
    "\n",
    "from ftfy import fix_text, badness\n",
    "\n",
    "# Traditional Modeling\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Sequential Modeling\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers import Input, Dropout, Flatten, Dense, Embedding, LSTM, GRU\n",
    "from keras.layers import BatchNormalization, TimeDistributed, Conv1D, MaxPooling1D\n",
    "from keras.constraints import max_norm, unit_norm\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "# Tools & Evaluation metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report, auc\n",
    "from sklearn.metrics import roc_curve, accuracy_score, precision_recall_curve\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjfS1VF6teuM"
   },
   "source": [
    "Reading the data from excel "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zBwzB1oFYRiQ"
   },
   "source": [
    "## Language Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n795kT7hYjSo"
   },
   "source": [
    "#### Load the consolidated final translated pickle file which contains the language translations. The Process used for language translation is commented below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "    \n",
    "def fn_lang_detect(df):                                        \n",
    "   try:                                                          \n",
    "      return detect(df)                                      \n",
    "   except:                                                       \n",
    "      return 'no'                                                  \n",
    "\n",
    "clean_data['language'] = clean_data['Clean_Description'].apply(fn_lang_detect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = clean_data[\"language\"].value_counts()\n",
    "x=x.sort_index()\n",
    "plt.figure(figsize=(10,6))\n",
    "ax= sns.barplot(x.index, x.values, alpha=0.8)\n",
    "plt.title(\"Distribution of text by language\")\n",
    "plt.ylabel('number of records')\n",
    "plt.xlabel('Language')\n",
    "rects = ax.patches\n",
    "labels = x.values\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = clean_data[(clean_data[\"language\"] != 'zh-cn') & (clean_data[\"language\"] != 'de')]\n",
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the translated pickle file \n",
    "#with open('/content/drive/MyDrive/Capstone/clean_data_translated.pkl','rb') as f:\n",
    "with open('clean_data_translated.pkl','rb') as f:\n",
    "    clean_data_translated_pkl = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data_translated_pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data_translated_pkl.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data_translated_pkl=clean_data_translated_pkl.drop(['Clean_Description', 'Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data_translated_pkl.rename(columns = {'Translation_Text': 'Clean_Description'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data_translated_pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes=[data1,clean_data_translated_pkl]\n",
    "clean_data_final_2= pd.concat(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data_final_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data_final_2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data_final_2 = clean_data_final_2[(clean_data_final_2['Assignment group'] != 'GRP_32')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data_final_2.to_csv('clean_data_final_2.csv', index=False, encoding='utf_8_sig')\n",
    "#with open('/content/drive/MyDrive/Capstone/Final_Translated_combined.pkl','wb') as f:\n",
    "with open('Final_Translated_combined.pkl','wb') as f:\n",
    "    pickle.dump(clean_data_final_2, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the consolidated final translated pickle file \n",
    "#with open('/content/drive/MyDrive/Capstone/Final_Translated_combined.pkl','rb') as f:\n",
    "with open('Final_Translated_combined.pkl','rb') as f:\n",
    "    clean_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "id": "RNYuIWcuhqrR",
    "outputId": "e4c746ec-cb9d-4c31-813e-2b7d1432789f"
   },
   "outputs": [],
   "source": [
    "clean_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HpDjY6fmrFaW",
    "outputId": "c2612b7d-ef04-412d-83cf-3c051a1e2385"
   },
   "outputs": [],
   "source": [
    "assignment_group_cnt=clean_data['Assignment group'].value_counts()\n",
    "assignment_group_cnt.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignment_group_cnt.tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WTZyoprphuPp"
   },
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install NLPAug Package\n",
    "!pip install nlpaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install dependencies for nlpaug\n",
    "!pip install torch>=1.6.0 transformers>=4.0.0\n",
    "!pip install nltk>=3.4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will use Word Augmentation methods. nlpaug supports character, word and sentence level augmentation methods\n",
    "import nlpaug.augmenter.word as naw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word embedding augmentation method\n",
    "aug1 = naw.WordEmbsAug(model_type='glove', model_path='glove.6B/glove.6B.50d.txt', action=\"substitute\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug2 = naw.WordEmbsAug(model_type='glove', model_path='glove.6B/glove.6B.50d.txt', action=\"insert\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'naw' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-6e36878e1cec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0maug3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnaw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mContextualWordEmbsAug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'bert-base-uncased'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"substitute\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'naw' is not defined"
     ]
    }
   ],
   "source": [
    "#Contextual Word augmentation method\n",
    "aug3 = naw.ContextualWordEmbsAug(model_path='bert-base-uncased', action=\"substitute\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug4 = naw.ContextualWordEmbsAug(model_path='roberta-base', action=\"substitute\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Synonym Augmentation method using PPDB models downloaded from http://paraphrase.org/#/download\n",
    "aug5 = naw.SynonymAug(aug_src='ppdb', model_path='ppdb-2.0-s-all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n_qdWy1yBsjO",
    "outputId": "8d1c098a-415d-46e2-84e7-7890c18f1d64"
   },
   "outputs": [],
   "source": [
    "#!pip3 install nltk\n",
    "import nltk \n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "Erip7CBw7P_P",
    "outputId": "43118f16-235c-4a77-d208-42ffba7bb27e"
   },
   "outputs": [],
   "source": [
    "#Create a new dataframe with records not in GRP_0\n",
    "zero_dataframe = clean_data[clean_data[\"Assignment group\"] == 'GRP_0']\n",
    "new_dataframe = clean_data[clean_data[\"Assignment group\"] != 'GRP_0']\n",
    "zero_dataframe.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataframe copies for different augmentation methods\n",
    "new_dataframe2 = new_dataframe.copy()\n",
    "new_dataframe3 = new_dataframe.copy()\n",
    "new_dataframe4 = new_dataframe.copy()\n",
    "new_dataframe5 = new_dataframe.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YB8ELEaPRyuQ",
    "outputId": "3fddeed1-12af-4e5b-aed6-01d0fd54a61c"
   },
   "outputs": [],
   "source": [
    "new_dataframe.shape, zero_dataframe.shape, new_dataframe2.shape, new_dataframe3.shape, new_dataframe4.shape, new_dataframe5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 623
    },
    "id": "yUnTAlH_SXEW",
    "outputId": "ab3a7390-48d4-4ffb-9c73-a411301d578e"
   },
   "outputs": [],
   "source": [
    "maxsyn=2\n",
    "new_dataframe[\"Augmented_data\"] = new_dataframe.apply(lambda x: create_set_of_new_sentences(x['Clean_Description'], maxsyn),axis=1)\n",
    "new_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 623
    },
    "id": "yUnTAlH_SXEW",
    "outputId": "ab3a7390-48d4-4ffb-9c73-a411301d578e"
   },
   "outputs": [],
   "source": [
    "new_dataframe[\"Augmented_data\"] = new_dataframe.apply(lambda x: aug1.augment(x['Clean_Description']),axis=1)\n",
    "new_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "ThnwxmfATCun",
    "outputId": "877994ac-1978-45f4-fe85-26522e7fc5f3"
   },
   "outputs": [],
   "source": [
    "s = new_dataframe.apply(lambda x: pd.Series(x['Augmented_data']), axis=1).stack().reset_index(level=1, drop=True)\n",
    "s.name = 'Final_Text'\n",
    "new_dataframe_aug = new_dataframe.drop(['New Description','Augmented_data', 'Clean_Description'],axis=1).join(s)\n",
    "new_dataframe_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 623
    },
    "id": "yUnTAlH_SXEW",
    "outputId": "ab3a7390-48d4-4ffb-9c73-a411301d578e"
   },
   "outputs": [],
   "source": [
    "new_dataframe2[\"Augmented_data\"] = new_dataframe2.apply(lambda x: aug2.augment(x['Clean_Description']),axis=1)\n",
    "new_dataframe2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "ThnwxmfATCun",
    "outputId": "877994ac-1978-45f4-fe85-26522e7fc5f3"
   },
   "outputs": [],
   "source": [
    "s = new_dataframe2.apply(lambda x: pd.Series(x['Augmented_data']), axis=1).stack().reset_index(level=1, drop=True)\n",
    "s.name = 'Final_Text'\n",
    "new_dataframe_aug2 = new_dataframe2.drop(['New Description','Augmented_data', 'Clean_Description'],axis=1).join(s)\n",
    "new_dataframe_aug2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 623
    },
    "id": "yUnTAlH_SXEW",
    "outputId": "ab3a7390-48d4-4ffb-9c73-a411301d578e"
   },
   "outputs": [],
   "source": [
    "new_dataframe3[\"Augmented_data\"] = new_dataframe3.apply(lambda x: aug3.augment(x['Clean_Description']),axis=1)\n",
    "new_dataframe3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "ThnwxmfATCun",
    "outputId": "877994ac-1978-45f4-fe85-26522e7fc5f3"
   },
   "outputs": [],
   "source": [
    "s = new_dataframe3.apply(lambda x: pd.Series(x['Augmented_data']), axis=1).stack().reset_index(level=1, drop=True)\n",
    "s.name = 'Final_Text'\n",
    "new_dataframe_aug3 = new_dataframe3.drop(['New Description','Augmented_data', 'Clean_Description'],axis=1).join(s)\n",
    "new_dataframe_aug3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 623
    },
    "id": "yUnTAlH_SXEW",
    "outputId": "ab3a7390-48d4-4ffb-9c73-a411301d578e"
   },
   "outputs": [],
   "source": [
    "new_dataframe4[\"Augmented_data\"] = new_dataframe4.apply(lambda x: aug4.augment(x['Clean_Description']),axis=1)\n",
    "new_dataframe4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "ThnwxmfATCun",
    "outputId": "877994ac-1978-45f4-fe85-26522e7fc5f3"
   },
   "outputs": [],
   "source": [
    "s = new_dataframe4.apply(lambda x: pd.Series(x['Augmented_data']), axis=1).stack().reset_index(level=1, drop=True)\n",
    "s.name = 'Final_Text'\n",
    "new_dataframe_aug4 = new_dataframe4.drop(['New Description','Augmented_data', 'Clean_Description'],axis=1).join(s)\n",
    "new_dataframe_aug4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 623
    },
    "id": "yUnTAlH_SXEW",
    "outputId": "ab3a7390-48d4-4ffb-9c73-a411301d578e"
   },
   "outputs": [],
   "source": [
    "new_dataframe5[\"Augmented_data\"] = new_dataframe5.apply(lambda x: aug5.augment(x['Clean_Description']),axis=1)\n",
    "new_dataframe5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "ThnwxmfATCun",
    "outputId": "877994ac-1978-45f4-fe85-26522e7fc5f3"
   },
   "outputs": [],
   "source": [
    "s = new_dataframe5.apply(lambda x: pd.Series(x['Augmented_data']), axis=1).stack().reset_index(level=1, drop=True)\n",
    "s.name = 'Final_Text'\n",
    "new_dataframe_aug5 = new_dataframe5.drop(['New Description','Augmented_data', 'Clean_Description'],axis=1).join(s)\n",
    "new_dataframe_aug5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "zcQ3e0XSBsjT",
    "outputId": "e06b22a7-4753-4ae7-b39b-8b362130e374"
   },
   "outputs": [],
   "source": [
    "zero_dataframe = zero_dataframe.rename(columns={\"Clean_Description\": \"Final_Text\"})\n",
    "zero_dataframe = zero_dataframe.drop(['New Description'], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding Original data (without Augmentation)\n",
    "new_dataframe6 = clean_data[clean_data[\"Assignment group\"] != 'GRP_0']\n",
    "new_dataframe6 = new_dataframe6.rename(columns={\"Clean_Description\": \"Final_Text\"})\n",
    "new_dataframe6 = new_dataframe6.drop(['New Description'], axis = 1)\n",
    "new_dataframe6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes=[new_dataframe_aug, new_dataframe_aug2, new_dataframe_aug3, new_dataframe_aug4, new_dataframe_aug5, zero_dataframe, new_dataframe6]\n",
    "clean_data_result= pd.concat(dataframes)\n",
    "clean_data_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove duplicate rows after augmentation\n",
    "clean_data_result = clean_data_result.drop_duplicates(subset='Final_Text', keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jFKf35cRQWq9"
   },
   "outputs": [],
   "source": [
    "# Serialize the Augmented dataset for later use\n",
    "clean_data_result.to_csv('Interim_data.csv', index=False, encoding='utf_8_sig')\n",
    "#with open('/content/Interim_data.pkl','wb') as f:\n",
    "with open('Interim_data.pkl','wb') as f:\n",
    "    pickle.dump(clean_data_result, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the consolidated final translated pickle file \n",
    "#with open('/content/drive/MyDrive/Capstone/Final_Translated_combined.pkl','rb') as f:\n",
    "with open('Interim_data.pkl','rb') as f:\n",
    "    clean_data_result = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UzxDdWyMBsjT"
   },
   "source": [
    "## Stop words removal and Lemmatise text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vSfE-xBvrFaY",
    "outputId": "700c66e6-2953-4b52-d2ef-c917f708c106"
   },
   "outputs": [],
   "source": [
    "clean_data_result.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "45rNIDGvBsjU",
    "outputId": "160f6518-da4a-4b8e-90af-9efe101250f7"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "processed_all_documents = list()\n",
    "\n",
    "for desc in clean_data_result['Final_Text']:\n",
    "    word_tokens = word_tokenize(desc) \n",
    "    \n",
    "    filtered_sentence = [] \n",
    "\n",
    "    # Removing Stopwords\n",
    "    for w in word_tokens: \n",
    "        if w not in stop_words: \n",
    "            filtered_sentence.append(w) \n",
    "\n",
    "    words = ' '.join(filtered_sentence)\n",
    "    processed_all_documents.append(words)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Dh0hMMrrFaZ"
   },
   "outputs": [],
   "source": [
    "clean_data_result['Final_Text'] = processed_all_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "xn4waA6MBsjU",
    "outputId": "230e1634-a6e1-4c0a-b7fa-c3a7fe1d4d73"
   },
   "outputs": [],
   "source": [
    "clean_data_result.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "UZ7s9d10EWG8",
    "outputId": "1e2fd799-6daa-4926-872a-1bd89dc444bd"
   },
   "outputs": [],
   "source": [
    "clean_data_result.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T8YU9JQqEWG8",
    "outputId": "2e54db72-3032-4285-adba-7479f4eadab9"
   },
   "outputs": [],
   "source": [
    "clean_data_result.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EWJN-ekLBsjU",
    "outputId": "f3e0531c-e4f1-478f-f359-137d8308d251",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Lemmatisation using spacy library\n",
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WeoMfBEBBsjU",
    "outputId": "4f7dacf1-dca0-4789-97fd-28eda534c534"
   },
   "outputs": [],
   "source": [
    "!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7HWRcbMrrOgn",
    "outputId": "562ade65-2a51-45d0-8639-cf04fa8323b8"
   },
   "outputs": [],
   "source": [
    "!pip3 install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UAGj1FplBsjV"
   },
   "outputs": [],
   "source": [
    "# Need to run \"python -m spacy download en\" in anaconda prompt to avoid 'en' not found issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Pxl8SyyBsjV"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']\n",
    "def lemmatize_text(text):\n",
    "    doc = nlp(text)\n",
    "    return ' '.join([token.lemma_ for token in doc])\n",
    "\n",
    "clean_data_result['Final_Text'] = clean_data_result['Final_Text'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "P0YLyD-iBsjV",
    "outputId": "87b8086f-3986-4b2f-941e-44e4e3860d83"
   },
   "outputs": [],
   "source": [
    "clean_data_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data_result1 = clean_data_result[clean_data_result['Final_Text'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data_result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignment_group_cnt=clean_data_result1['Assignment group'].value_counts()\n",
    "assignment_group_cnt.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignment_group_cnt.tail(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3yJf4QCZaoah"
   },
   "outputs": [],
   "source": [
    "# Serialize the translated dataset\n",
    "clean_data_result1.to_csv('Final_data.csv', index=False, encoding='utf_8_sig')\n",
    "#with open('/content/Final_data.pkl','wb') as f:\n",
    "with open('Final_data.pkl','wb') as f:\n",
    "    pickle.dump(clean_data_result1, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "e-zXZRYDaoah"
   },
   "outputs": [],
   "source": [
    "# Load the translated pickle file \n",
    "#with open('/content/Final_data.pkl','rb') as f:\n",
    "with open('Final_data.pkl','rb') as f:\n",
    "    clean_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QhCUgD6_Bsjn"
   },
   "source": [
    "## Prepping Dataframe for Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "9ijBA6KgW13C",
    "outputId": "895065ec-3e80-4944-a8c8-85cd9cc37510"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Caller</th>\n",
       "      <th>Assignment group</th>\n",
       "      <th>language</th>\n",
       "      <th>Final_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vrfpyjwi nzhvgqiw</td>\n",
       "      <td>GRP_24</td>\n",
       "      <td>de</td>\n",
       "      <td>hello however ' happen the pc see title repeat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vrfpyjwi nzhvgqiw</td>\n",
       "      <td>GRP_24</td>\n",
       "      <td>de</td>\n",
       "      <td>hello Ben Tige right Number Block Keyboard R L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vrfpyjwi nzhvgqiw</td>\n",
       "      <td>GRP_24</td>\n",
       "      <td>de</td>\n",
       "      <td>Fyi Axesnghb Cyzuomxa Sent Thursday 20 Ughzilf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vrfpyjwi nzhvgqiw</td>\n",
       "      <td>GRP_24</td>\n",
       "      <td>de</td>\n",
       "      <td>after IE browser open CRM , even prompt user c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lpnzjimy mwtvondq</td>\n",
       "      <td>GRP_25</td>\n",
       "      <td>de</td>\n",
       "      <td>current change enter first follow EU Tool Erro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>wgmqlnzh vpebwoat</td>\n",
       "      <td>GRP_30</td>\n",
       "      <td>zh-cn</td>\n",
       "      <td>the display appear morning . display light</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>rtjwbuev gfpwdetq</td>\n",
       "      <td>GRP_31</td>\n",
       "      <td>zh-cn</td>\n",
       "      <td>the prtsid _ - file print printer , prompt pri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>fupikdoa gjkytoeh</td>\n",
       "      <td>GRP_48</td>\n",
       "      <td>zh-cn</td>\n",
       "      <td>the online delivery unit provide customer ca n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>kyagjxdh dmtjpbnz</td>\n",
       "      <td>GRP_30</td>\n",
       "      <td>zh-cn</td>\n",
       "      <td>show ` ` data employee , please notify system ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>xqyjztnm onfusvlz</td>\n",
       "      <td>GRP_30</td>\n",
       "      <td>zh-cn</td>\n",
       "      <td>to small congratulation , computer open comput...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27992 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Caller Assignment group language  \\\n",
       "0   vrfpyjwi nzhvgqiw           GRP_24       de   \n",
       "0   vrfpyjwi nzhvgqiw           GRP_24       de   \n",
       "0   vrfpyjwi nzhvgqiw           GRP_24       de   \n",
       "0   vrfpyjwi nzhvgqiw           GRP_24       de   \n",
       "1   lpnzjimy mwtvondq           GRP_25       de   \n",
       "..                ...              ...      ...   \n",
       "44  wgmqlnzh vpebwoat           GRP_30    zh-cn   \n",
       "45  rtjwbuev gfpwdetq           GRP_31    zh-cn   \n",
       "46  fupikdoa gjkytoeh           GRP_48    zh-cn   \n",
       "47  kyagjxdh dmtjpbnz           GRP_30    zh-cn   \n",
       "48  xqyjztnm onfusvlz           GRP_30    zh-cn   \n",
       "\n",
       "                                           Final_Text  \n",
       "0   hello however ' happen the pc see title repeat...  \n",
       "0   hello Ben Tige right Number Block Keyboard R L...  \n",
       "0   Fyi Axesnghb Cyzuomxa Sent Thursday 20 Ughzilf...  \n",
       "0   after IE browser open CRM , even prompt user c...  \n",
       "1   current change enter first follow EU Tool Erro...  \n",
       "..                                                ...  \n",
       "44         the display appear morning . display light  \n",
       "45  the prtsid _ - file print printer , prompt pri...  \n",
       "46  the online delivery unit provide customer ca n...  \n",
       "47  show ` ` data employee , please notify system ...  \n",
       "48  to small congratulation , computer open comput...  \n",
       "\n",
       "[27992 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FRguKfOkEWHJ",
    "outputId": "b5ec8f33-a259-47e4-e3b2-2a60d3447a0e"
   },
   "outputs": [],
   "source": [
    "'''# Import label encoder \n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "  \n",
    "# label_encoder object knows how to understand word labels. \n",
    "label_encoder = preprocessing.LabelEncoder() \n",
    "  \n",
    "# Encode labels in column 'species'. \n",
    "clean_data['Assignment group LabelEncoded']= label_encoder.fit_transform(clean_data['Assignment group']) \n",
    "  \n",
    "\n",
    "\n",
    "onehot_encoder = OneHotEncoder()\n",
    "clean_data['Assignment group LabelEncoded'] = clean_data['Assignment group LabelEncoded'].values.reshape(len(clean_data['Assignment group LabelEncoded']), 1)\n",
    "clean_data['Assignment group OneHotEncoded'] = onehot_encoder.fit_transform(clean_data[['Assignment group LabelEncoded']])\n",
    "clean_data['Assignment group OneHotEncoded']'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FRguKfOkEWHJ",
    "outputId": "b5ec8f33-a259-47e4-e3b2-2a60d3447a0e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     17\n",
       "0     17\n",
       "0     17\n",
       "0     17\n",
       "1     18\n",
       "      ..\n",
       "44    24\n",
       "45    25\n",
       "46    40\n",
       "47    24\n",
       "48    24\n",
       "Name: Assignment group LabelEncoded, Length: 27992, dtype: int32"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import label encoder \n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "  \n",
    "# label_encoder object knows how to understand word labels. \n",
    "label_encoder = preprocessing.LabelEncoder() \n",
    "  \n",
    "# Encode labels in column 'species'. \n",
    "clean_data['Assignment group LabelEncoded']= label_encoder.fit_transform(clean_data['Assignment group']) \n",
    "\n",
    "target_strings = label_encoder.inverse_transform(clean_data['Assignment group LabelEncoded'])\n",
    "target_strings_list = np.unique(target_strings).tolist()\n",
    "clean_data['Assignment group LabelEncoded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Create a target categorical column\n",
    "clean_data['Assignment group LabelEncoded'] = clean_data['Assignment group'].astype('category').cat.codes\n",
    "clean_data.info()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6jsBDjGIBsjn",
    "outputId": "80cff84f-f1ab-4213-e1f0-6f7f2d8c55a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'GRP_24': 17,\n",
       " 'GRP_25': 18,\n",
       " 'GRP_33': 26,\n",
       " 'GRP_28': 21,\n",
       " 'GRP_1': 1,\n",
       " 'GRP_2': 12,\n",
       " 'GRP_42': 34,\n",
       " 'GRP_3': 23,\n",
       " 'GRP_10': 2,\n",
       " 'GRP_34': 27,\n",
       " 'GRP_4': 31,\n",
       " 'GRP_12': 4,\n",
       " 'GRP_5': 42,\n",
       " 'GRP_6': 48,\n",
       " 'GRP_7': 52,\n",
       " 'GRP_8': 53,\n",
       " 'GRP_9': 54,\n",
       " 'GRP_11': 3,\n",
       " 'GRP_13': 5,\n",
       " 'GRP_14': 6,\n",
       " 'GRP_15': 7,\n",
       " 'GRP_16': 8,\n",
       " 'GRP_17': 9,\n",
       " 'GRP_18': 10,\n",
       " 'GRP_19': 11,\n",
       " 'GRP_20': 13,\n",
       " 'GRP_21': 14,\n",
       " 'GRP_22': 15,\n",
       " 'GRP_23': 16,\n",
       " 'GRP_26': 19,\n",
       " 'GRP_27': 20,\n",
       " 'GRP_29': 22,\n",
       " 'GRP_30': 24,\n",
       " 'GRP_31': 25,\n",
       " 'GRP_36': 28,\n",
       " 'GRP_37': 29,\n",
       " 'GRP_39': 30,\n",
       " 'GRP_40': 32,\n",
       " 'GRP_41': 33,\n",
       " 'GRP_43': 35,\n",
       " 'GRP_44': 36,\n",
       " 'GRP_45': 37,\n",
       " 'GRP_46': 38,\n",
       " 'GRP_47': 39,\n",
       " 'GRP_49': 41,\n",
       " 'GRP_50': 43,\n",
       " 'GRP_51': 44,\n",
       " 'GRP_52': 45,\n",
       " 'GRP_53': 46,\n",
       " 'GRP_48': 40,\n",
       " 'GRP_59': 47,\n",
       " 'GRP_60': 49,\n",
       " 'GRP_62': 50,\n",
       " 'GRP_65': 51,\n",
       " 'GRP_0': 0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoded_dict = dict(zip(clean_data['Assignment group'].unique(), clean_data['Assignment group LabelEncoded'].unique()))\n",
    "label_encoded_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UaaV3zyqBsjo",
    "outputId": "50c824e2-62cf-4e84-b530-97390b194802"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mShape of the training set:\u001b[0m (19594,) (8398,)\n",
      "\u001b[1mShape of the test set:\u001b[0m (19594,) (8398,)\n"
     ]
    }
   ],
   "source": [
    "# Splitting Train Test \n",
    "from sklearn.model_selection import train_test_split\n",
    "#Y = np.array(clean_data['Assignment group OneHotEncoded'])\n",
    "\n",
    "X_train, X_test, y_train1, y_test1 = train_test_split(clean_data['Final_Text'], clean_data['Assignment group LabelEncoded'], test_size=0.3, random_state = 0, stratify=clean_data['Assignment group LabelEncoded'])\n",
    "print('\\033[1mShape of the training set:\\033[0m', X_train.shape, X_test.shape)\n",
    "print('\\033[1mShape of the test set:\\033[0m', y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OdB5_mvzQWrN",
    "outputId": "a13b8161-d4b4-4de9-d659-389ad0a37b60"
   },
   "outputs": [],
   "source": [
    "#Using TFIDF vectors feature extraction\n",
    "Tfidf_vect = TfidfVectorizer(max_features=2500)\n",
    "Tfidf_vect.fit(clean_data.Final_Text.astype(str))\n",
    "X_train_tfidf = Tfidf_vect.transform(X_train)\n",
    "X_test_tfidf = Tfidf_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n3UGzor_Bsjo",
    "outputId": "e58cedb8-7627-4ea6-c004-5078978e671d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Input Feature : (27992, 2500)\n",
      "Shape of Target Feature : (27992,)\n"
     ]
    }
   ],
   "source": [
    "#using count vectorizer for features extraction\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "CV = CountVectorizer(max_features=2500)\n",
    "\n",
    "X_BoW = CV.fit_transform(clean_data['Final_Text']).toarray()\n",
    "y = clean_data['Assignment group LabelEncoded']\n",
    "\n",
    "print(\"Shape of Input Feature :\",np.shape(X_BoW))\n",
    "print(\"Shape of Target Feature :\",np.shape(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UaaV3zyqBsjo",
    "outputId": "50c824e2-62cf-4e84-b530-97390b194802"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mShape of the training set:\u001b[0m (19594, 2500) (8398, 2500)\n",
      "\u001b[1mShape of the test set:\u001b[0m (19594,) (8398,)\n"
     ]
    }
   ],
   "source": [
    "# Splitting Train Test \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_BoW, y, test_size=0.3, random_state = 0, stratify=y)\n",
    "print('\\033[1mShape of the training set:\\033[0m', X_train.shape, X_test.shape)\n",
    "print('\\033[1mShape of the test set:\\033[0m', y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "EIni9BjpBsjo"
   },
   "outputs": [],
   "source": [
    "def run_classification(estimator, X_train, X_test, y_train, y_test, arch_name=None, pipelineRequired=True, isDeepModel=False):\n",
    "    # train the model\n",
    "    clf = estimator\n",
    "\n",
    "    if pipelineRequired :\n",
    "        clf = Pipeline([('tfidf', TfidfTransformer()),\n",
    "                     ('clf', estimator),\n",
    "                     ])\n",
    "      \n",
    "    if isDeepModel :\n",
    "        clf.fit(X_train, y_train, validation_data=(X_test, y_test),epochs=25, batch_size=128,verbose=1,callbacks=call_backs(arch_name))\n",
    "        # predict from the clasiffier\n",
    "        y_pred = clf.predict(X_test)\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "        y_train_pred = clf.predict(X_train)\n",
    "        y_train_pred = np.argmax(y_train_pred, axis=1)\n",
    "    else :\n",
    "        clf.fit(X_train, y_train)\n",
    "        # predict from the clasiffier\n",
    "        y_pred = clf.predict(X_test)\n",
    "        y_train_pred = clf.predict(X_train)\n",
    "    \n",
    "    print('Estimator:', clf)\n",
    "    print('='*80)\n",
    "    print('Training accuracy: %.2f%%' % (accuracy_score(y_train,y_train_pred) * 100))\n",
    "    print('Testing accuracy: %.2f%%' % (accuracy_score(y_test, y_pred) * 100))\n",
    "    print('='*80)\n",
    "    print('Confusion matrix:\\n %s' % (confusion_matrix(y_test, y_pred)))\n",
    "    print('='*80)\n",
    "    print('Classification report:\\n %s' % (classification_report(y_test, y_pred, target_names=target_strings_list)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uxlH4wqyBsjo"
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l_RXTWZfBsjp",
    "outputId": "dd1d77ac-0bc1-4cd9-b22a-8df42de620c1"
   },
   "outputs": [],
   "source": [
    "run_classification(LogisticRegression(C=5, penalty='l2', solver='liblinear'), X_train_tfidf, X_test_tfidf, y_train1, y_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l_RXTWZfBsjp",
    "outputId": "dd1d77ac-0bc1-4cd9-b22a-8df42de620c1"
   },
   "outputs": [],
   "source": [
    "run_classification(LogisticRegression(C=5, penalty='l2', solver='liblinear'), X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7r6p2SGBsjp"
   },
   "source": [
    "## Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6hlgbYXyBsjp",
    "outputId": "f75910b3-5c53-46d9-8025-732bc5e6c807"
   },
   "outputs": [],
   "source": [
    "run_classification(MultinomialNB(alpha=0, class_prior=None, fit_prior='True'), X_train_tfidf, X_test_tfidf, y_train1, y_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6hlgbYXyBsjp",
    "outputId": "f75910b3-5c53-46d9-8025-732bc5e6c807"
   },
   "outputs": [],
   "source": [
    "run_classification(MultinomialNB(alpha=0, class_prior=None, fit_prior='True'), X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gs5ztA2kBsjp"
   },
   "source": [
    "## K-nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A4qwMKt7Bsjq",
    "outputId": "0e7b1e07-5dcc-42da-daad-e7be330ea147"
   },
   "outputs": [],
   "source": [
    "run_classification(KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='euclidean',\n",
    "                     metric_params=None, n_jobs=None, n_neighbors=2, p=2,\n",
    "                     weights='distance'), X_train_tfidf, X_test_tfidf, y_train1, y_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A4qwMKt7Bsjq",
    "outputId": "0e7b1e07-5dcc-42da-daad-e7be330ea147"
   },
   "outputs": [],
   "source": [
    "run_classification(KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='euclidean',\n",
    "                     metric_params=None, n_jobs=None, n_neighbors=2, p=2,\n",
    "                     weights='distance'), X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDMha2lxBsjq"
   },
   "source": [
    "## Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jBc7yY11Bsjq",
    "outputId": "af737f37-54df-4175-c68a-6ba71733fdd6"
   },
   "outputs": [],
   "source": [
    "run_classification(LinearSVC(C=1), X_train_tfidf, X_test_tfidf, y_train1, y_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jBc7yY11Bsjq",
    "outputId": "af737f37-54df-4175-c68a-6ba71733fdd6"
   },
   "outputs": [],
   "source": [
    "run_classification(LinearSVC(C=1), X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mDPisp5SBsjq"
   },
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oyvv2qn5Bsjq",
    "outputId": "df8e2cbf-b090-40c2-d8b9-48eaa4027c2e"
   },
   "outputs": [],
   "source": [
    "run_classification(DecisionTreeClassifier(criterion='gini', min_samples_leaf=2), X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oyvv2qn5Bsjq",
    "outputId": "df8e2cbf-b090-40c2-d8b9-48eaa4027c2e"
   },
   "outputs": [],
   "source": [
    "run_classification(DecisionTreeClassifier(criterion='gini', min_samples_leaf=2), X_train_tfidf, X_test_tfidf, y_train1, y_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpJIipnsBsjr"
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8b28KjuoBsjr",
    "outputId": "c7170675-6fd1-4e46-bf10-76a309af123f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator: Pipeline(steps=[('tfidf', TfidfTransformer()),\n",
      "                ('clf',\n",
      "                 RandomForestClassifier(criterion='entropy', random_state=0))])\n",
      "================================================================================\n",
      "Training accuracy: 97.99%\n",
      "Testing accuracy: 77.49%\n",
      "================================================================================\n",
      "Confusion matrix:\n",
      " [[862   0   0 ...   1   0   0]\n",
      " [  0  34   0 ...   0  12   0]\n",
      " [ 13   0 160 ...   0  48   6]\n",
      " ...\n",
      " [ 17   0   0 ... 103   0   0]\n",
      " [ 13   0   6 ...   0 969  53]\n",
      " [ 15   0   5 ...   0 207 147]]\n",
      "================================================================================\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       GRP_0       0.64      0.90      0.75       955\n",
      "       GRP_1       0.94      0.56      0.70        61\n",
      "      GRP_10       0.89      0.65      0.75       246\n",
      "      GRP_11       0.98      0.87      0.92        55\n",
      "      GRP_12       0.84      0.86      0.85       419\n",
      "      GRP_13       0.93      0.89      0.91       259\n",
      "      GRP_14       0.92      0.85      0.88       208\n",
      "      GRP_15       0.99      0.94      0.96        70\n",
      "      GRP_16       0.98      0.88      0.93       150\n",
      "      GRP_17       0.95      0.78      0.85        45\n",
      "      GRP_18       0.96      0.87      0.91       156\n",
      "      GRP_19       0.91      0.84      0.87       386\n",
      "       GRP_2       0.87      0.84      0.85       362\n",
      "      GRP_20       0.98      0.87      0.92        63\n",
      "      GRP_21       1.00      0.85      0.92        52\n",
      "      GRP_22       0.98      0.88      0.92        56\n",
      "      GRP_23       0.95      0.82      0.88        45\n",
      "      GRP_24       0.73      0.79      0.76       407\n",
      "      GRP_25       0.91      0.83      0.87       199\n",
      "      GRP_26       0.99      0.89      0.94        99\n",
      "      GRP_27       0.93      0.84      0.89        32\n",
      "      GRP_28       0.91      0.70      0.79        76\n",
      "      GRP_29       0.98      0.89      0.93       172\n",
      "       GRP_3       0.86      0.86      0.86       367\n",
      "      GRP_30       0.29      0.59      0.38        34\n",
      "      GRP_31       0.75      0.62      0.68        98\n",
      "      GRP_33       0.72      0.58      0.64       212\n",
      "      GRP_34       0.77      0.66      0.71       109\n",
      "      GRP_36       0.75      0.68      0.71        22\n",
      "      GRP_37       1.00      0.90      0.95        29\n",
      "      GRP_39       0.77      0.48      0.59        21\n",
      "       GRP_4       0.94      0.74      0.83       182\n",
      "      GRP_40       1.00      0.86      0.93        81\n",
      "      GRP_41       1.00      0.85      0.92        72\n",
      "      GRP_42       0.67      0.55      0.60        58\n",
      "      GRP_43       1.00      0.78      0.88         9\n",
      "      GRP_44       1.00      0.93      0.96        27\n",
      "      GRP_45       0.94      0.73      0.82        60\n",
      "      GRP_46       1.00      0.57      0.73         7\n",
      "      GRP_47       0.82      0.65      0.73        43\n",
      "      GRP_48       0.12      0.06      0.08        17\n",
      "      GRP_49       1.00      0.75      0.86         8\n",
      "       GRP_5       0.76      0.43      0.55       231\n",
      "      GRP_50       1.00      0.83      0.90        23\n",
      "      GRP_51       1.00      0.86      0.92        14\n",
      "      GRP_52       0.93      0.87      0.90        15\n",
      "      GRP_53       0.94      0.75      0.83        20\n",
      "      GRP_59       1.00      1.00      1.00         9\n",
      "       GRP_6       0.76      0.49      0.60       312\n",
      "      GRP_60       0.74      0.44      0.55        32\n",
      "      GRP_62       0.78      0.58      0.67        43\n",
      "      GRP_65       1.00      0.80      0.89        20\n",
      "       GRP_7       0.94      0.82      0.88       125\n",
      "       GRP_8       0.61      0.86      0.72      1121\n",
      "       GRP_9       0.60      0.36      0.45       404\n",
      "\n",
      "    accuracy                           0.77      8398\n",
      "   macro avg       0.87      0.75      0.79      8398\n",
      "weighted avg       0.80      0.77      0.77      8398\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_classification(RandomForestClassifier(criterion= 'entropy', n_estimators=100, random_state=0), X_train_tfidf, X_test_tfidf, y_train1, y_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8b28KjuoBsjr",
    "outputId": "c7170675-6fd1-4e46-bf10-76a309af123f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator: Pipeline(steps=[('tfidf', TfidfTransformer()),\n",
      "                ('clf',\n",
      "                 RandomForestClassifier(criterion='entropy', random_state=0))])\n",
      "================================================================================\n",
      "Training accuracy: 97.99%\n",
      "Testing accuracy: 77.88%\n",
      "================================================================================\n",
      "Confusion matrix:\n",
      " [[865   0   0 ...   1   0   0]\n",
      " [  0  35   0 ...   0  12   0]\n",
      " [  9   0 166 ...   0  52   4]\n",
      " ...\n",
      " [ 14   0   0 ... 105   0   0]\n",
      " [ 14   0   4 ...   0 976  53]\n",
      " [ 20   0   3 ...   0 207 144]]\n",
      "================================================================================\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.91      0.75       955\n",
      "           1       0.90      0.57      0.70        61\n",
      "           2       0.93      0.67      0.78       246\n",
      "           3       1.00      0.85      0.92        55\n",
      "           4       0.86      0.87      0.87       419\n",
      "           5       0.92      0.90      0.91       259\n",
      "           6       0.95      0.84      0.89       208\n",
      "           7       0.99      0.94      0.96        70\n",
      "           8       0.97      0.87      0.92       150\n",
      "           9       0.93      0.87      0.90        45\n",
      "          10       0.97      0.89      0.93       156\n",
      "          11       0.91      0.83      0.87       386\n",
      "          12       0.87      0.82      0.85       362\n",
      "          13       0.98      0.87      0.92        63\n",
      "          14       1.00      0.81      0.89        52\n",
      "          15       0.98      0.86      0.91        56\n",
      "          16       0.97      0.80      0.88        45\n",
      "          17       0.74      0.79      0.76       407\n",
      "          18       0.89      0.84      0.86       199\n",
      "          19       1.00      0.88      0.94        99\n",
      "          20       0.92      0.75      0.83        32\n",
      "          21       0.95      0.74      0.83        76\n",
      "          22       0.96      0.89      0.92       172\n",
      "          23       0.86      0.85      0.86       367\n",
      "          24       0.31      0.62      0.42        34\n",
      "          25       0.75      0.64      0.69        98\n",
      "          26       0.73      0.60      0.66       212\n",
      "          27       0.80      0.68      0.74       109\n",
      "          28       0.74      0.64      0.68        22\n",
      "          29       1.00      0.86      0.93        29\n",
      "          30       0.88      0.67      0.76        21\n",
      "          31       0.95      0.77      0.85       182\n",
      "          32       1.00      0.81      0.90        81\n",
      "          33       0.98      0.89      0.93        72\n",
      "          34       0.67      0.53      0.60        58\n",
      "          35       1.00      0.78      0.88         9\n",
      "          36       1.00      0.96      0.98        27\n",
      "          37       0.96      0.73      0.83        60\n",
      "          38       1.00      0.71      0.83         7\n",
      "          39       0.88      0.65      0.75        43\n",
      "          40       0.17      0.12      0.14        17\n",
      "          41       1.00      0.75      0.86         8\n",
      "          42       0.80      0.42      0.55       231\n",
      "          43       0.94      0.74      0.83        23\n",
      "          44       1.00      0.86      0.92        14\n",
      "          45       0.86      0.80      0.83        15\n",
      "          46       0.94      0.75      0.83        20\n",
      "          47       1.00      1.00      1.00         9\n",
      "          48       0.75      0.50      0.60       312\n",
      "          49       0.75      0.47      0.58        32\n",
      "          50       0.81      0.60      0.69        43\n",
      "          51       1.00      0.85      0.92        20\n",
      "          52       0.92      0.84      0.88       125\n",
      "          53       0.61      0.87      0.72      1121\n",
      "          54       0.60      0.36      0.45       404\n",
      "\n",
      "    accuracy                           0.78      8398\n",
      "   macro avg       0.87      0.75      0.80      8398\n",
      "weighted avg       0.80      0.78      0.78      8398\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_classification(RandomForestClassifier(criterion= 'entropy', n_estimators=100, random_state=0), X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOuKESHfrFai"
   },
   "source": [
    "## GradientBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RJr4CJMBBsjr",
    "outputId": "0306f2e9-d6e7-417f-b4ee-00e629a545a0"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "run_classification(GradientBoostingClassifier(max_depth=15, n_estimators=50, random_state=42), X_train_tfidf, X_test_tfidf, y_train1, y_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RJr4CJMBBsjr",
    "outputId": "0306f2e9-d6e7-417f-b4ee-00e629a545a0"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "run_classification(GradientBoostingClassifier(max_depth=15, n_estimators=50, random_state=42), X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j-lWxfbVrFai"
   },
   "source": [
    "## XGBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wIYQ2WZJrFai",
    "outputId": "9d8a7f22-93f0-4580-ee23-d4a0d52173ef"
   },
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8q7-fcTurFai",
    "outputId": "31b50f94-418f-46bb-998c-ceac90867846"
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "run_classification(XGBClassifier(n_estimators=50, max_depth=15), X_train_tfidf, X_test_tfidf, y_train1, y_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1MizYVnZrFai"
   },
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KPuCWrZGrFai",
    "outputId": "01ea7b26-24b6-44a4-b74a-75e3a675e7a9"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "run_classification(BaggingClassifier(n_estimators=100, random_state=10), X_train_tfidf, X_test_tfidf, y_train1, y_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nd3PAGHmrFaj"
   },
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mfLNzU90rFaj",
    "outputId": "4412db03-d4d5-430e-dce6-f94901af3611"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "estimators = [('rf', RandomForestClassifier(n_estimators=100, random_state=42)), ('svr', make_pipeline(StandardScaler(with_mean=False), LinearSVC(random_state=42)))]\n",
    "\n",
    "run_classification(StackingClassifier(estimators=estimators, final_estimator=DecisionTreeClassifier()), X_train_tfidf, X_test_tfidf, y_train1, y_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XCl8T1jNrFaj"
   },
   "source": [
    "## Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Q_lE-UerFaj",
    "outputId": "ef14b7c6-f83f-40b2-f286-f6b06111d794"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "\n",
    "estimators = [('rf', RandomForestClassifier(criterion= 'entropy', n_estimators=100, random_state=42)), ('knn', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='euclidean',\n",
    "                     metric_params=None, n_jobs=None, n_neighbors=2, p=2,\n",
    "                     weights='distance')), ('bg', BaggingClassifier(n_estimators=100, random_state=42)), ('lsvc', LinearSVC(C=1, random_state=42))]\n",
    "\n",
    "run_classification(VotingClassifier(estimators=estimators, voting='hard'), X_train_tfidf, X_test_tfidf, y_train1, y_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gVNcUZ1BQWrL"
   },
   "source": [
    "## Deep Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4RelfajlQWrL"
   },
   "outputs": [],
   "source": [
    "# Load the augmented data from pickle file \n",
    "#with open('/content/Interim_data.pkl','rb') as f:\n",
    "with open('Interim_data.pkl','rb') as f:\n",
    "    clean_data_DL = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "15XBDLMRQWrL",
    "outputId": "0d63c014-d77f-4f34-9b27-1cd367d7845b"
   },
   "outputs": [],
   "source": [
    "clean_data_DL.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FxgUQxvhQWrL"
   },
   "outputs": [],
   "source": [
    "clean_data_DL['Final_Text'] = clean_data_DL['Final_Text'].replace(np.nan, '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HzJ4VnECQWrM",
    "outputId": "31898b2e-9882-45b1-f978-52847818a44f"
   },
   "outputs": [],
   "source": [
    "clean_data_DL.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oTiSKPsIr7RX",
    "outputId": "9fcf80c9-d085-4892-e249-85fe3f900737"
   },
   "outputs": [],
   "source": [
    "# Import label encoder \n",
    "from sklearn import preprocessing \n",
    "  \n",
    "# label_encoder object knows how to understand word labels. \n",
    "label_encoder = preprocessing.LabelEncoder() \n",
    "  \n",
    "# Encode labels in column 'species'. \n",
    "clean_data_DL['Assignment group LabelEncoded']= label_encoder.fit_transform(clean_data_DL['Assignment group']) \n",
    "  \n",
    "clean_data_DL['Assignment group LabelEncoded'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hwiIGk4ar7gA",
    "outputId": "aa4f124b-8bfb-4b4f-855d-dbf8bc620277"
   },
   "outputs": [],
   "source": [
    "label_encoded_dict = dict(zip(clean_data_DL['Assignment group'].unique(), clean_data_DL['Assignment group LabelEncoded'].unique()))\n",
    "len(label_encoded_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ibiTTlGLQWrM",
    "outputId": "b151faa3-9e0f-4816-cf88-ba3eb99b5c24"
   },
   "outputs": [],
   "source": [
    "# Splitting Train Test \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(clean_data_DL['Final_Text'], clean_data_DL['Assignment group LabelEncoded'], test_size=0.3, random_state = 0, stratify=clean_data_DL['Assignment group LabelEncoded'])\n",
    "print('\\033[1mShape of the training set:\\033[0m', X_train.shape, X_test.shape)\n",
    "print('\\033[1mShape of the test set:\\033[0m', y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2GOVYYV9QWrM"
   },
   "source": [
    "### Create checkpoints function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3RG6wVQXQWrN"
   },
   "outputs": [],
   "source": [
    "#Path where you want to save the weights, model and checkpoints\n",
    "model_path = \"Weights/\"\n",
    "%mkdir Weights\n",
    "\n",
    "# Define model callbacks\n",
    "def call_backs(name):\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', mode='min', min_delta=0.01, patience=3)\n",
    "    model_checkpoint =  ModelCheckpoint(model_path + name + '_epoch{epoch:02d}_loss{val_loss:.4f}.h5',\n",
    "                                                               monitor='val_loss',\n",
    "                                                               verbose=1,\n",
    "                                                               save_best_only=True,\n",
    "                                                               save_weights_only=False,\n",
    "                                                               mode='min',\n",
    "                                                               period=1)\n",
    "    return [model_checkpoint, early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lAC5r5UcQWrN"
   },
   "outputs": [],
   "source": [
    "# Function to build Neural Network\n",
    "def Build_Model_DNN_Text(shape, nClasses, dropout=0.3):\n",
    "    \"\"\"\n",
    "    buildModel_DNN_Tex(shape, nClasses,dropout)\n",
    "    Build Deep neural networks Model for text classification\n",
    "    Shape is input feature space\n",
    "    nClasses is number of classes\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    node = 512 # number of nodes\n",
    "    nLayers = 4 # number of  hidden layer\n",
    "    model.add(Dense(node,input_dim=shape,activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    for i in range(0,nLayers):\n",
    "        model.add(Dense(node,input_dim=node,activation='relu'))\n",
    "        model.add(Dropout(dropout))\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Dense(nClasses, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OdB5_mvzQWrN",
    "outputId": "a13b8161-d4b4-4de9-d659-389ad0a37b60"
   },
   "outputs": [],
   "source": [
    "#from tensorflow.keras import Sequential\n",
    "#from tensorflow.keras.layers import Conv2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "\n",
    "Tfidf_vect = TfidfVectorizer(max_features=2500)\n",
    "Tfidf_vect.fit(clean_data_DL.Final_Text.astype(str))\n",
    "X_train_tfidf = Tfidf_vect.transform(X_train)\n",
    "X_test_tfidf = Tfidf_vect.transform(X_test)\n",
    "\n",
    "# Instantiate the network\n",
    "model_DNN = Build_Model_DNN_Text(X_train_tfidf.shape[1], 55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "a6aC4UqOQWrN",
    "outputId": "d12d6dcf-83ce-4dd5-f827-b3e58c8862be"
   },
   "outputs": [],
   "source": [
    "run_classification(model_DNN, X_train_tfidf, X_test_tfidf, y_train, y_test,pipelineRequired = False,isDeepModel=True, arch_name='DNN')\n",
    "\n",
    "'''model_DNN.fit(X_train_tfidf, y_train,\n",
    "                              validation_data=(X_test_tfidf, y_test),\n",
    "                              callbacks=call_backs(\"NN\"),\n",
    "                              epochs=10,\n",
    "                              batch_size=128,\n",
    "                              verbose=2)\n",
    "predicted = model_DNN.predict(X_test_tfidf)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LaCQnFBRQWrO"
   },
   "source": [
    "### Extract Glove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hMA3MFYUQWrO",
    "outputId": "c1f900ed-460d-425a-c93d-c7f7b4f719d2"
   },
   "outputs": [],
   "source": [
    "#download the glove embedding zip file from http://nlp.stanford.edu/data/wordvecs/glove.6B.zip\n",
    "from zipfile import ZipFile\n",
    "# Check if it is already extracted else Open the zipped file as readonly\n",
    "if not os.path.isfile('glove.6B/glove.6B.200d.txt'):\n",
    "    glove_embeddings = 'glove.6B.zip'\n",
    "    #glove_embeddings = '/content/drive/MyDrive/Capstone/glove.6B.zip'\n",
    "    with ZipFile(glove_embeddings, 'r') as archive:\n",
    "        archive.extractall('glove.6B')\n",
    "\n",
    "# List the files under extracted folder\n",
    "os.listdir('glove.6B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r2Xtecr3QWrO"
   },
   "source": [
    "## Convolutional Neural Networks (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ghIAygEfQWrO"
   },
   "outputs": [],
   "source": [
    "gloveFileName = 'glove.6B/glove.6B.200d.txt'\n",
    "#gloveFileName = '/content/glove.6B/glove.6B.200d.txt'\n",
    "MAX_SEQUENCE_LENGTH = 500\n",
    "EMBEDDING_DIM=200\n",
    "MAX_NB_WORDS=75000\n",
    "\n",
    "# Function to generate Embedding\n",
    "def loadData_Tokenizer(X_train, X_test,filename):\n",
    "    np.random.seed(7)\n",
    "    text = np.concatenate((X_train, X_test), axis=0)\n",
    "    text = np.array(text)\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    sequences = tokenizer.texts_to_sequences(text)\n",
    "    word_index = tokenizer.word_index\n",
    "    text = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "    indices = np.arange(text.shape[0])\n",
    "    # np.random.shuffle(indices)\n",
    "    text = text[indices]\n",
    "    print(text.shape)\n",
    "    X_train = text[0:len(X_train), ]\n",
    "    X_test = text[len(X_train):, ]\n",
    "    embeddings_index = {}\n",
    "    f = open(filename, encoding=\"utf8\")\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        try:\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "        except:\n",
    "            pass\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Total %s word vectors.' % len(embeddings_index))\n",
    "    return (X_train, X_test, word_index,embeddings_index)\n",
    "\n",
    "\n",
    "embedding_matrix = []\n",
    "\n",
    "def buildEmbed_matrices(word_index,embedding_dim):\n",
    "    embedding_matrix = np.random.random((len(word_index) + 1, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            if len(embedding_matrix[i]) !=len(embedding_vector):\n",
    "                print(\"could not broadcast input array from shape\",str(len(embedding_matrix[i])), \"into shape\",str(len(embedding_vector)),\n",
    "                      \" Please make sure your\"\" EMBEDDING_DIM is equal to embedding_vector file ,GloVe,\")\n",
    "                exit(1)\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DljFFfKxQWrO",
    "outputId": "a24cbbb9-d8ec-4b2f-80af-feb23291c3cf"
   },
   "outputs": [],
   "source": [
    "# Generate Glove embedded datasets\n",
    "X_train_Glove, X_test_Glove, word_index, embeddings_index = loadData_Tokenizer(X_train,X_test,gloveFileName)\n",
    "embedding_matrix = buildEmbed_matrices(word_index,EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "URiIMsVEQWrO"
   },
   "outputs": [],
   "source": [
    "def Build_Model_CNN_Text(word_index, embeddings_matrix, nclasses,dropout=0.5):\n",
    "    \"\"\"\n",
    "        def buildModel_CNN(word_index, embeddings_index, nclasses, MAX_SEQUENCE_LENGTH=500, EMBEDDING_DIM=50, dropout=0.5):\n",
    "        word_index in word index ,\n",
    "        embeddings_index is embeddings index, look at data_helper.py\n",
    "        nClasses is number of classes,\n",
    "        MAX_SEQUENCE_LENGTH is maximum lenght of text sequences,\n",
    "        EMBEDDING_DIM is an int value for dimention of word embedding look at data_helper.py\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    embedding_layer = Embedding(len(word_index) + 1,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embeddings_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=True)\n",
    "    # applying a more complex convolutional approach\n",
    "    convs = []\n",
    "    filter_sizes = []\n",
    "    layer = 5\n",
    "    print(\"Filter  \",layer)\n",
    "    for fl in range(0,layer):\n",
    "        filter_sizes.append((fl+2))\n",
    "    node = 128\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    for fsz in filter_sizes:\n",
    "        l_conv = Conv1D(node, kernel_size=fsz, activation='relu')(embedded_sequences)\n",
    "        l_pool = MaxPooling1D(5)(l_conv)\n",
    "        #l_pool = Dropout(0.25)(l_pool)\n",
    "        convs.append(l_pool)\n",
    "    l_merge = Concatenate(axis=1)(convs)\n",
    "    l_cov1 = Conv1D(node, 5, activation='relu')(l_merge)\n",
    "    l_cov1 = Dropout(dropout)(l_cov1)\n",
    "    l_batch1 = BatchNormalization()(l_cov1)\n",
    "    l_pool1 = MaxPooling1D(5)(l_batch1)\n",
    "    l_cov2 = Conv1D(node, 5, activation='relu')(l_pool1)\n",
    "    l_cov2 = Dropout(dropout)(l_cov2)\n",
    "    l_batch2 = BatchNormalization()(l_cov2)\n",
    "    l_pool2 = MaxPooling1D(30)(l_batch2)\n",
    "    l_flat = Flatten()(l_pool2)\n",
    "    l_dense = Dense(1024, activation='relu')(l_flat)\n",
    "    l_dense = Dropout(dropout)(l_dense)\n",
    "    l_dense = Dense(512, activation='relu')(l_dense)\n",
    "    l_dense = Dropout(dropout)(l_dense)\n",
    "    preds = Dense(nclasses, activation='softmax')(l_dense)\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fxdeJLw9QWrP",
    "outputId": "15656869-7b1e-4d24-d7ad-ada2e986771e"
   },
   "outputs": [],
   "source": [
    "# Train the network and run classification\n",
    "model_CNN = Build_Model_CNN_Text(word_index,embedding_matrix, 55)\n",
    "run_classification(model_CNN, X_train_Glove, X_test_Glove, y_train, y_test,pipelineRequired = False,isDeepModel=True, arch_name='CNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sMu2ONzLQWrP"
   },
   "source": [
    "## Recurrent Neural Networks (RNN) --> Gated Recurrent Unit (GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "grFaHdizQWrP"
   },
   "outputs": [],
   "source": [
    "def Build_Model_RNN_Text(word_index, embeddings_matrix, nclasses,dropout=0.5):\n",
    "    \"\"\"\n",
    "    def buildModel_RNN(word_index, embeddings_matrix, nclasses,  MAX_SEQUENCE_LENGTH=500, EMBEDDING_DIM=100, dropout=0.5):\n",
    "    word_index in word index ,\n",
    "    embeddings_matrix is embeddings_matrix, look at data_helper.py\n",
    "    nClasses is number of classes,\n",
    "    MAX_SEQUENCE_LENGTH is maximum lenght of text sequences\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    hidden_layer = 3\n",
    "    gru_node = 32\n",
    "    \n",
    "    model.add(Embedding(len(word_index) + 1,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embeddings_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=True))\n",
    "    print(gru_node)\n",
    "    for i in range(0,hidden_layer):\n",
    "        model.add(GRU(gru_node,return_sequences=True, recurrent_dropout=0.2))\n",
    "        model.add(Dropout(dropout))\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(GRU(gru_node, recurrent_dropout=0.2))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(nclasses, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                      optimizer='sgd',\n",
    "                      metrics=['accuracy'])\n",
    "    \n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GlNeKO-3QWrP",
    "outputId": "b58fe69c-92ca-4c24-b38a-f6bf0ac81ad4"
   },
   "outputs": [],
   "source": [
    "# Train the network and run classification\n",
    "model_RNN = Build_Model_RNN_Text(word_index,embedding_matrix, 55)\n",
    "run_classification(model_RNN, X_train_Glove, X_test_Glove, y_train, y_test,pipelineRequired = False,isDeepModel=True, arch_name='RNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MVAZo1u8QWrP"
   },
   "source": [
    "## RNN with LSTM networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gtFjAJ4oQWrQ"
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 200\n",
    "gloveFileName = 'glove.6B/glove.6B.100d.txt'\n",
    "#gloveFileName = '/content/glove.6B/glove.6B.200d.txt'\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, TimeDistributed, Activation\n",
    "from keras.layers import Flatten, Permute, merge, Input\n",
    "from keras.layers import Embedding\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, multiply, concatenate, Dropout\n",
    "from keras.layers import GRU, Bidirectional\n",
    "\n",
    "\n",
    "def Build_Model_LTSM_Text(word_index, embeddings_matrix, nclasses):\n",
    "    kernel_size = 2\n",
    "    filters = 256\n",
    "    pool_size = 2\n",
    "    gru_node = 256\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(word_index) + 1,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embeddings_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=True))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv1D(filters, kernel_size, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    model.add(Conv1D(filters, kernel_size, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    model.add(Conv1D(filters, kernel_size, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    model.add(Conv1D(filters, kernel_size, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    model.add(Bidirectional(LSTM(gru_node, return_sequences=True, recurrent_dropout=0.2)))\n",
    "    model.add(Bidirectional(LSTM(gru_node, return_sequences=True, recurrent_dropout=0.2)))\n",
    "    model.add(Bidirectional(LSTM(gru_node, return_sequences=True, recurrent_dropout=0.2)))\n",
    "    model.add(Bidirectional(LSTM(gru_node, recurrent_dropout=0.2)))\n",
    "    model.add(Dense(1024,activation='relu'))\n",
    "    model.add(Dense(nclasses))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u0973kvsQWrQ",
    "outputId": "4e22e215-9641-43d0-f8c9-7e47fe01ecff"
   },
   "outputs": [],
   "source": [
    "X_train_Glove,X_test_Glove, word_index,embeddings_index = loadData_Tokenizer(X_train,X_test,gloveFileName)\n",
    "embedding_matrix = buildEmbed_matrices(word_index,EMBEDDING_DIM)\n",
    "\n",
    "model_LTSM = Build_Model_LTSM_Text(word_index,embedding_matrix, 55)\n",
    "run_classification(model_LTSM, X_train_Glove, X_test_Glove, y_train, y_test,pipelineRequired = False,isDeepModel=True, arch_name='LSTM')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Recreated_16thDec.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
