{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "30Dec_updated.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guptapawan227/Capstone_AIML/blob/Pawan/31_Dec_Visualiztion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XycWjq1YtXnI"
      },
      "source": [
        "Mounting Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_AZpolni4uT",
        "outputId": "21f62fc5-056f-4193-a083-c56797f8a714"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IX79JvNILSQK",
        "outputId": "b69d261f-4a90-4ed1-fb65-8ef792cef7ec"
      },
      "source": [
        "!pip3 install ftfy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/e2/3b51c53dffb1e52d9210ebc01f1fb9f2f6eba9b3201fa971fd3946643c71/ftfy-5.8.tar.gz (64kB)\n",
            "\r\u001b[K     |█████▏                          | 10kB 27.2MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 20kB 33.1MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 30kB 21.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 40kB 20.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 51kB 21.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 61kB 15.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 8.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy) (0.2.5)\n",
            "Building wheels for collected packages: ftfy\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-5.8-cp36-none-any.whl size=45613 sha256=b765be24ee1364c73319bee5990f623350224d712f42d20156f64e6bae786ed8\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/c0/ef/f28c4da5ac84a4e06ac256ca9182fc34fa57fefffdbc68425b\n",
            "Successfully built ftfy\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-5.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeHuMI62tbyY"
      },
      "source": [
        "Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01_3Wr6nw-ee",
        "outputId": "724afd50-7c90-42c8-c171-6d5c8ecd6113"
      },
      "source": [
        "# Using TensorFlow 1.x only in colab as found a issue with 2.3 version used by colab while working with DNN model fit. Did not observe any issue with Tensor flow 2.1 version on local jupyter enviornment.\n",
        "%tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDYfqgy9q1p_",
        "outputId": "c6429a38-d2fa-48df-f15f-d08b345ab59e"
      },
      "source": [
        "import pandas as pd \n",
        "import numpy as np \n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import time, os, sys, itertools, re \n",
        "from PIL import Image\n",
        "import warnings, pickle, string\n",
        "from dateutil import parser\n",
        "%matplotlib inline\n",
        "\n",
        "# Data Visualization\n",
        "import cufflinks as cf\n",
        "import plotly as py\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import download_plotlyjs,init_notebook_mode,plot,iplot\n",
        "\n",
        "from ftfy import fix_text, badness\n",
        "\n",
        "# Traditional Modeling\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Sequential Modeling\n",
        "import keras.backend as K\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers.merge import Concatenate\n",
        "from keras.layers import Input, Dropout, Flatten, Dense, Embedding, LSTM, GRU\n",
        "from keras.layers import BatchNormalization, TimeDistributed, Conv1D, MaxPooling1D\n",
        "from keras.constraints import max_norm, unit_norm\n",
        "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "\n",
        "# Tools & Evaluation metrics\n",
        "from sklearn.metrics import confusion_matrix, classification_report, auc\n",
        "from sklearn.metrics import roc_curve, accuracy_score, precision_recall_curve\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjfS1VF6teuM"
      },
      "source": [
        "Reading the data from excel "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBwzB1oFYRiQ"
      },
      "source": [
        "## Language Translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n795kT7hYjSo"
      },
      "source": [
        "#### Load the consolidated final translated pickle file which contains the language translations. The Process used for language translation is commented below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GR5NR19Ml_RX",
        "outputId": "edcbc904-9772-444f-86dc-6bb39d893d70"
      },
      "source": [
        "!pip install langdetect"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.6/dist-packages (1.0.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from langdetect) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DP_kU29EuDcJ"
      },
      "source": [
        "with open('/content/drive/MyDrive/Capstone/Final_Translated_combined.pkl','rb') as f:\r\n",
        "#with open('Final_Translated_combined.pkl','rb') as f:\r\n",
        "    clean_data = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "d9nfJKbjl_RX"
      },
      "source": [
        "from langdetect import detect\n",
        "    \n",
        "def fn_lang_detect(df):                                        \n",
        "   try:                                                          \n",
        "      return detect(df)                                      \n",
        "   except:                                                       \n",
        "      return 'no'                                                  \n",
        "\n",
        "clean_data['language'] = clean_data['Clean_Description'].apply(fn_lang_detect)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "-ZHdJbzAl_RY",
        "outputId": "63024c9a-e97b-45fb-8d5b-6b0324ceee17"
      },
      "source": [
        "x = clean_data[\"language\"].value_counts()\n",
        "x=x.sort_index()\n",
        "plt.figure(figsize=(10,6))\n",
        "ax= sns.barplot(x.index, x.values, alpha=0.8)\n",
        "plt.title(\"Distribution of text by language\")\n",
        "plt.ylabel('number of records')\n",
        "plt.xlabel('Language')\n",
        "rects = ax.patches\n",
        "labels = x.values\n",
        "for rect, label in zip(rects, labels):\n",
        "    height = rect.get_height()\n",
        "    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-bb133365120d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"language\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbarplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Distribution of text by language\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'clean_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxPwaHbNl_RY"
      },
      "source": [
        "data1 = clean_data[(clean_data[\"language\"] != 'zh-cn') & (clean_data[\"language\"] != 'de')]\n",
        "data1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAqThg4Cl_RY"
      },
      "source": [
        "data1.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "ZJiv3zmKl_RZ",
        "outputId": "1bf67430-37dc-47a3-95d8-203d4cd17fec"
      },
      "source": [
        "# Load the translated pickle file \n",
        "#with open('/content/drive/MyDrive/Capstone_AIML_Pawan/clean_data_translated.pkl','rb') as f:\n",
        "with open('clean_data_translated.pkl','rb') as f:\n",
        "    clean_data_translated_pkl = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-285e77763eb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the translated pickle file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#with open('/content/drive/MyDrive/Capstone_AIML_Pawan/clean_data_translated.pkl','rb') as f:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'clean_data_translated.pkl'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mclean_data_translated_pkl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'clean_data_translated.pkl'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgN1ZPw7l_RZ"
      },
      "source": [
        "clean_data_translated_pkl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLLnBcaZl_RZ"
      },
      "source": [
        "clean_data_translated_pkl.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEVp8tQil_RZ"
      },
      "source": [
        "clean_data_translated_pkl=clean_data_translated_pkl.drop(['Clean_Description', 'Unnamed: 0'], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2_FonZQl_RZ"
      },
      "source": [
        "clean_data_translated_pkl.rename(columns = {'Translation_Text': 'Clean_Description'}, inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NalIG1Tbl_Ra"
      },
      "source": [
        "clean_data_translated_pkl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RI2-4HHAl_Ra"
      },
      "source": [
        "dataframes=[data1,clean_data_translated_pkl]\n",
        "clean_data_final_2= pd.concat(dataframes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKLNb0eKl_Ra"
      },
      "source": [
        "clean_data_final_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_UkDnQpl_Ra"
      },
      "source": [
        "clean_data_final_2.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gFS6JvMl_Ra"
      },
      "source": [
        "clean_data_final_2 = clean_data_final_2[(clean_data_final_2['Assignment group'] != 'GRP_32')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUwrhQFUl_Rb"
      },
      "source": [
        "clean_data_final_2.to_csv('clean_data_final_2.csv', index=False, encoding='utf_8_sig')\n",
        "#with open('/content/drive/MyDrive/Capstone/Final_Translated_combined.pkl','wb') as f:\n",
        "with open('Final_Translated_combined.pkl','wb') as f:\n",
        "    pickle.dump(clean_data_final_2, f, pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cY8Zf2Jml_Rb"
      },
      "source": [
        "# Load the consolidated final translated pickle file \n",
        "#with open('/content/drive/MyDrive/Capstone/Final_Translated_combined.pkl','rb') as f:\n",
        "with open('Final_Translated_combined.pkl','rb') as f:\n",
        "    clean_data = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNYuIWcuhqrR"
      },
      "source": [
        "clean_data.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpDjY6fmrFaW"
      },
      "source": [
        "assignment_group_cnt=clean_data['Assignment group'].value_counts()\n",
        "assignment_group_cnt.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6EQNYTUl_Rb"
      },
      "source": [
        "assignment_group_cnt.tail(50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7YH90Mpl_Rc"
      },
      "source": [
        "clean_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTZyoprphuPp"
      },
      "source": [
        "## Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baerbpKQl_Rc"
      },
      "source": [
        "#Install NLPAug Package\n",
        "!pip install nlpaug"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0-6dpb9l_Rc"
      },
      "source": [
        "#Install dependencies for nlpaug\n",
        "!pip install torch>=1.6.0 transformers>=4.0.0\n",
        "!pip install nltk>=3.4.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGSwjyh8l_Rd"
      },
      "source": [
        "#We will use Word Augmentation methods. nlpaug supports character, word and sentence level augmentation methods\n",
        "import nlpaug.augmenter.word as naw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuZ4jE3wl_Rd"
      },
      "source": [
        "#Word embedding augmentation method\n",
        "aug1 = naw.WordEmbsAug(model_type='glove', model_path='glove.6B/glove.6B.50d.txt', action=\"substitute\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "az3hq-Dyl_Rd"
      },
      "source": [
        "aug2 = naw.WordEmbsAug(model_type='glove', model_path='glove.6B/glove.6B.50d.txt', action=\"insert\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r43F4C79l_Re"
      },
      "source": [
        "#Contextual Word augmentation method\n",
        "aug3 = naw.ContextualWordEmbsAug(model_path='bert-base-uncased', action=\"substitute\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1A7xesPl_Rf"
      },
      "source": [
        "aug4 = naw.ContextualWordEmbsAug(model_path='roberta-base', action=\"substitute\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76HnIEB0l_Rf"
      },
      "source": [
        "#Synonym Augmentation method using PPDB models downloaded from http://paraphrase.org/#/download\n",
        "aug5 = naw.SynonymAug(aug_src='ppdb', model_path='ppdb-2.0-s-all')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_qdWy1yBsjO"
      },
      "source": [
        "#!pip3 install nltk\n",
        "import nltk \n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.corpus import wordnet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Erip7CBw7P_P"
      },
      "source": [
        "#Create a new dataframe with records not in GRP_0\n",
        "zero_dataframe = clean_data[clean_data[\"Assignment group\"] == 'GRP_0']\n",
        "new_dataframe = clean_data[clean_data[\"Assignment group\"] != 'GRP_0']\n",
        "zero_dataframe.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKEJcuR6l_Rg"
      },
      "source": [
        "#Create dataframe copies for different augmentation methods\n",
        "new_dataframe2 = new_dataframe.copy()\n",
        "new_dataframe3 = new_dataframe.copy()\n",
        "new_dataframe4 = new_dataframe.copy()\n",
        "new_dataframe5 = new_dataframe.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YB8ELEaPRyuQ"
      },
      "source": [
        "new_dataframe.shape, zero_dataframe.shape, new_dataframe2.shape, new_dataframe3.shape, new_dataframe4.shape, new_dataframe5.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUnTAlH_SXEW"
      },
      "source": [
        "maxsyn=2\n",
        "new_dataframe[\"Augmented_data\"] = new_dataframe.apply(lambda x: create_set_of_new_sentences(x['Clean_Description'], maxsyn),axis=1)\n",
        "new_dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WO4mY0HPl_Rh"
      },
      "source": [
        "new_dataframe[\"Augmented_data\"] = new_dataframe.apply(lambda x: aug1.augment(x['Clean_Description']),axis=1)\n",
        "new_dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThnwxmfATCun"
      },
      "source": [
        "s = new_dataframe.apply(lambda x: pd.Series(x['Augmented_data']), axis=1).stack().reset_index(level=1, drop=True)\n",
        "s.name = 'Final_Text'\n",
        "new_dataframe_aug = new_dataframe.drop(['New Description','Augmented_data', 'Clean_Description'],axis=1).join(s)\n",
        "new_dataframe_aug"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ynxpLJtl_Ri"
      },
      "source": [
        "new_dataframe2[\"Augmented_data\"] = new_dataframe2.apply(lambda x: aug2.augment(x['Clean_Description']),axis=1)\n",
        "new_dataframe2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNE8mFazl_Rj"
      },
      "source": [
        "s = new_dataframe2.apply(lambda x: pd.Series(x['Augmented_data']), axis=1).stack().reset_index(level=1, drop=True)\n",
        "s.name = 'Final_Text'\n",
        "new_dataframe_aug2 = new_dataframe2.drop(['New Description','Augmented_data', 'Clean_Description'],axis=1).join(s)\n",
        "new_dataframe_aug2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTnFa80_l_Rj"
      },
      "source": [
        "new_dataframe3[\"Augmented_data\"] = new_dataframe3.apply(lambda x: aug3.augment(x['Clean_Description']),axis=1)\n",
        "new_dataframe3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCavpekNl_Rj"
      },
      "source": [
        "s = new_dataframe3.apply(lambda x: pd.Series(x['Augmented_data']), axis=1).stack().reset_index(level=1, drop=True)\n",
        "s.name = 'Final_Text'\n",
        "new_dataframe_aug3 = new_dataframe3.drop(['New Description','Augmented_data', 'Clean_Description'],axis=1).join(s)\n",
        "new_dataframe_aug3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XF0JOdiZl_Rj"
      },
      "source": [
        "new_dataframe4[\"Augmented_data\"] = new_dataframe4.apply(lambda x: aug4.augment(x['Clean_Description']),axis=1)\n",
        "new_dataframe4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NED0cEDSl_Rk"
      },
      "source": [
        "s = new_dataframe4.apply(lambda x: pd.Series(x['Augmented_data']), axis=1).stack().reset_index(level=1, drop=True)\n",
        "s.name = 'Final_Text'\n",
        "new_dataframe_aug4 = new_dataframe4.drop(['New Description','Augmented_data', 'Clean_Description'],axis=1).join(s)\n",
        "new_dataframe_aug4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5EjVhuLl_Rk"
      },
      "source": [
        "new_dataframe5[\"Augmented_data\"] = new_dataframe5.apply(lambda x: aug5.augment(x['Clean_Description']),axis=1)\n",
        "new_dataframe5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZEQn0MTl_Rl"
      },
      "source": [
        "s = new_dataframe5.apply(lambda x: pd.Series(x['Augmented_data']), axis=1).stack().reset_index(level=1, drop=True)\n",
        "s.name = 'Final_Text'\n",
        "new_dataframe_aug5 = new_dataframe5.drop(['New Description','Augmented_data', 'Clean_Description'],axis=1).join(s)\n",
        "new_dataframe_aug5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcQ3e0XSBsjT"
      },
      "source": [
        "zero_dataframe = zero_dataframe.rename(columns={\"Clean_Description\": \"Final_Text\"})\n",
        "zero_dataframe = zero_dataframe.drop(['New Description'], axis = 1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxpmQJ6Il_Rl"
      },
      "source": [
        "zero_dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QghaEUNpl_Rl"
      },
      "source": [
        "#Adding Original data (without Augmentation)\n",
        "new_dataframe6 = clean_data[clean_data[\"Assignment group\"] != 'GRP_0']\n",
        "new_dataframe6 = new_dataframe6.rename(columns={\"Clean_Description\": \"Final_Text\"})\n",
        "new_dataframe6 = new_dataframe6.drop(['New Description'], axis = 1)\n",
        "new_dataframe6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dv-q94szl_Rm"
      },
      "source": [
        "dataframes=[new_dataframe_aug, new_dataframe_aug2, new_dataframe_aug3, new_dataframe_aug4, new_dataframe_aug5, zero_dataframe, new_dataframe6]\n",
        "clean_data_result= pd.concat(dataframes)\n",
        "clean_data_result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQz_WKp0l_Rn"
      },
      "source": [
        "#Remove duplicate rows after augmentation\n",
        "clean_data_result = clean_data_result.drop_duplicates(subset='Final_Text', keep=\"first\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFKf35cRQWq9"
      },
      "source": [
        "# Serialize the Augmented dataset for later use\n",
        "clean_data_result.to_csv('Interim_data.csv', index=False, encoding='utf_8_sig')\n",
        "#with open('/content/Interim_data.pkl','wb') as f:\n",
        "with open('Interim_data.pkl','wb') as f:\n",
        "    pickle.dump(clean_data_result, f, pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJHVM5Fql_Rn"
      },
      "source": [
        "# Load the consolidated final translated pickle file \n",
        "#with open('/content/drive/MyDrive/Capstone/Final_Translated_combined.pkl','rb') as f:\n",
        "with open('Interim_data.pkl','rb') as f:\n",
        "    clean_data_result = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtY9iC0el_Rn"
      },
      "source": [
        "clean_data_result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzxDdWyMBsjT"
      },
      "source": [
        "## Stop words removal and Lemmatise text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSfE-xBvrFaY"
      },
      "source": [
        "clean_data_result.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45rNIDGvBsjU"
      },
      "source": [
        "import re\n",
        "import string\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "processed_all_documents = list()\n",
        "\n",
        "for desc in clean_data_result['Final_Text']:\n",
        "    word_tokens = word_tokenize(desc) \n",
        "    \n",
        "    filtered_sentence = [] \n",
        "\n",
        "    # Removing Stopwords\n",
        "    for w in word_tokens: \n",
        "        if w not in stop_words: \n",
        "            filtered_sentence.append(w) \n",
        "\n",
        "    words = ' '.join(filtered_sentence)\n",
        "    processed_all_documents.append(words)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Dh0hMMrrFaZ"
      },
      "source": [
        "clean_data_result['Final_Text'] = processed_all_documents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xn4waA6MBsjU"
      },
      "source": [
        "clean_data_result.head(50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZ7s9d10EWG8"
      },
      "source": [
        "clean_data_result.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8YU9JQqEWG8"
      },
      "source": [
        "clean_data_result.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWJN-ekLBsjU",
        "scrolled": true
      },
      "source": [
        "#Lemmatisation using spacy library\n",
        "!pip install spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeoMfBEBBsjU"
      },
      "source": [
        "!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HWRcbMrrOgn"
      },
      "source": [
        "!pip3 install spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAGj1FplBsjV"
      },
      "source": [
        "# Need to run \"python -m spacy download en\" in anaconda prompt to avoid 'en' not found issue."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Pxl8SyyBsjV"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
        "allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']\n",
        "def lemmatize_text(text):\n",
        "    doc = nlp(text)\n",
        "    return ' '.join([token.lemma_ for token in doc])\n",
        "\n",
        "clean_data_result['Final_Text'] = clean_data_result['Final_Text'].apply(lemmatize_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0YLyD-iBsjV"
      },
      "source": [
        "clean_data_result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JVNAQ1Gl_Rs"
      },
      "source": [
        "clean_data_result1 = clean_data_result[clean_data_result['Final_Text'] != '']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VYbYUmsl_Rs"
      },
      "source": [
        "clean_data_result1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEhT90aSl_Rt"
      },
      "source": [
        "assignment_group_cnt=clean_data_result1['Assignment group'].value_counts()\n",
        "assignment_group_cnt.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0w9q5vkAl_Rt"
      },
      "source": [
        "assignment_group_cnt.tail(24)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yJf4QCZaoah"
      },
      "source": [
        "# Serialize the translated dataset\n",
        "clean_data_result1.to_csv('Final_data.csv', index=False, encoding='utf_8_sig')\n",
        "#with open('/content/Final_data.pkl','wb') as f:\n",
        "with open('Final_data.pkl','wb') as f:\n",
        "    pickle.dump(clean_data_result1, f, pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-zXZRYDaoah"
      },
      "source": [
        "# Load the translated pickle file \n",
        "#with open('/content/Final_data.pkl','rb') as f:\n",
        "with open('Final_data.pkl','rb') as f:\n",
        "    clean_data = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhCUgD6_Bsjn"
      },
      "source": [
        "## Prepping Dataframe for Model Building"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ijBA6KgW13C"
      },
      "source": [
        "clean_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRguKfOkEWHJ"
      },
      "source": [
        "'''# Import label encoder \n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "  \n",
        "# label_encoder object knows how to understand word labels. \n",
        "label_encoder = preprocessing.LabelEncoder() \n",
        "  \n",
        "# Encode labels in column 'species'. \n",
        "clean_data['Assignment group LabelEncoded']= label_encoder.fit_transform(clean_data['Assignment group']) \n",
        "  \n",
        "\n",
        "\n",
        "onehot_encoder = OneHotEncoder()\n",
        "clean_data['Assignment group LabelEncoded'] = clean_data['Assignment group LabelEncoded'].values.reshape(len(clean_data['Assignment group LabelEncoded']), 1)\n",
        "clean_data['Assignment group OneHotEncoded'] = onehot_encoder.fit_transform(clean_data[['Assignment group LabelEncoded']])\n",
        "clean_data['Assignment group OneHotEncoded']'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3WLAluVl_Rv"
      },
      "source": [
        "# Import label encoder \n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "  \n",
        "# label_encoder object knows how to understand word labels. \n",
        "label_encoder = preprocessing.LabelEncoder() \n",
        "  \n",
        "# Encode labels in column 'species'. \n",
        "clean_data['Assignment group LabelEncoded']= label_encoder.fit_transform(clean_data['Assignment group']) \n",
        "\n",
        "target_strings = label_encoder.inverse_transform(clean_data['Assignment group LabelEncoded'])\n",
        "target_strings_list = np.unique(target_strings).tolist()\n",
        "clean_data['Assignment group LabelEncoded']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odyEuJzEl_Rw"
      },
      "source": [
        "'''# Create a target categorical column\n",
        "clean_data['Assignment group LabelEncoded'] = clean_data['Assignment group'].astype('category').cat.codes\n",
        "clean_data.info()'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBnYl6rcl_Rw"
      },
      "source": [
        "clean_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlFNRnh1l_Rw"
      },
      "source": [
        "clean_data.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jsBDjGIBsjn"
      },
      "source": [
        "label_encoded_dict = dict(zip(clean_data['Assignment group'].unique(), clean_data['Assignment group LabelEncoded'].unique()))\n",
        "label_encoded_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaaV3zyqBsjo"
      },
      "source": [
        "# Splitting Train Test \n",
        "from sklearn.model_selection import train_test_split\n",
        "#Y = np.array(clean_data['Assignment group OneHotEncoded'])\n",
        "\n",
        "X_train, X_test, y_train1, y_test1 = train_test_split(clean_data['Final_Text'], clean_data['Assignment group LabelEncoded'], test_size=0.3, random_state = 0, stratify=clean_data['Assignment group LabelEncoded'])\n",
        "print('\\033[1mShape of the training set:\\033[0m', X_train.shape, X_test.shape)\n",
        "print('\\033[1mShape of the test set:\\033[0m', y_train.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdB5_mvzQWrN"
      },
      "source": [
        "#Using TFIDF vectors feature extraction\n",
        "Tfidf_vect = TfidfVectorizer(max_features=2500)\n",
        "Tfidf_vect.fit(clean_data.Final_Text.astype(str))\n",
        "X_train_tfidf = Tfidf_vect.transform(X_train)\n",
        "X_test_tfidf = Tfidf_vect.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3UGzor_Bsjo"
      },
      "source": [
        "#using count vectorizer for features extraction\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "CV = CountVectorizer(max_features=2500)\n",
        "\n",
        "X_BoW = CV.fit_transform(clean_data['Final_Text']).toarray()\n",
        "y = clean_data['Assignment group LabelEncoded']\n",
        "\n",
        "print(\"Shape of Input Feature :\",np.shape(X_BoW))\n",
        "print(\"Shape of Target Feature :\",np.shape(y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bp1F5vXyl_Rx"
      },
      "source": [
        "# Splitting Train Test \n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_BoW, y, test_size=0.3, random_state = 0, stratify=y)\n",
        "print('\\033[1mShape of the training set:\\033[0m', X_train.shape, X_test.shape)\n",
        "print('\\033[1mShape of the test set:\\033[0m', y_train.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIni9BjpBsjo"
      },
      "source": [
        "def run_classification(estimator, X_train, X_test, y_train, y_test, arch_name=None, pipelineRequired=True, isDeepModel=False):\n",
        "    # train the model\n",
        "    clf = estimator\n",
        "\n",
        "    if pipelineRequired :\n",
        "        clf = Pipeline([('tfidf', TfidfTransformer()),\n",
        "                     ('clf', estimator),\n",
        "                     ])\n",
        "      \n",
        "    if isDeepModel :\n",
        "        clf.fit(X_train, y_train, validation_data=(X_test, y_test),epochs=25, batch_size=128,verbose=1,callbacks=call_backs(arch_name))\n",
        "        # predict from the clasiffier\n",
        "        y_pred = clf.predict(X_test)\n",
        "        y_pred = np.argmax(y_pred, axis=1)\n",
        "        y_train_pred = clf.predict(X_train)\n",
        "        y_train_pred = np.argmax(y_train_pred, axis=1)\n",
        "    else :\n",
        "        clf.fit(X_train, y_train)\n",
        "        # predict from the clasiffier\n",
        "        y_pred = clf.predict(X_test)\n",
        "        y_train_pred = clf.predict(X_train)\n",
        "    \n",
        "    print('Estimator:', clf)\n",
        "    print('='*80)\n",
        "    print('Training accuracy: %.2f%%' % (accuracy_score(y_train,y_train_pred) * 100))\n",
        "    print('Testing accuracy: %.2f%%' % (accuracy_score(y_test, y_pred) * 100))\n",
        "    print('='*80)\n",
        "    print('Confusion matrix:\\n %s' % (confusion_matrix(y_test, y_pred)))\n",
        "    print('='*80)\n",
        "    print('Classification report:\\n %s' % (classification_report(y_test, y_pred, target_names=target_strings_list)))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxlH4wqyBsjo"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_RXTWZfBsjp"
      },
      "source": [
        "run_classification(LogisticRegression(C=5, penalty='l2', solver='liblinear'), X_train_tfidf, X_test_tfidf, y_train1, y_test1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMmg4gVll_Ry"
      },
      "source": [
        "run_classification(LogisticRegression(C=5, penalty='l2', solver='liblinear'), X_train, X_test, y_train, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7r6p2SGBsjp"
      },
      "source": [
        "## Naive Bayes Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hlgbYXyBsjp"
      },
      "source": [
        "run_classification(MultinomialNB(alpha=0, class_prior=None, fit_prior='True'), X_train_tfidf, X_test_tfidf, y_train1, y_test1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JarYQspdl_Rz"
      },
      "source": [
        "run_classification(MultinomialNB(alpha=0, class_prior=None, fit_prior='True'), X_train, X_test, y_train, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gs5ztA2kBsjp"
      },
      "source": [
        "## K-nearest Neighbor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4qwMKt7Bsjq"
      },
      "source": [
        "run_classification(KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='euclidean',\n",
        "                     metric_params=None, n_jobs=None, n_neighbors=2, p=2,\n",
        "                     weights='distance'), X_train_tfidf, X_test_tfidf, y_train1, y_test1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3jiu7Z4l_R0"
      },
      "source": [
        "run_classification(KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='euclidean',\n",
        "                     metric_params=None, n_jobs=None, n_neighbors=2, p=2,\n",
        "                     weights='distance'), X_train, X_test, y_train, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDMha2lxBsjq"
      },
      "source": [
        "## Support Vector Machine (SVM)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBc7yY11Bsjq"
      },
      "source": [
        "run_classification(LinearSVC(C=1), X_train_tfidf, X_test_tfidf, y_train1, y_test1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yQGz1JMl_R1"
      },
      "source": [
        "run_classification(LinearSVC(C=1), X_train, X_test, y_train, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDPisp5SBsjq"
      },
      "source": [
        "## Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyvv2qn5Bsjq"
      },
      "source": [
        "run_classification(DecisionTreeClassifier(criterion='gini', min_samples_leaf=2), X_train, X_test, y_train, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9iz2ueil_R2"
      },
      "source": [
        "run_classification(DecisionTreeClassifier(criterion='gini', min_samples_leaf=2), X_train_tfidf, X_test_tfidf, y_train1, y_test1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpJIipnsBsjr"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8b28KjuoBsjr"
      },
      "source": [
        "run_classification(RandomForestClassifier(criterion= 'entropy', n_estimators=100, random_state=0), X_train_tfidf, X_test_tfidf, y_train1, y_test1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hxfc5lG5l_R8"
      },
      "source": [
        "run_classification(RandomForestClassifier(criterion= 'entropy', n_estimators=100, random_state=0), X_train, X_test, y_train, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOuKESHfrFai"
      },
      "source": [
        "## GradientBoosting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJr4CJMBBsjr"
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "run_classification(GradientBoostingClassifier(max_depth=15, n_estimators=50, random_state=42), X_train_tfidf, X_test_tfidf, y_train1, y_test1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uqEe_Ixl_R9"
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "run_classification(GradientBoostingClassifier(max_depth=15, n_estimators=50, random_state=42), X_train, X_test, y_train, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-lWxfbVrFai"
      },
      "source": [
        "## XGBoosting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIYQ2WZJrFai"
      },
      "source": [
        "!pip install xgboost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8q7-fcTurFai"
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "run_classification(XGBClassifier(n_estimators=50, max_depth=15), X_train_tfidf, X_test_tfidf, y_train1, y_test1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MizYVnZrFai"
      },
      "source": [
        "## Bagging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPuCWrZGrFai"
      },
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "run_classification(BaggingClassifier(n_estimators=100, random_state=10), X_train_tfidf, X_test_tfidf, y_train1, y_test1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nd3PAGHmrFaj"
      },
      "source": [
        "## Stacking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfLNzU90rFaj"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "\n",
        "estimators = [('rf', RandomForestClassifier(n_estimators=100, random_state=42)), ('svr', make_pipeline(StandardScaler(with_mean=False), LinearSVC(random_state=42)))]\n",
        "\n",
        "run_classification(StackingClassifier(estimators=estimators, final_estimator=DecisionTreeClassifier()), X_train_tfidf, X_test_tfidf, y_train1, y_test1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCl8T1jNrFaj"
      },
      "source": [
        "## Voting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Q_lE-UerFaj"
      },
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "\n",
        "estimators = [('rf', RandomForestClassifier(criterion= 'entropy', n_estimators=100, random_state=42)), ('knn', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='euclidean',\n",
        "                     metric_params=None, n_jobs=None, n_neighbors=2, p=2,\n",
        "                     weights='distance')), ('bg', BaggingClassifier(n_estimators=100, random_state=42)), ('lsvc', LinearSVC(C=1, random_state=42))]\n",
        "\n",
        "run_classification(VotingClassifier(estimators=estimators, voting='hard'), X_train_tfidf, X_test_tfidf, y_train1, y_test1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVNcUZ1BQWrL"
      },
      "source": [
        "## Deep Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RelfajlQWrL"
      },
      "source": [
        "# Load the augmented data from pickle file \n",
        "#with open('/content/Interim_data.pkl','rb') as f:\n",
        "with open('Interim_data.pkl','rb') as f:\n",
        "    clean_data_DL = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15XBDLMRQWrL"
      },
      "source": [
        "clean_data_DL.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxgUQxvhQWrL"
      },
      "source": [
        "clean_data_DL['Final_Text'] = clean_data_DL['Final_Text'].replace(np.nan, '', regex=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzJ4VnECQWrM"
      },
      "source": [
        "clean_data_DL.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTiSKPsIr7RX"
      },
      "source": [
        "# Import label encoder \n",
        "from sklearn import preprocessing \n",
        "  \n",
        "# label_encoder object knows how to understand word labels. \n",
        "label_encoder = preprocessing.LabelEncoder() \n",
        "  \n",
        "# Encode labels in column 'species'. \n",
        "clean_data_DL['Assignment group LabelEncoded']= label_encoder.fit_transform(clean_data_DL['Assignment group']) \n",
        "  \n",
        "clean_data_DL['Assignment group LabelEncoded'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwiIGk4ar7gA"
      },
      "source": [
        "label_encoded_dict = dict(zip(clean_data_DL['Assignment group'].unique(), clean_data_DL['Assignment group LabelEncoded'].unique()))\n",
        "len(label_encoded_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibiTTlGLQWrM"
      },
      "source": [
        "# Splitting Train Test \n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(clean_data_DL['Final_Text'], clean_data_DL['Assignment group LabelEncoded'], test_size=0.3, random_state = 0, stratify=clean_data_DL['Assignment group LabelEncoded'])\n",
        "print('\\033[1mShape of the training set:\\033[0m', X_train.shape, X_test.shape)\n",
        "print('\\033[1mShape of the test set:\\033[0m', y_train.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GOVYYV9QWrM"
      },
      "source": [
        "### Create checkpoints function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RG6wVQXQWrN"
      },
      "source": [
        "#Path where you want to save the weights, model and checkpoints\n",
        "model_path = \"Weights/\"\n",
        "%mkdir Weights\n",
        "\n",
        "# Define model callbacks\n",
        "def call_backs(name):\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', mode='min', min_delta=0.01, patience=3)\n",
        "    model_checkpoint =  ModelCheckpoint(model_path + name + '_epoch{epoch:02d}_loss{val_loss:.4f}.h5',\n",
        "                                                               monitor='val_loss',\n",
        "                                                               verbose=1,\n",
        "                                                               save_best_only=True,\n",
        "                                                               save_weights_only=False,\n",
        "                                                               mode='min',\n",
        "                                                               period=1)\n",
        "    return [model_checkpoint, early_stopping]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAC5r5UcQWrN"
      },
      "source": [
        "# Function to build Neural Network\n",
        "def Build_Model_DNN_Text(shape, nClasses, dropout=0.3):\n",
        "    \"\"\"\n",
        "    buildModel_DNN_Tex(shape, nClasses,dropout)\n",
        "    Build Deep neural networks Model for text classification\n",
        "    Shape is input feature space\n",
        "    nClasses is number of classes\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "    node = 512 # number of nodes\n",
        "    nLayers = 4 # number of  hidden layer\n",
        "    model.add(Dense(node,input_dim=shape,activation='relu'))\n",
        "    model.add(Dropout(dropout))\n",
        "    for i in range(0,nLayers):\n",
        "        model.add(Dense(node,input_dim=node,activation='relu'))\n",
        "        model.add(Dropout(dropout))\n",
        "        model.add(BatchNormalization())\n",
        "    model.add(Dense(nClasses, activation='softmax'))\n",
        "    model.compile(loss='sparse_categorical_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "    print(model.summary())\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeuJUVsVl_SA"
      },
      "source": [
        "#from tensorflow.keras import Sequential\n",
        "#from tensorflow.keras.layers import Conv2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "\n",
        "Tfidf_vect = TfidfVectorizer(max_features=2500)\n",
        "Tfidf_vect.fit(clean_data_DL.Final_Text.astype(str))\n",
        "X_train_tfidf = Tfidf_vect.transform(X_train)\n",
        "X_test_tfidf = Tfidf_vect.transform(X_test)\n",
        "\n",
        "# Instantiate the network\n",
        "model_DNN = Build_Model_DNN_Text(X_train_tfidf.shape[1], 55)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6aC4UqOQWrN"
      },
      "source": [
        "run_classification(model_DNN, X_train_tfidf, X_test_tfidf, y_train, y_test,pipelineRequired = False,isDeepModel=True, arch_name='DNN')\n",
        "\n",
        "'''model_DNN.fit(X_train_tfidf, y_train,\n",
        "                              validation_data=(X_test_tfidf, y_test),\n",
        "                              callbacks=call_backs(\"NN\"),\n",
        "                              epochs=10,\n",
        "                              batch_size=128,\n",
        "                              verbose=2)\n",
        "predicted = model_DNN.predict(X_test_tfidf)'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaCQnFBRQWrO"
      },
      "source": [
        "### Extract Glove Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMA3MFYUQWrO"
      },
      "source": [
        "#download the glove embedding zip file from http://nlp.stanford.edu/data/wordvecs/glove.6B.zip\n",
        "from zipfile import ZipFile\n",
        "# Check if it is already extracted else Open the zipped file as readonly\n",
        "if not os.path.isfile('glove.6B/glove.6B.200d.txt'):\n",
        "    glove_embeddings = 'glove.6B.zip'\n",
        "    #glove_embeddings = '/content/drive/MyDrive/Capstone/glove.6B.zip'\n",
        "    with ZipFile(glove_embeddings, 'r') as archive:\n",
        "        archive.extractall('glove.6B')\n",
        "\n",
        "# List the files under extracted folder\n",
        "os.listdir('glove.6B')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2Xtecr3QWrO"
      },
      "source": [
        "## Convolutional Neural Networks (CNN)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghIAygEfQWrO"
      },
      "source": [
        "gloveFileName = 'glove.6B/glove.6B.200d.txt'\n",
        "#gloveFileName = '/content/glove.6B/glove.6B.200d.txt'\n",
        "MAX_SEQUENCE_LENGTH = 500\n",
        "EMBEDDING_DIM=200\n",
        "MAX_NB_WORDS=75000\n",
        "\n",
        "# Function to generate Embedding\n",
        "def loadData_Tokenizer(X_train, X_test,filename):\n",
        "    np.random.seed(7)\n",
        "    text = np.concatenate((X_train, X_test), axis=0)\n",
        "    text = np.array(text)\n",
        "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
        "    tokenizer.fit_on_texts(text)\n",
        "    sequences = tokenizer.texts_to_sequences(text)\n",
        "    word_index = tokenizer.word_index\n",
        "    text = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "    print('Found %s unique tokens.' % len(word_index))\n",
        "    indices = np.arange(text.shape[0])\n",
        "    # np.random.shuffle(indices)\n",
        "    text = text[indices]\n",
        "    print(text.shape)\n",
        "    X_train = text[0:len(X_train), ]\n",
        "    X_test = text[len(X_train):, ]\n",
        "    embeddings_index = {}\n",
        "    f = open(filename, encoding=\"utf8\")\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        try:\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "        except:\n",
        "            pass\n",
        "        embeddings_index[word] = coefs\n",
        "    f.close()\n",
        "    print('Total %s word vectors.' % len(embeddings_index))\n",
        "    return (X_train, X_test, word_index,embeddings_index)\n",
        "\n",
        "\n",
        "embedding_matrix = []\n",
        "\n",
        "def buildEmbed_matrices(word_index,embedding_dim):\n",
        "    embedding_matrix = np.random.random((len(word_index) + 1, embedding_dim))\n",
        "    for word, i in word_index.items():\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            # words not found in embedding index will be all-zeros.\n",
        "            if len(embedding_matrix[i]) !=len(embedding_vector):\n",
        "                print(\"could not broadcast input array from shape\",str(len(embedding_matrix[i])), \"into shape\",str(len(embedding_vector)),\n",
        "                      \" Please make sure your\"\" EMBEDDING_DIM is equal to embedding_vector file ,GloVe,\")\n",
        "                exit(1)\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "    return embedding_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DljFFfKxQWrO"
      },
      "source": [
        "# Generate Glove embedded datasets\n",
        "X_train_Glove, X_test_Glove, word_index, embeddings_index = loadData_Tokenizer(X_train,X_test,gloveFileName)\n",
        "embedding_matrix = buildEmbed_matrices(word_index,EMBEDDING_DIM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URiIMsVEQWrO"
      },
      "source": [
        "def Build_Model_CNN_Text(word_index, embeddings_matrix, nclasses,dropout=0.5):\n",
        "    \"\"\"\n",
        "        def buildModel_CNN(word_index, embeddings_index, nclasses, MAX_SEQUENCE_LENGTH=500, EMBEDDING_DIM=50, dropout=0.5):\n",
        "        word_index in word index ,\n",
        "        embeddings_index is embeddings index, look at data_helper.py\n",
        "        nClasses is number of classes,\n",
        "        MAX_SEQUENCE_LENGTH is maximum lenght of text sequences,\n",
        "        EMBEDDING_DIM is an int value for dimention of word embedding look at data_helper.py\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "    embedding_layer = Embedding(len(word_index) + 1,\n",
        "                                EMBEDDING_DIM,\n",
        "                                weights=[embeddings_matrix],\n",
        "                                input_length=MAX_SEQUENCE_LENGTH,\n",
        "                                trainable=True)\n",
        "    # applying a more complex convolutional approach\n",
        "    convs = []\n",
        "    filter_sizes = []\n",
        "    layer = 5\n",
        "    print(\"Filter  \",layer)\n",
        "    for fl in range(0,layer):\n",
        "        filter_sizes.append((fl+2))\n",
        "    node = 128\n",
        "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "    embedded_sequences = embedding_layer(sequence_input)\n",
        "    for fsz in filter_sizes:\n",
        "        l_conv = Conv1D(node, kernel_size=fsz, activation='relu')(embedded_sequences)\n",
        "        l_pool = MaxPooling1D(5)(l_conv)\n",
        "        #l_pool = Dropout(0.25)(l_pool)\n",
        "        convs.append(l_pool)\n",
        "    l_merge = Concatenate(axis=1)(convs)\n",
        "    l_cov1 = Conv1D(node, 5, activation='relu')(l_merge)\n",
        "    l_cov1 = Dropout(dropout)(l_cov1)\n",
        "    l_batch1 = BatchNormalization()(l_cov1)\n",
        "    l_pool1 = MaxPooling1D(5)(l_batch1)\n",
        "    l_cov2 = Conv1D(node, 5, activation='relu')(l_pool1)\n",
        "    l_cov2 = Dropout(dropout)(l_cov2)\n",
        "    l_batch2 = BatchNormalization()(l_cov2)\n",
        "    l_pool2 = MaxPooling1D(30)(l_batch2)\n",
        "    l_flat = Flatten()(l_pool2)\n",
        "    l_dense = Dense(1024, activation='relu')(l_flat)\n",
        "    l_dense = Dropout(dropout)(l_dense)\n",
        "    l_dense = Dense(512, activation='relu')(l_dense)\n",
        "    l_dense = Dropout(dropout)(l_dense)\n",
        "    preds = Dense(nclasses, activation='softmax')(l_dense)\n",
        "    model = Model(sequence_input, preds)\n",
        "    model.compile(loss='sparse_categorical_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    print(model.summary())\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxdeJLw9QWrP"
      },
      "source": [
        "# Train the network and run classification\n",
        "model_CNN = Build_Model_CNN_Text(word_index,embedding_matrix, 55)\n",
        "run_classification(model_CNN, X_train_Glove, X_test_Glove, y_train, y_test,pipelineRequired = False,isDeepModel=True, arch_name='CNN')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMu2ONzLQWrP"
      },
      "source": [
        "## Recurrent Neural Networks (RNN) --> Gated Recurrent Unit (GRU)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grFaHdizQWrP"
      },
      "source": [
        "def Build_Model_RNN_Text(word_index, embeddings_matrix, nclasses,dropout=0.5):\n",
        "    \"\"\"\n",
        "    def buildModel_RNN(word_index, embeddings_matrix, nclasses,  MAX_SEQUENCE_LENGTH=500, EMBEDDING_DIM=100, dropout=0.5):\n",
        "    word_index in word index ,\n",
        "    embeddings_matrix is embeddings_matrix, look at data_helper.py\n",
        "    nClasses is number of classes,\n",
        "    MAX_SEQUENCE_LENGTH is maximum lenght of text sequences\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "    hidden_layer = 3\n",
        "    gru_node = 32\n",
        "    \n",
        "    model.add(Embedding(len(word_index) + 1,\n",
        "                                EMBEDDING_DIM,\n",
        "                                weights=[embeddings_matrix],\n",
        "                                input_length=MAX_SEQUENCE_LENGTH,\n",
        "                                trainable=True))\n",
        "    print(gru_node)\n",
        "    for i in range(0,hidden_layer):\n",
        "        model.add(GRU(gru_node,return_sequences=True, recurrent_dropout=0.2))\n",
        "        model.add(Dropout(dropout))\n",
        "        model.add(BatchNormalization())\n",
        "    model.add(GRU(gru_node, recurrent_dropout=0.2))\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dense(nclasses, activation='softmax'))\n",
        "    model.compile(loss='sparse_categorical_crossentropy',\n",
        "                      optimizer='sgd',\n",
        "                      metrics=['accuracy'])\n",
        "    \n",
        "    print(model.summary())\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlNeKO-3QWrP"
      },
      "source": [
        "# Train the network and run classification\n",
        "model_RNN = Build_Model_RNN_Text(word_index,embedding_matrix, 55)\n",
        "run_classification(model_RNN, X_train_Glove, X_test_Glove, y_train, y_test,pipelineRequired = False,isDeepModel=True, arch_name='RNN')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVAZo1u8QWrP"
      },
      "source": [
        "## RNN with LSTM networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtFjAJ4oQWrQ"
      },
      "source": [
        "EMBEDDING_DIM = 200\n",
        "gloveFileName = 'glove.6B/glove.6B.100d.txt'\n",
        "#gloveFileName = '/content/glove.6B/glove.6B.200d.txt'\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, TimeDistributed, Activation\n",
        "from keras.layers import Flatten, Permute, merge, Input\n",
        "from keras.layers import Embedding\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, multiply, concatenate, Dropout\n",
        "from keras.layers import GRU, Bidirectional\n",
        "\n",
        "\n",
        "def Build_Model_LTSM_Text(word_index, embeddings_matrix, nclasses):\n",
        "    kernel_size = 2\n",
        "    filters = 256\n",
        "    pool_size = 2\n",
        "    gru_node = 256\n",
        "    \n",
        "    model = Sequential()\n",
        "    model.add(Embedding(len(word_index) + 1,\n",
        "                                EMBEDDING_DIM,\n",
        "                                weights=[embeddings_matrix],\n",
        "                                input_length=MAX_SEQUENCE_LENGTH,\n",
        "                                trainable=True))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Conv1D(filters, kernel_size, activation='relu'))\n",
        "    model.add(MaxPooling1D(pool_size=pool_size))\n",
        "    model.add(Conv1D(filters, kernel_size, activation='relu'))\n",
        "    model.add(MaxPooling1D(pool_size=pool_size))\n",
        "    model.add(Conv1D(filters, kernel_size, activation='relu'))\n",
        "    model.add(MaxPooling1D(pool_size=pool_size))\n",
        "    model.add(Conv1D(filters, kernel_size, activation='relu'))\n",
        "    model.add(MaxPooling1D(pool_size=pool_size))\n",
        "    model.add(Bidirectional(LSTM(gru_node, return_sequences=True, recurrent_dropout=0.2)))\n",
        "    model.add(Bidirectional(LSTM(gru_node, return_sequences=True, recurrent_dropout=0.2)))\n",
        "    model.add(Bidirectional(LSTM(gru_node, return_sequences=True, recurrent_dropout=0.2)))\n",
        "    model.add(Bidirectional(LSTM(gru_node, recurrent_dropout=0.2)))\n",
        "    model.add(Dense(1024,activation='relu'))\n",
        "    model.add(Dense(nclasses))\n",
        "    model.add(Activation('softmax'))\n",
        "    model.compile(loss='sparse_categorical_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    print(model.summary())\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0973kvsQWrQ"
      },
      "source": [
        "X_train_Glove,X_test_Glove, word_index,embeddings_index = loadData_Tokenizer(X_train,X_test,gloveFileName)\n",
        "embedding_matrix = buildEmbed_matrices(word_index,EMBEDDING_DIM)\n",
        "\n",
        "model_LTSM = Build_Model_LTSM_Text(word_index,embedding_matrix, 55)\n",
        "run_classification(model_LTSM, X_train_Glove, X_test_Glove, y_train, y_test,pipelineRequired = False,isDeepModel=True, arch_name='LSTM')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}