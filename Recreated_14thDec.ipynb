{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Recreated_14thDec.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guptapawan227/Capstone_AIML/blob/Ashish/Recreated_14thDec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XycWjq1YtXnI"
      },
      "source": [
        "Mounting Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_AZpolni4uT"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IX79JvNILSQK"
      },
      "source": [
        "!pip3 install ftfy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeHuMI62tbyY"
      },
      "source": [
        "Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01_3Wr6nw-ee"
      },
      "source": [
        "# Using TensorFlow 1.x only in colab as found a issue with 2.3 version used by colab while working with DNN model fit. Did not observe any issue with Tensor flow 2.1 version on local jupyter enviornment.\n",
        "%tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDYfqgy9q1p_"
      },
      "source": [
        "import pandas as pd \n",
        "import numpy as np \n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import time, os, sys, itertools, re \n",
        "from PIL import Image\n",
        "import warnings, pickle, string\n",
        "from dateutil import parser\n",
        "%matplotlib inline\n",
        "\n",
        "# Data Visualization\n",
        "import cufflinks as cf\n",
        "import plotly as py\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import download_plotlyjs,init_notebook_mode,plot,iplot\n",
        "\n",
        "from ftfy import fix_text, badness\n",
        "\n",
        "# Traditional Modeling\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Sequential Modeling\n",
        "import keras.backend as K\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers.merge import Concatenate\n",
        "from keras.layers import Input, Dropout, Flatten, Dense, Embedding, LSTM, GRU\n",
        "from keras.layers import BatchNormalization, TimeDistributed, Conv1D, MaxPooling1D\n",
        "from keras.constraints import max_norm, unit_norm\n",
        "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# Tools & Evaluation metrics\n",
        "from sklearn.metrics import confusion_matrix, classification_report, auc\n",
        "from sklearn.metrics import roc_curve, accuracy_score, precision_recall_curve\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjfS1VF6teuM"
      },
      "source": [
        "Reading the data from excel "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5aLjCsPjai0"
      },
      "source": [
        "data=pd.read_excel('/content/drive/MyDrive/Capstone/input_data.xlsx')\n",
        "#data=pd.read_excel('input_data.xlsx')\n",
        "data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvAO8EEEtmny"
      },
      "source": [
        "## Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXoojqxIEWGh"
      },
      "source": [
        "## Univariate visualization\n",
        "Single-variable or univariate visualization is the simplest type of visualization which consists of observations on only a single characteristic or attribute. Univariate visualization includes histogram, bar plots and line charts.\n",
        "\n",
        "### The distribution of Assignment groups\n",
        "Plots how the assignments groups are scattered across the dataset. The bar chart, histogram and pie chart tells the frequency of any ticket assigned to any group OR the tickets count for each group."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4HmTOOQqTg7"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXfghUHQjo95"
      },
      "source": [
        "assignment_group_count=data['Assignment group'].value_counts()\n",
        "assignment_group_count.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dd91Cg37qrN3"
      },
      "source": [
        "plt.subplots(figsize=(50,10))\n",
        "ax=sns.countplot(x='Assignment group', data=data)\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=30)\n",
        "plt.tight_layout\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1aYzasoBsjB"
      },
      "source": [
        "assignment_group_count.head(50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFeL_BzlBsjB"
      },
      "source": [
        "assignment_group_count.tail(24)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eAeh9jCj5xd"
      },
      "source": [
        "### Check Missing Values in dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LHjYyAk9b1a"
      },
      "source": [
        "data.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9u7hQHExVL1"
      },
      "source": [
        "data[data[\"Short description\"].isnull()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyohpEPk9icQ"
      },
      "source": [
        "### Copy Short Description to Description if the Description value is NaN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jL2xmtIN9EE3"
      },
      "source": [
        "data.Description.fillna(data[\"Short description\"], inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ea8mfm119PZZ"
      },
      "source": [
        "data[data[\"Description\"].isnull()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imMRGWAqeJQB"
      },
      "source": [
        "data['Short description'] = data['Short description'].replace(np.nan, '', regex=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCyUJGoMz2rT"
      },
      "source": [
        "data.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kHK1RmQEWGl"
      },
      "source": [
        "init_notebook_mode()\n",
        "cf.go_offline()\n",
        "\n",
        "# Assignment group distribution\n",
        "print('\\033[1mTotal assignment groups:\\033[0m', data['Assignment group'].nunique())\n",
        "\n",
        "# Histogram\n",
        "data['Assignment group'].iplot(\n",
        "    kind='hist',\n",
        "    xTitle='Assignment Group',\n",
        "    yTitle='count',\n",
        "    title='Assignment Group Distribution- Histogram (Fig-1)')\n",
        "\n",
        "# Pie chart\n",
        "assgn_grp = pd.DataFrame(data.groupby('Assignment group').size(),columns = ['Count']).reset_index()\n",
        "assgn_grp.iplot(\n",
        "    kind='pie', \n",
        "    labels='Assignment group', \n",
        "    values='Count', \n",
        "    title='Assignment Group Distribution- Pie Chart (Fig-2)', \n",
        "    hoverinfo=\"label+percent+name\", hole=0.25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5Bsf1i2EWGp"
      },
      "source": [
        "### Lets visualize the percentage of incidents per assignment group"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tqKaGygEWGp"
      },
      "source": [
        "# Plot to visualize the percentage data distribution across different groups\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.figure(figsize=(20,5))\n",
        "ax = sns.countplot(x=\"Assignment group\", data=data, order=data[\"Assignment group\"].value_counts().index)\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
        "for p in ax.patches:\n",
        "  ax.annotate(str(format(p.get_height()/len(data.index)*100, '.2f')+\"%\"), (p.get_x() + p.get_width() / 2., p.get_height()), ha = 'center', va = 'bottom', rotation=90, xytext = (0, 10), textcoords = 'offset points')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzt6MERbEWGp"
      },
      "source": [
        "### Top 20 and Bottom 20 assignment groups"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdWL2G_6EWGq"
      },
      "source": [
        "top_20 = data['Assignment group'].value_counts().nlargest(20).reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_42WL7ChEWGq"
      },
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "bars = plt.bar(top_20['index'],top_20['Assignment group'])\n",
        "plt.title('Top 20 Assignment groups with highest number of Tickets')\n",
        "plt.xlabel('Assignment Group')\n",
        "plt.xticks(rotation=90)\n",
        "plt.ylabel('Number of Tickets')\n",
        "\n",
        "for bar in bars:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x(), yval + .005, yval)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LK54m2f1EWGq"
      },
      "source": [
        "bottom_20 = data['Assignment group'].value_counts().nsmallest(20).reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3NUKDXWEWGq"
      },
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "bars = plt.bar(bottom_20['index'],bottom_20['Assignment group'])\n",
        "plt.title('Bottom 20 Assignment groups with small number of Tickets')\n",
        "plt.xlabel('Assignment Group')\n",
        "plt.xticks(rotation=90)\n",
        "plt.ylabel('Number of Tickets')\n",
        "for bar in bars:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x(), yval + .005, yval)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wt_wGlGxEWGr"
      },
      "source": [
        "### The distribution of Callers\n",
        "Plots how the callers are associated with tickets and what are the assignment groups they most frequently raise tickets for."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCyEHmv5EWGr"
      },
      "source": [
        "# Find out top 10 callers in terms of frequency of raising tickets in the entire dataset\n",
        "print('\\033[1mTotal caller count:\\033[0m', data['Caller'].nunique())\n",
        "df = pd.DataFrame(data.groupby(['Caller']).size().nlargest(10), columns=['Count']).reset_index()\n",
        "df.iplot(kind='pie',\n",
        "         labels='Caller', \n",
        "         values='Count', \n",
        "         title='Top 10 caller- Pie Chart (Fig-7)',\n",
        "         colorscale='-spectral',\n",
        "         pull=[0,0,0,0,0.05,0.1,0.15,0.2,0.25,0.3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRlvMWhqEWGs"
      },
      "source": [
        "### Top 5 callers in each assignment group"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blKl0U3wEWGs"
      },
      "source": [
        "top_n = 5\n",
        "s = data['Caller'].groupby(data['Assignment group']).value_counts()\n",
        "caller_grp = pd.DataFrame(s.groupby(level=0).nlargest(top_n).reset_index(level=0, drop=True))\n",
        "caller_grp.head(15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BNsOOO3EWGs"
      },
      "source": [
        "### The distribution of description lengths\n",
        "Plots the variation of length and word count of new description attribute"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMk1BMBDEWGs"
      },
      "source": [
        "data.insert(1, 'desc_len', data['Description'].astype(str).apply(len))\n",
        "data.insert(5, 'desc_word_count', data['Description'].apply(lambda x: len(str(x).split())))\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kypLEsJrEWGt"
      },
      "source": [
        "# Description text length\n",
        "data['desc_len'].iplot(\n",
        "    kind='bar',\n",
        "    xTitle='text length',\n",
        "    yTitle='count',\n",
        "    colorscale='-ylgn',\n",
        "    title='Description Text Length Distribution (Fig-11)')\n",
        "\n",
        "# Description word count\n",
        "data['desc_word_count'].iplot(\n",
        "    kind='bar',\n",
        "    xTitle='word count',\n",
        "    linecolor='black',\n",
        "    yTitle='count',\n",
        "    colorscale='-bupu',\n",
        "    title='Description Word Count Distribution (Fig-12)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coPKRzP7BsjE"
      },
      "source": [
        "## Create a rule based engine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNgvxqASBsjE"
      },
      "source": [
        "df_rules = pd.read_csv('/content/drive/MyDrive/Capstone/Rule_matrix.csv')\n",
        "#df_rules = pd.read_csv(\"Rule_matrix.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6-JgP7vBsjF"
      },
      "source": [
        "def applyRules(datadf,rulesdf,Description,ShortDescription):\n",
        "    datadf['pred_group'] = np.nan\n",
        "    for i, row in rulesdf.iterrows():                  \n",
        "        for j, row in datadf.iterrows():\n",
        "            if pd.notna(datadf[ShortDescription][j]):\n",
        "                if (('erp' in datadf[ShortDescription][j]) and (('EU_tool' in datadf[ShortDescription][j]))):\n",
        "                        datadf['pred_group'][j] = 'GRP_25'\n",
        "        for j, row in datadf.iterrows():\n",
        "            if pd.notna(datadf[Description][j]):\n",
        "                if (datadf[Description][j] == 'the'):\n",
        "                    datadf['pred_group'][j] = 'GRP_17' \n",
        "                \n",
        "                if (('finance_app' in ((datadf[ShortDescription][j]) or datadf[Description][j])) and ('HostName_1132' not in datadf[ShortDescription][j])):\n",
        "                    datadf['pred_group'][j] = 'GRP_55'\n",
        "                \n",
        "                if (('processor' in datadf[Description][j]) and ('engg' in datadf[Description][j])):\n",
        "                    datadf['pred_group'][j] = 'GRP_58'\n",
        "                \n",
        "                                     \n",
        "        if rulesdf['Short Desc Rule'][i] == 'begins with' and rulesdf['Desc Rule'][i] == 'begins with' and pd.isna(rulesdf['User'][i]):\n",
        "            for j, row in datadf.iterrows():\n",
        "                if pd.notna(datadf[ShortDescription][j]) and pd.notna(datadf[Description][j]):\n",
        "                    if ((datadf[ShortDescription][j].startswith(rulesdf['Short Dec Keyword'][i])) and (datadf[Description][j].startswith(rulesdf['Dec keyword'][i]))):\n",
        "                        datadf['pred_group'][j] = rulesdf['Group'][i]\n",
        "                        \n",
        "        if pd.isna(rulesdf['Short Desc Rule'][i]) and rulesdf['Desc Rule'][i] == 'begins with' and pd.notna(rulesdf['User'][i]):\n",
        "            for j, row in datadf.iterrows():\n",
        "                if pd.notna(datadf[Description][j]) and pd.notna(datadf['Caller'][j]):\n",
        "                    if ((datadf[Description][j].startswith(rulesdf['Desc Rule'][i]) and (rulesdf['User'][i] == datadf['Caller'][j]))):\n",
        "                        datadf['pred_group'][j] = rulesdf['Group'][i]\n",
        "                        \n",
        "        if rulesdf['Short Desc Rule'][i] == 'contains' and pd.notna(rulesdf['User'][i]):\n",
        "            for j, row in datadf.iterrows():\n",
        "                if (pd.notna(datadf[ShortDescription][j]) and pd.notna(datadf['Caller'][j])):\n",
        "                     if ((rulesdf['Short Dec Keyword'][i] in datadf[ShortDescription][j]) and (rulesdf['User'][i] == datadf['Caller'][j])):\n",
        "                        datadf['pred_group'][j] = rulesdf['Group'][i]\n",
        "        if rulesdf['Short Desc Rule'][i] == 'contains' and pd.isna(rulesdf['Desc Rule'][i]) and pd.isna(rulesdf['User'][i]):\n",
        "            for j, row in datadf.iterrows():\n",
        "                if pd.notna(datadf[ShortDescription][j]):\n",
        "                    if (rulesdf['Short Dec Keyword'][i] in datadf[ShortDescription][j]):\n",
        "                        datadf['pred_group'][j] = rulesdf['Group'][i]\n",
        "        if pd.isna(rulesdf['Short Desc Rule'][i]) and rulesdf['Desc Rule'][i] == 'begins with' and pd.isna(rulesdf['User'][i]):\n",
        "            for j, row in datadf.iterrows():\n",
        "                if pd.notna(datadf[Description][j]):\n",
        "                    if (datadf[Description][j].startswith(rulesdf['Dec keyword'][i])):\n",
        "                        datadf['pred_group'][j] = rulesdf['Group'][i]\n",
        "        if pd.isna(rulesdf['Short Desc Rule'][i]) and rulesdf['Desc Rule'][i] == 'contains' and pd.isna(rulesdf['User'][i]):\n",
        "            for j, row in datadf.iterrows():\n",
        "                if pd.notna(datadf[Description][j]):\n",
        "                    if (rulesdf['Dec keyword'][i] in datadf[Description][j]):\n",
        "                        datadf['pred_group'][j] = rulesdf['Group'][i]\n",
        "        if pd.isna(rulesdf['Short Desc Rule'][i]) and rulesdf['Desc Rule'][i] == 'not contain' and pd.isna(rulesdf['User'][i]):\n",
        "            for j, row in datadf.iterrows():\n",
        "                if pd.notna(datadf[Description][j]):\n",
        "                    if (rulesdf['Dec keyword'][i] in datadf[Description][j]):\n",
        "                        datadf['pred_group'][j] = rulesdf['Group'][i]\n",
        "\n",
        "\n",
        "        if rulesdf['Short Desc Rule'][i] == 'not contain' and pd.isna(rulesdf['Desc Rule'][i]) and pd.isna(rulesdf['User'][i]):\n",
        "            for j, row in datadf.iterrows():\n",
        "\n",
        "                if pd.notna(datadf[ShortDescription][j]):\n",
        "                    if (rulesdf['Short Dec Keyword'][i] in datadf[ShortDescription][j]):\n",
        "                        datadf['pred_group'][j] = rulesdf['Group'][i]\n",
        "        if pd.isna(rulesdf['Short Desc Rule'][i]) and rulesdf['Desc Rule'][i] == 'not contain' and pd.isna(rulesdf['User'][i]):\n",
        "            for j, row in datadf.iterrows():\n",
        "                if pd.notna(datadf[Description][j]):\n",
        "                    if (datadf[Description][j].startswith(rulesdf['Dec keyword'][i])):\n",
        "                        datadf['pred_group'][j] = rulesdf['Group'][i]\n",
        "        if pd.isna(rulesdf['Short Desc Rule'][i]) and rulesdf['Desc Rule'][i] == 'contains' and pd.isna(rulesdf['User'][i]):\n",
        "            for j, row in datadf.iterrows():\n",
        "                if pd.notna(datadf[Description][j]):\n",
        "                    if (rulesdf['Dec keyword'][i] in datadf[Description][j]):\n",
        "                        datadf['pred_group'][j] = rulesdf['Group'][i]\n",
        "\n",
        "    return datadf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bb6Q0RQGBsjG"
      },
      "source": [
        "rules_applied_df = applyRules(data,df_rules,'Description','Short description')\n",
        "rules_applied_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9NJjimoBsjH"
      },
      "source": [
        "rules_applied_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlLZAXimBsjH"
      },
      "source": [
        "rules_applied_df = rules_applied_df[(rules_applied_df['pred_group'].isna())]\n",
        "rules_applied_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAka7ylMBsjH"
      },
      "source": [
        "assignment_group_count=rules_applied_df['Assignment group'].value_counts()\n",
        "assignment_group_count.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jsmb0Q7u90k2"
      },
      "source": [
        "### Concatenate Short Description and Description Column into New Description, drop the previous columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEfP-wfh-0Nb"
      },
      "source": [
        "#Concatenate Short Description and Description columns\n",
        "rules_applied_df['New Description'] = rules_applied_df['Description'] + ' ' +rules_applied_df['Short description']\n",
        "\n",
        "clean_data=rules_applied_df.drop(['Short description', 'Description', 'pred_group', 'desc_len', 'desc_word_count'], axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVB_Dzw7bz-S"
      },
      "source": [
        "clean_data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWWXT5EnBsjI"
      },
      "source": [
        "## Fixing Garbled Text/ Mojibake using ftfy library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9bghLidBsjJ"
      },
      "source": [
        "# Write a function to apply to the dataset to detect Mojibakes\n",
        "def is_mojibake_impacted(text):\n",
        "    if not badness.sequence_weirdness(text):\n",
        "        # nothing weird, should be okay\n",
        "        return True\n",
        "    try:\n",
        "        text.encode('sloppy-windows-1252')\n",
        "    except UnicodeEncodeError:\n",
        "        # Not CP-1252 encodable, probably fine\n",
        "        return True\n",
        "    else:\n",
        "        # Encodable as CP-1252, Mojibake alert level high\n",
        "        return False\n",
        "# Check the dataset for mojibake impact\n",
        "clean_data[~clean_data.iloc[:,:].applymap(is_mojibake_impacted).all(1)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEYRzxjDBsjJ"
      },
      "source": [
        "# Take an example of row# 8471 Short Desc and fix it\n",
        "print('Grabled text: \\033[1m%s\\033[0m\\nFixed text: \\033[1m%s\\033[0m' % (clean_data['New Description'][8471], \n",
        "                                                                        fix_text(clean_data['New Description'][8471])))\n",
        "\n",
        "# List all mojibakes defined in ftfy library\n",
        "print('\\nMojibake Symbol RegEx:\\n', badness.MOJIBAKE_SYMBOL_RE.pattern)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxVgsJGRBsjJ"
      },
      "source": [
        "# Sanitize the dataset from Mojibakes\n",
        "clean_data['New Description'] = clean_data['New Description'].apply(fix_text)\n",
        "\n",
        "# Visualize that row# 8471\n",
        "clean_data.loc[8471]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4INDyU8-_RoR"
      },
      "source": [
        "## Cleaning & Processing the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXY-QspE_9e5"
      },
      "source": [
        "def date_validity(date_str):\n",
        "    try:\n",
        "        parser.parse(date_str)\n",
        "        return True\n",
        "    except:\n",
        "        return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zOXTxcwZ3b2"
      },
      "source": [
        "\n",
        "def process(text_string):\n",
        "    text=text_string.lower()\n",
        "    text_string = ' '.join([w for w in text_string.split() if not date_validity(w)])\n",
        "    text_string = re.sub(r\"received from:\",'',text_string)\n",
        "    text_string = re.sub(r\"from:\",' ',text_string)\n",
        "    text_string = re.sub(r\"to:\",' ',text_string)\n",
        "    text_string = re.sub(r\"subject:\",' ',text_string)\n",
        "    text_string = re.sub(r\"sent:\",' ',text_string)\n",
        "    text_string = re.sub(r\"ic:\",' ',text_string)\n",
        "    text_string = re.sub(r\"cc:\",' ',text_string)\n",
        "    text_string = re.sub(r\"bcc:\",' ',text_string)\n",
        "    text_string = re.sub(r'\\S*@\\S*\\s?', '', text_string)\n",
        "    text_string = re.sub(r'\\d+','' ,text_string)\n",
        "    text_string = re.sub(r'\\n',' ',text_string)\n",
        "    text_string = re.sub(r'#','', text_string)\n",
        "    text_string = re.sub(r'&;?', 'and',text_string)\n",
        "    text_string = re.sub(r'\\&\\w*;', '', text_string)\n",
        "    text_string = re.sub(r'https?:\\/\\/.*\\/\\w*', '', text_string)  \n",
        "    #text_string= ''.join(c for c in text_string if c <= '\\uFFFF') \n",
        "    text_string = text_string.strip()\n",
        "    #text_string = ' '.join(re.sub(\"[^\\u0030-\\u0039\\u0041-\\u005a\\u0061-\\u007a]\", \" \", text_string).split())\n",
        "    text_string = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', text_string)\n",
        "    text_string = re.sub(' +', ' ', text_string)\n",
        "    text_string = text_string.replace(r'\\b\\w\\b','').replace(r'\\s+', ' ')\n",
        "    text_string = text_string.strip()\n",
        "    return text_string\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UBm9uIvPr8f"
      },
      "source": [
        "clean_data[\"Clean_Description\"] = clean_data[\"New Description\"].apply(process)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CsFdd6BS1pw"
      },
      "source": [
        "clean_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBwzB1oFYRiQ"
      },
      "source": [
        "## Language Translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n795kT7hYjSo"
      },
      "source": [
        "#### Load the consolidated final translated pickle file which contains the language translations. The Process used for language translation is commented below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yVx3oMXVWG5"
      },
      "source": [
        "with open('/content/drive/MyDrive/Capstone/Final_Translated_combined.pkl','rb') as f:\r\n",
        "    clean_data = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKfqoJo1Vu4K"
      },
      "source": [
        "clean_data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4S-ah1_UfHjC"
      },
      "source": [
        "#!pip install langdetect"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJBjoH3Taoah"
      },
      "source": [
        "'''from langdetect import detect\n",
        "    \n",
        "def fn_lang_detect(df):                                        \n",
        "   try:                                                          \n",
        "      return detect(df)                                      \n",
        "   except:                                                       \n",
        "      return 'no'                                                  \n",
        "\n",
        "clean_data['language'] = clean_data['Clean_Description'].apply(fn_lang_detect)'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJo72j1taoah"
      },
      "source": [
        "'''x = clean_data[\"language\"].value_counts()\n",
        "x=x.sort_index()\n",
        "plt.figure(figsize=(10,6))\n",
        "ax= sns.barplot(x.index, x.values, alpha=0.8)\n",
        "plt.title(\"Distribution of text by language\")\n",
        "plt.ylabel('number of records')\n",
        "plt.xlabel('Language')\n",
        "rects = ax.patches\n",
        "labels = x.values\n",
        "for rect, label in zip(rects, labels):\n",
        "    height = rect.get_height()\n",
        "    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n",
        "plt.show();'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NJyCjPbsVYK"
      },
      "source": [
        "#clean_data = clean_data[(clean_data[\"language\"]!='zh-cn')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cg4GJAaFaoah"
      },
      "source": [
        "#!pip install goslate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnPGyLbMvrO5"
      },
      "source": [
        "'''import goslate\n",
        "gs = goslate.Goslate()\n",
        "def translate(text_string):\n",
        "  translation = gs.translate(text_string, 'en')\n",
        "  return translation'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cn2MDmaKaoah"
      },
      "source": [
        "We can see that most of the tickets are in english, followed by tickets in German language. We need to translate these into english."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cG2YtyrtYS63"
      },
      "source": [
        "#german_data = pd.read_csv(\"/content/drive/MyDrive/Capstone/german.csv\")\n",
        "#german_data = pd.read_csv('german.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWGhyshhZSZW"
      },
      "source": [
        "#german_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fr8nlGTyYkJD"
      },
      "source": [
        "#german_dictionary = german_data.to_dict(orient='records')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CD7tHLwMNcMZ"
      },
      "source": [
        "'''\n",
        "def translate_function(text):\n",
        "    translated_text = []\n",
        "    text_split = text.split()\n",
        "    for text in text_split:\n",
        "        word_found = False\n",
        "        for item in range(len(german_dictionary)):\n",
        "            if text == german_dictionary[item][\"German\"]:\n",
        "                translated_text.append(german_dictionary[item][\"English\"])\n",
        "                word_found = True\n",
        "        if word_found == False:\n",
        "            translated_text.append(text)\n",
        "    translate = ' '.join([word for word in translated_text])        \n",
        "    return translate\n",
        "\n",
        "clean_data[\"Translated Text\"] = clean_data[\"Clean_Description\"].apply(translate_function)'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIOtS7WzqL7S"
      },
      "source": [
        "#clean_data.tail(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkkDCx8kBsjO"
      },
      "source": [
        "#clean_data[clean_data.language == 'de']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTdxgtulZUWz"
      },
      "source": [
        "'''# Load the translated pickle file \r\n",
        "with open('/content/drive/MyDrive/Capstone/clean_data_translated.pkl','rb') as f:\r\n",
        "    clean_data_translated_pkl = pickle.load(f)'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQjKzZO5Z6XV"
      },
      "source": [
        "#clean_data_translated_pkl.rename(columns = {'Translation_Text':'Translated Text'}, inplace = True) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p87y7ieWZY7g"
      },
      "source": [
        "'''dataframes=[clean_data,clean_data_translated_pkl]\r\n",
        "clean_data_final_2= pd.concat(dataframes)'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyFJbrzUZsgg"
      },
      "source": [
        "'''clean_data_final_2.to_csv('clean_data_final_2.csv', index=False, encoding='utf_8_sig')\r\n",
        "with open('/content/drive/MyDrive/Capstone/Final_Translated_combined.pkl','wb') as f:\r\n",
        "    pickle.dump(clean_data_final_2, f, pickle.HIGHEST_PROTOCOL)'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqyJbbOPb_q2"
      },
      "source": [
        "'''# Load the consolidated final translated pickle file \r\n",
        "with open('/content/drive/MyDrive/Capstone/Final_Translated_combined.pkl','rb') as f:\r\n",
        "    clean_data = pickle.load(f)'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cnS3dFecMg5"
      },
      "source": [
        "#clean_data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bs3QV2wmpJX"
      },
      "source": [
        "#clean_data = clean_data[clean_data['Assignment group'].map(clean_data['Assignment group'].value_counts()) > 15]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5n8SxQn-VMjJ"
      },
      "source": [
        "'''clean_data.to_csv('clean_data.csv', index=False, encoding='utf_8_sig')\r\n",
        "with open('/content/drive/MyDrive/Capstone/Final_Translated_combined.pkl','wb') as f:\r\n",
        "    pickle.dump(clean_data, f, pickle.HIGHEST_PROTOCOL)'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5W1Mf97BsjO"
      },
      "source": [
        "## Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_qdWy1yBsjO"
      },
      "source": [
        "!pip3 install nltk\n",
        "import nltk \n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import wordnet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gt5RRxDkBsjO"
      },
      "source": [
        "from collections import OrderedDict\n",
        "from nltk.tokenize import word_tokenize\n",
        "def find_synonyms(word):\n",
        "  synonyms = []\n",
        "  for synset in wordnet.synsets(word):\n",
        "    for syn in synset.lemma_names():\n",
        "      synonyms.append(syn)\n",
        "\n",
        "  # using this to drop duplicates while maintaining word order (closest synonyms comes first)\n",
        "  synonyms_without_duplicates = list(OrderedDict.fromkeys(synonyms))\n",
        "  return synonyms_without_duplicates"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkwvCo2YBsjP"
      },
      "source": [
        "def create_set_of_new_sentences(sentence, max_syn_per_word = 3):\n",
        "  count = 0\n",
        "  new_sentences = []\n",
        "  for word in word_tokenize(sentence):\n",
        "    if len(word)<=3 : continue \n",
        "    for synonym in find_synonyms(word)[0:max_syn_per_word]:\n",
        "      synonym = synonym.replace('_', ' ') #restore space character\n",
        "      new_sentence = sentence.replace(word,synonym)\n",
        "      if count <= 4:\n",
        "        new_sentences.append(new_sentence)\n",
        "        count += 1    \n",
        "  return new_sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpdCaFhhBsjP"
      },
      "source": [
        "med_records=['GRP_8','GRP_3','GRP_12','GRP_2','GRP_13','GRP_19']\n",
        "\n",
        "low_records=['GRP_24','GRP_9','GRP_6','GRP_10','GRP_5','GRP_14','GRP_25','GRP_33','GRP_4','GRP_29','GRP_18','GRP_16','GRP_17','GRP_31','GRP_7','GRP_34','GRP_26','GRP_40','GRP_28','GRP_41'\n",
        ",'GRP_15','GRP_30','GRP_42','GRP_20','GRP_45','GRP_22','GRP_1','GRP_11']\n",
        "\n",
        "vlow_records =['GRP_21','GRP_47','GRP_23','GRP_62','GRP_48','GRP_60','GRP_39','GRP_27','GRP_37','GRP_44','GRP_36','GRP_50','GRP_53','GRP_65','GRP_53','GRP_52','GRP_55','GRP_51','GRP_59','GRP_49','GRP_46','GRP_43','GRP_66','GRP_32','GRP_63','GRP_58','GRP_56','GRP_38','GRP_68','GRP_69','GRP_57','GRP_72','GRP_71','GRP_54','GRP_35','GRP_64','GRP_70','GRP_61','GRP_67','GRP_73']\n",
        "\n",
        "clean_data1 = clean_data[clean_data[\"Assignment group\"].isin(med_records)]\n",
        "clean_data2 = clean_data[clean_data[\"Assignment group\"].isin(low_records)]\n",
        "clean_data3 = clean_data[clean_data[\"Assignment group\"] .isin(vlow_records)]\n",
        "\n",
        "clean_data4 = clean_data[clean_data[\"Assignment group\"] == 'GRP_0']\n",
        "clean_data4['Final_Text'] = clean_data4['Translated Text']\n",
        "clean_data4 = clean_data4.drop(['New Description','Clean_Description','Translated Text'],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooIwNETeeB-O"
      },
      "source": [
        "Do after this.... "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bhou5LYdBsjP"
      },
      "source": [
        "clean_data1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTPAApjrBsjP"
      },
      "source": [
        "clean_data2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPFm5iEjBsjQ"
      },
      "source": [
        "clean_data3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLjiFF9UBsjQ"
      },
      "source": [
        "clean_data4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEKEZ-wgBsjQ"
      },
      "source": [
        "maxsyn=1\n",
        "#clean_data1[\"Augmented_data\"] = clean_data1[\"Translated Text\"].apply(create_set_of_new_sentences)\n",
        "clean_data1[\"Augmented_data\"] = clean_data1.apply(lambda x: create_set_of_new_sentences(x['Translated Text'], maxsyn),axis=1)\n",
        "\n",
        "clean_data1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Sw-WgxIBsjQ"
      },
      "source": [
        "s = clean_data1.apply(lambda x: pd.Series(x['Augmented_data']), axis=1).stack().reset_index(level=1, drop=True)\n",
        "s.name = 'Final_Text'\n",
        "clean_data_aug1 = clean_data1.drop(['New Description','Augmented_data', 'Clean_Description', 'Translated Text'],axis=1).join(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dr94fyDFBsjQ"
      },
      "source": [
        "# Assignment group distribution\n",
        "print('\\033[1mTotal assignment groups:\\033[0m', clean_data_aug1['Assignment group'].nunique())\n",
        "\n",
        "# Histogram\n",
        "clean_data_aug1['Assignment group'].iplot(\n",
        "    kind='hist',\n",
        "    xTitle='Assignment Group',\n",
        "    yTitle='count',\n",
        "    title='Assignment Group Distribution- Histogram (Fig-1)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieJJ8qh9BsjR"
      },
      "source": [
        "maxsyn=6\n",
        "clean_data2[\"Augmented_data\"] = clean_data2.apply(lambda x: create_set_of_new_sentences(x['Translated Text'], maxsyn),axis=1)\n",
        "\n",
        "clean_data2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsv_Z2M7BsjR"
      },
      "source": [
        "s = clean_data2.apply(lambda x: pd.Series(x['Augmented_data']), axis=1).stack().reset_index(level=1, drop=True)\n",
        "s.name = 'Final_Text'\n",
        "clean_data_aug2 = clean_data2.drop(['New Description','Augmented_data', 'Clean_Description', 'Translated Text'],axis=1).join(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnmS6sRVBsjR"
      },
      "source": [
        "# Assignment group distribution\n",
        "print('\\033[1mTotal assignment groups:\\033[0m', clean_data_aug2['Assignment group'].nunique())\n",
        "\n",
        "# Histogram\n",
        "clean_data_aug2['Assignment group'].iplot(\n",
        "    kind='hist',\n",
        "    xTitle='Assignment Group',\n",
        "    yTitle='count',\n",
        "    title='Assignment Group Distribution- Histogram (Fig-2)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOjhKDMhBsjR"
      },
      "source": [
        "maxsyn=10\n",
        "clean_data3[\"Augmented_data\"] = clean_data3.apply(lambda x: create_set_of_new_sentences(x['Translated Text'], maxsyn),axis=1)\n",
        "\n",
        "clean_data3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHadfd3vBsjS"
      },
      "source": [
        "s = clean_data3.apply(lambda x: pd.Series(x['Augmented_data']), axis=1).stack().reset_index(level=1, drop=True)\n",
        "s.name = 'Final_Text'\n",
        "clean_data_aug3 = clean_data3.drop(['New Description','Augmented_data', 'Clean_Description', 'Translated Text'],axis=1).join(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fP3-sKnlBsjS",
        "scrolled": true
      },
      "source": [
        "# Assignment group distribution\n",
        "print('\\033[1mTotal assignment groups:\\033[0m', clean_data_aug3['Assignment group'].nunique())\n",
        "\n",
        "# Histogram\n",
        "clean_data_aug3['Assignment group'].iplot(\n",
        "    kind='hist',\n",
        "    xTitle='Assignment Group',\n",
        "    yTitle='count',\n",
        "    title='Assignment Group Distribution- Histogram (Fig-4)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STc8Fk5DBsjS"
      },
      "source": [
        "'''maxsyn=1\n",
        "clean_data4[\"Augmented_data\"] = clean_data4.apply(lambda x: create_set_of_new_sentences(x['Translated Text'], maxsyn),axis=1)\n",
        "\n",
        "clean_data4'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_jw8J77BsjS"
      },
      "source": [
        "'''s = clean_data4.apply(lambda x: pd.Series(x['Augmented_data']), axis=1).stack().reset_index(level=1, drop=True)\n",
        "s.name = 'Final_Text'\n",
        "clean_data_aug4 = clean_data4.drop(['New Description','Augmented_data', 'Clean_Description', 'Translated Text'],axis=1).join(s)'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nDaLJoFBsjS"
      },
      "source": [
        "'''# Assignment group distribution\n",
        "print('\\033[1mTotal assignment groups:\\033[0m', clean_data_aug4['Assignment group'].nunique())\n",
        "\n",
        "# Histogram\n",
        "clean_data_aug4['Assignment group'].iplot(\n",
        "    kind='hist',\n",
        "    xTitle='Assignment Group',\n",
        "    yTitle='count',\n",
        "    title='Assignment Group Distribution- Histogram (Fig-5)')'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EScjYfrBsjT"
      },
      "source": [
        "clean_data_mod4 = clean_data4.drop(['New Description', 'Clean_Description'],axis=1)\n",
        "clean_data_mod4.rename(columns={'Translated Text': 'Final_Text'}, inplace=True)\n",
        "clean_data_mod4.head()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcQ3e0XSBsjT"
      },
      "source": [
        "dataframes=[clean_data_aug1,clean_data_aug2,clean_data_aug3,clean_data4]\n",
        "#dataframes=[clean_data_aug1,clean_data_aug2,clean_data_aug3]\n",
        "clean_data_result= pd.concat(dataframes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVwHlgzVBsjT"
      },
      "source": [
        "# Assignment group distribution\n",
        "print('\\033[1mTotal assignment groups:\\033[0m', clean_data_result['Assignment group'].nunique())\n",
        "\n",
        "# Histogram\n",
        "clean_data_result['Assignment group'].iplot(\n",
        "    kind='hist',\n",
        "    xTitle='Assignment Group',\n",
        "    yTitle='count',\n",
        "    title='Assignment Group Distribution- Histogram (Fig-5)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmyviySPhD0z"
      },
      "source": [
        "clean_data_result.shape, clean_data_aug1.shape, clean_data_aug2.shape, clean_data_aug3.shape, clean_data4.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFKf35cRQWq9"
      },
      "source": [
        "# Serialize the Augmented dataset for later use\n",
        "clean_data_result.to_csv('Interim_data.csv', index=False, encoding='utf_8_sig')\n",
        "with open('/content/Interim_data.pkl','wb') as f:\n",
        "    pickle.dump(clean_data_result, f, pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzxDdWyMBsjT"
      },
      "source": [
        "## Stop words removal and Lemmatise text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45rNIDGvBsjU"
      },
      "source": [
        "#Stop words removal\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "sr = stopwords.words('english')\n",
        "for i,text in enumerate(clean_data_result['Final_Text']):\n",
        "  try:\n",
        "    clean_data_result['Final_Text'][i]=\" \".join(word for word in text.split(' ') if word not in sr)\n",
        "  except:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xn4waA6MBsjU"
      },
      "source": [
        "clean_data_result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZ7s9d10EWG8"
      },
      "source": [
        "clean_data_result.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8t0_Di2EWG8"
      },
      "source": [
        "clean_data_result['Final_Text'] = clean_data_result['Final_Text'].replace(np.nan, '', regex=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8YU9JQqEWG8"
      },
      "source": [
        "clean_data_result.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWJN-ekLBsjU",
        "scrolled": true
      },
      "source": [
        "#Lemmatisation using spacy library\n",
        "!pip install spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeoMfBEBBsjU"
      },
      "source": [
        "!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HWRcbMrrOgn"
      },
      "source": [
        "!pip3 install spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAGj1FplBsjV"
      },
      "source": [
        "# Need to run \"python -m spacy download en\" in anaconda prompt to avoid 'en' not found issue."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Pxl8SyyBsjV"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
        "allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']\n",
        "def lemmatize_text(text):\n",
        "    doc = nlp(text)\n",
        "    return ' '.join([token.lemma_ for token in doc])\n",
        "\n",
        "clean_data_result['Final_Text'] = clean_data_result['Final_Text'].apply(lemmatize_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0YLyD-iBsjV"
      },
      "source": [
        "clean_data_result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hCn99wvBsjV"
      },
      "source": [
        "### Attempt to use Google Translate library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yJf4QCZaoah"
      },
      "source": [
        "# Serialize the translated dataset\n",
        "clean_data_result.to_csv('Final_data.csv', index=False, encoding='utf_8_sig')\n",
        "with open('Final_data.pkl','wb') as f:\n",
        "    pickle.dump(clean_data_result, f, pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-zXZRYDaoah"
      },
      "source": [
        "# Load the translated pickle file \n",
        "with open('/content/Final_data.pkl','rb') as f:\n",
        "    clean_data = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MufrYo4zaoah"
      },
      "source": [
        "### Univariate visualization\n",
        "Single-variable or univariate visualization is the simplest type of visualization which consists of observations on only a single characteristic or attribute. Univariate visualization includes histogram, bar plots and line charts.\n",
        "\n",
        "#### The distribution of Assignment groups\n",
        "Plots how the assignments groups are scattered across the dataset. The bar chart, histogram and pie chart tells the frequency of any ticket assigned to any group OR the tickets count for each group."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEzuZQTdaoah"
      },
      "source": [
        "# Assignment group distribution\n",
        "print('\\033[1mTotal assignment groups:\\033[0m', clean_data['Assignment group'].nunique())\n",
        "\n",
        "# Histogram\n",
        "clean_data['Assignment group'].iplot(\n",
        "    kind='hist',\n",
        "    xTitle='Assignment Group',\n",
        "    yTitle='count',\n",
        "    title='Assignment Group Distribution- Histogram (Fig-1)')\n",
        "\n",
        "# Pie chart\n",
        "assgn_grp = pd.DataFrame(clean_data.groupby('Assignment group').size(),columns = ['Count']).reset_index()\n",
        "assgn_grp.iplot(\n",
        "    kind='pie', \n",
        "    labels='Assignment group', \n",
        "    values='Count', \n",
        "    title='Assignment Group Distribution- Pie Chart (Fig-2)', \n",
        "    hoverinfo=\"label+percent+name\", hole=0.25)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Hn2Iwq_aoah"
      },
      "source": [
        "### Lets visualize the percentage of incidents per assignment group"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fw4H53FGaoah"
      },
      "source": [
        "# Plot to visualize the percentage data distribution across different groups\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.figure(figsize=(20,5))\n",
        "ax = sns.countplot(x=\"Assignment group\", data=clean_data, order=clean_data[\"Assignment group\"].value_counts().index)\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
        "for p in ax.patches:\n",
        "  ax.annotate(str(format(p.get_height()/len(clean_data.index)*100, '.2f')+\"%\"), (p.get_x() + p.get_width() / 2., p.get_height()), ha = 'center', va = 'bottom', rotation=90, xytext = (0, 10), textcoords = 'offset points')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HRA85rBaoah"
      },
      "source": [
        "top_20 = clean_data['Assignment group'].value_counts().nlargest(20).reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mk193_BAaoah"
      },
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "bars = plt.bar(top_20['index'],top_20['Assignment group'])\n",
        "plt.title('Top 20 Assignment groups with highest number of Tickets')\n",
        "plt.xlabel('Assignment Group')\n",
        "plt.xticks(rotation=90)\n",
        "plt.ylabel('Number of Tickets')\n",
        "\n",
        "for bar in bars:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x(), yval + .005, yval)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oL1odSqxaoah"
      },
      "source": [
        "bottom_20 = clean_data['Assignment group'].value_counts().nsmallest(20).reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKTbEME0aoah"
      },
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "bars = plt.bar(bottom_20['index'],bottom_20['Assignment group'])\n",
        "plt.title('Bottom 20 Assignment groups with small number of Tickets')\n",
        "plt.xlabel('Assignment Group')\n",
        "plt.xticks(rotation=90)\n",
        "plt.ylabel('Number of Tickets')\n",
        "for bar in bars:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x(), yval + .005, yval)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTEdYvB4aoah"
      },
      "source": [
        "#### The distribution of Callers\n",
        "Plots how the callers are associated with tickets and what are the assignment groups they most frequently raise tickets for."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DF-i2z1aoah"
      },
      "source": [
        "# Find out top 10 callers in terms of frequency of raising tickets in the entire dataset\n",
        "print('\\033[1mTotal caller count:\\033[0m', clean_data['Caller'].nunique())\n",
        "df = pd.DataFrame(clean_data.groupby(['Caller']).size().nlargest(10), columns=['Count']).reset_index()\n",
        "df.iplot(kind='pie',\n",
        "         labels='Caller', \n",
        "         values='Count', \n",
        "         title='Top 10 caller- Pie Chart (Fig-7)',\n",
        "         colorscale='-spectral',\n",
        "         pull=[0,0,0,0,0.05,0.1,0.15,0.2,0.25,0.3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTiANuJnaoah"
      },
      "source": [
        "# Top 5 callers in each assignment group\n",
        "top_n = 5\n",
        "s = clean_data['Caller'].groupby(clean_data['Assignment group']).value_counts()\n",
        "caller_grp = pd.DataFrame(s.groupby(level=0).nlargest(top_n).reset_index(level=0, drop=True))\n",
        "caller_grp.head(15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbHn-V3Maoah"
      },
      "source": [
        "#### The distribution of description lengths\n",
        "Plots the variation of length and word count of new description attribute"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdrJgg48aoai"
      },
      "source": [
        "clean_data.insert(1, 'desc_len', clean_data['Final_Text'].astype(str).apply(len))\n",
        "clean_data.insert(5, 'desc_word_count', clean_data['Final_Text'].apply(lambda x: len(str(x).split())))\n",
        "clean_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38p2Thiaaoai"
      },
      "source": [
        "# Description text length\n",
        "clean_data['desc_len'].iplot(\n",
        "    kind='bar',\n",
        "    xTitle='text length',\n",
        "    yTitle='count',\n",
        "    colorscale='-ylgn',\n",
        "    title='Description Text Length Distribution (Fig-11)')\n",
        "\n",
        "# Description word count\n",
        "clean_data['desc_word_count'].iplot(\n",
        "    kind='bar',\n",
        "    xTitle='word count',\n",
        "    linecolor='black',\n",
        "    yTitle='count',\n",
        "    colorscale='-bupu',\n",
        "    title='Description Word Count Distribution (Fig-12)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qTz5tP8aoai"
      },
      "source": [
        "### N-Grams\n",
        "N-gram is a contiguous sequence of N items from a given sample of text or speech, in the fields of computational linguistics and probability. The items can be phonemes, syllables, letters, words or base pairs according to the application. N-grams are used to describe the number of words used as observation points, e.g., unigram means singly-worded, bigram means 2-worded phrase, and trigram means 3-worded phrase. \n",
        "\n",
        "We'll be using scikit-learn’s CountVectorizer function to derive n-grams and compare them before and after removing stop words. Stop words are a set of commonly used words in any language. We'll be using english corpus stopwords and extend it to include some business specific common words considered to be stop words in our case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2c0ivkwEaoai"
      },
      "source": [
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Extend the English Stop Wordss\n",
        "STOP_WORDS = STOPWORDS.union({'yes','na','hi',\n",
        "                              'receive','hello',\n",
        "                              'regards','thanks',\n",
        "                              'from','greeting',\n",
        "                              'forward','reply',\n",
        "                              'will','please',\n",
        "                              'see','help','able'})\n",
        "\n",
        "# Generic function to derive top N n-grams from the corpus\n",
        "def get_top_n_ngrams(corpus, top_n=None, ngram_range=(1,1), stopwords=None):\n",
        "    vec = CountVectorizer(ngram_range=ngram_range, \n",
        "                          stop_words=stopwords).fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0) \n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "    return words_freq[:top_n]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BaGo4WSaoai"
      },
      "source": [
        "### Top Unigrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pejZdQiTaoai"
      },
      "source": [
        "# Top 50 Unigrams before removing stop words\n",
        "top_n = 50\n",
        "ngram_range = (1,1)\n",
        "uni_grams = get_top_n_ngrams(clean_data.Final_Text, top_n, ngram_range)\n",
        "\n",
        "df = pd.DataFrame(uni_grams, columns = ['Final_Text' , 'count'])\n",
        "df.groupby('Final_Text').sum()['count'].sort_values(ascending=False).iplot(\n",
        "    kind='bar', \n",
        "    yTitle='Count', \n",
        "    linecolor='black', \n",
        "    colorscale='piyg',\n",
        "    title=f'Top {top_n} Unigrams in Final_Text')\n",
        "\n",
        "# Top 50 Unigrams after removing stop words\n",
        "uni_grams_sw = get_top_n_ngrams(clean_data.Final_Text, top_n, ngram_range, stopwords=STOP_WORDS)\n",
        "\n",
        "df = pd.DataFrame(uni_grams_sw, columns = ['Final_Text' , 'count'])\n",
        "df.groupby('Final_Text').sum()['count'].sort_values(ascending=False).iplot(\n",
        "    kind='bar', \n",
        "    yTitle='Count', \n",
        "    linecolor='black',\n",
        "    colorscale='-piyg',\n",
        "    title=f'Top {top_n} Unigrams in Final_Text without stop words')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jb8iWkH-aoai"
      },
      "source": [
        "### Top Bigrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Qrr4qJNaoai"
      },
      "source": [
        "# Top 50 Bigrams before removing stop words\n",
        "top_n = 50\n",
        "ngram_range = (2,2)\n",
        "bi_grams = get_top_n_ngrams(clean_data.Final_Text, top_n, ngram_range)\n",
        "\n",
        "df = pd.DataFrame(bi_grams, columns = ['Final_Text' , 'count'])\n",
        "df.groupby('Final_Text').sum()['count'].sort_values(ascending=False).iplot(\n",
        "    kind='bar', \n",
        "    yTitle='Count', \n",
        "    linecolor='black', \n",
        "    colorscale='piyg',\n",
        "    title=f'Top {top_n} Bigrams in Final_Text')\n",
        "\n",
        "# Top 50 Bigrams after removing stop words\n",
        "bi_grams_sw = get_top_n_ngrams(clean_data.Final_Text, top_n, ngram_range, stopwords=STOP_WORDS)\n",
        "\n",
        "df = pd.DataFrame(bi_grams_sw, columns = ['Final_Text' , 'count'])\n",
        "df.groupby('Final_Text').sum()['count'].sort_values(ascending=False).iplot(\n",
        "    kind='bar', \n",
        "    yTitle='Count', \n",
        "    linecolor='black',\n",
        "    colorscale='-piyg',\n",
        "    title=f'Top {top_n} Bigrams in Final_Text without stop words')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xc62MwL5aoai"
      },
      "source": [
        "### Top Trigrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LE3n2E_zaoai"
      },
      "source": [
        "# Top 50 Trigrams before removing stop words\n",
        "top_n = 50\n",
        "ngram_range = (3,3)\n",
        "tri_grams = get_top_n_ngrams(clean_data.Final_Text, top_n, ngram_range)\n",
        "\n",
        "df = pd.DataFrame(tri_grams, columns = ['Final_Text' , 'count'])\n",
        "df.groupby('Final_Text').sum()['count'].sort_values(ascending=False).iplot(\n",
        "    kind='bar', \n",
        "    yTitle='Count', \n",
        "    linecolor='black', \n",
        "    colorscale='piyg',\n",
        "    title=f'Top {top_n} Trigrams in Final_Text')\n",
        "\n",
        "# Top 50 Trigrams after removing stop words\n",
        "tri_grams_sw = get_top_n_ngrams(clean_data.Final_Text, top_n, ngram_range, stopwords=STOP_WORDS)\n",
        "\n",
        "df = pd.DataFrame(tri_grams_sw, columns = ['Final_Text' , 'count'])\n",
        "df.groupby('Final_Text').sum()['count'].sort_values(ascending=False).iplot(\n",
        "    kind='bar', \n",
        "    yTitle='Count', \n",
        "    linecolor='black',\n",
        "    colorscale='-piyg',\n",
        "    title=f'Top {top_n} Trigrams in Final_Text without stop words')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4hSbsitaoai"
      },
      "source": [
        "### Word Cloud\n",
        "Let us attempt to visualize this as a word cloud for top three groups that has got maximum records. A word cloud enables us to visualize the data as cluster of words and each words displayed in different font size based on the number of occurences of that word . Basically; the bolder and bigger the word show up in the visualization, it implies its more often it’s mentioned within a given text compared to other words in the cloud and therefore would be more important for us.\n",
        "\n",
        "Let's write a generic method to generate Word Clouds for both Short and Long Description columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoPIxAzmEWHI"
      },
      "source": [
        "# replace any single word character with a word boundary\n",
        "#clean_data.Final_Text.str.replace(r'\\b\\w\\b','').str.replace(r'\\s+', ' ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKttDbCQaoai"
      },
      "source": [
        "def generate_word_cloud(corpus):\n",
        "        # Instantiate the wordcloud object\n",
        "    wordcloud = WordCloud(width = 800, height = 800, \n",
        "                    background_color ='white', \n",
        "                    stopwords=STOP_WORDS,\n",
        "                    # mask=mask,\n",
        "                    min_font_size = 10).generate(corpus)\n",
        "\n",
        "    # plot the WordCloud image                        \n",
        "    plt.figure(figsize = (12, 12), facecolor = None) \n",
        "    plt.imshow(wordcloud) \n",
        "    plt.axis(\"off\") \n",
        "    plt.tight_layout(pad = 0) \n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKpk_Grwaoai"
      },
      "source": [
        "# Word Cloud for all tickets assigned to GRP_0\n",
        "generate_word_cloud(' '.join(clean_data[clean_data['Assignment group'] == 'GRP_0'].Final_Text.str.strip()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJkOwVhbaoai"
      },
      "source": [
        "# Word Cloud for all tickets assigned to GRP_8\n",
        "generate_word_cloud(' '.join(clean_data[clean_data['Assignment group'] == 'GRP_8'].Final_Text.str.strip()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLQhXr6Yaoai"
      },
      "source": [
        "# Word Cloud for all tickets assigned to GRP_25\n",
        "generate_word_cloud(' '.join(clean_data[clean_data['Assignment group'] == 'GRP_25'].Final_Text.str.strip()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IqWo_cPaoai",
        "scrolled": true
      },
      "source": [
        "# Generate wordcloud for Final_Text field\n",
        "generate_word_cloud(' '.join(clean_data.Final_Text.str.strip()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhCUgD6_Bsjn"
      },
      "source": [
        "## Prepping Dataframe for Model Building"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tb8URlQ4Z85A"
      },
      "source": [
        "'''# Create a target categorical column\r\n",
        "clean_data['Assignment group OneHotEncoded'] = clean_data['Assignment group'].astype('category').cat.codes\r\n",
        "clean_data.info()'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FW9b_WeCBsjn"
      },
      "source": [
        "'''# Import OneHot encoder \n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn import preprocessing \n",
        "clean_data['Assignment group OneHotEncoded'] = np.nan\n",
        "# OneHot_encoder object knows how to understand word labels. \n",
        "#onehot_encoder = preprocessing.OneHotEncoder() #categories=62\n",
        "onehot_encoder = LabelBinarizer()\n",
        "onehot_encoder.fit(clean_data['Assignment group'])\n",
        "# Encode labels in column\n",
        "#transformed = onehot_encoder.fit_transform(clean_data['Assignment group'])\n",
        "#temp_df = pd.DataFrame(transformed, columns=onehot_encoder.get_feature_names())\n",
        "transformed = onehot_encoder.transform(clean_data['Assignment group'])\n",
        "temp_df = pd.DataFrame(transformed)\n",
        "clean_data = pd.concat([clean_data, temp_df], axis=1)\n",
        "#clean_data\n",
        "#clean_data['Assignment group OneHotEncoded'].unique()\n",
        "clean_data'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRguKfOkEWHJ"
      },
      "source": [
        "# Import label encoder \n",
        "from sklearn import preprocessing \n",
        "  \n",
        "# label_encoder object knows how to understand word labels. \n",
        "label_encoder = preprocessing.LabelEncoder() \n",
        "  \n",
        "# Encode labels in column 'species'. \n",
        "clean_data['Assignment group LabelEncoded']= label_encoder.fit_transform(clean_data['Assignment group']) \n",
        "  \n",
        "clean_data['Assignment group LabelEncoded'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jsBDjGIBsjn"
      },
      "source": [
        "label_encoded_dict = dict(zip(clean_data['Assignment group'].unique(), clean_data['Assignment group LabelEncoded'].unique()))\n",
        "len(label_encoded_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ha-VhcWRBsjn"
      },
      "source": [
        "## Feature Extraction : Bag of Words using CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3UGzor_Bsjo"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "CV = CountVectorizer(max_features=2000)\n",
        "\n",
        "X_BoW = CV.fit_transform(clean_data['Final_Text']).toarray()\n",
        "y = clean_data['Assignment group LabelEncoded']\n",
        "\n",
        "print(\"Shape of Input Feature :\",np.shape(X_BoW))\n",
        "print(\"Shape of Target Feature :\",np.shape(y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaaV3zyqBsjo"
      },
      "source": [
        "# Splitting Train Test \n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_BoW, y, test_size=0.3, random_state = 0, stratify=y)\n",
        "print('\\033[1mShape of the training set:\\033[0m', X_train.shape, X_test.shape)\n",
        "print('\\033[1mShape of the test set:\\033[0m', y_train.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIni9BjpBsjo"
      },
      "source": [
        "\n",
        "def run_classification(estimator, X_train, X_test, y_train, y_test, arch_name=None, pipelineRequired=True, isDeepModel=False):\n",
        "    # train the model\n",
        "    clf = estimator\n",
        "\n",
        "    if pipelineRequired :\n",
        "        clf = Pipeline([('tfidf', TfidfTransformer()),\n",
        "                     ('clf', estimator),\n",
        "                     ])\n",
        "      \n",
        "    if isDeepModel :\n",
        "        clf.fit(X_train, y_train, validation_data=(X_test, y_test),epochs=20, batch_size=128,verbose=1,callbacks=call_backs(arch_name))\n",
        "        # predict from the clasiffier\n",
        "        y_pred = clf.predict(X_test)\n",
        "        y_pred = np.argmax(y_pred, axis=1)\n",
        "        y_train_pred = clf.predict(X_train)\n",
        "        y_train_pred = np.argmax(y_train_pred, axis=1)\n",
        "    else :\n",
        "        clf.fit(X_train, y_train)\n",
        "        # predict from the clasiffier\n",
        "        y_pred = clf.predict(X_test)\n",
        "        y_train_pred = clf.predict(X_train)\n",
        "    \n",
        "    print('Estimator:', clf)\n",
        "    print('='*80)\n",
        "    print('Training accuracy: %.2f%%' % (accuracy_score(y_train,y_train_pred) * 100))\n",
        "    print('Testing accuracy: %.2f%%' % (accuracy_score(y_test, y_pred) * 100))\n",
        "    print('='*80)\n",
        "    print('Confusion matrix:\\n %s' % (confusion_matrix(y_test, y_pred)))\n",
        "    print('='*80)\n",
        "    print('Classification report:\\n %s' % (classification_report(y_test, y_pred)))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxlH4wqyBsjo"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_RXTWZfBsjp"
      },
      "source": [
        "run_classification(LogisticRegression(), X_train, X_test, y_train, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7r6p2SGBsjp"
      },
      "source": [
        "## Naive Bayes Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hlgbYXyBsjp"
      },
      "source": [
        "run_classification(MultinomialNB(), X_train, X_test, y_train, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gs5ztA2kBsjp"
      },
      "source": [
        "## K-nearest Neighbor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4qwMKt7Bsjq"
      },
      "source": [
        "run_classification(KNeighborsClassifier(), X_train, X_test, y_train, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDMha2lxBsjq"
      },
      "source": [
        "## Support Vector Machine (SVM)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBc7yY11Bsjq"
      },
      "source": [
        "run_classification(LinearSVC(), X_train, X_test, y_train, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDPisp5SBsjq"
      },
      "source": [
        "## Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyvv2qn5Bsjq"
      },
      "source": [
        "run_classification(DecisionTreeClassifier(), X_train, X_test, y_train, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpJIipnsBsjr"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8b28KjuoBsjr"
      },
      "source": [
        "run_classification(RandomForestClassifier(n_estimators=100), X_train, X_test, y_train, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJr4CJMBBsjr"
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "run_classification(GradientBoostingClassifier(n_estimators=100), X_train, X_test, y_train, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVNcUZ1BQWrL"
      },
      "source": [
        "## Deep Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RelfajlQWrL"
      },
      "source": [
        "# Load the augmented data from pickle file \n",
        "with open('/content/Interim_data.pkl','rb') as f:\n",
        "    clean_data_DL = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15XBDLMRQWrL"
      },
      "source": [
        "clean_data_DL.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxgUQxvhQWrL"
      },
      "source": [
        "clean_data_DL['Final_Text'] = clean_data_DL['Final_Text'].replace(np.nan, '', regex=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzJ4VnECQWrM"
      },
      "source": [
        "clean_data_DL.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTiSKPsIr7RX"
      },
      "source": [
        "# Import OneHot encoder \n",
        "from sklearn import preprocessing \n",
        "  \n",
        "# onehot_encoder object knows how to understand word labels. \n",
        "onehot_encoder = preprocessing.OneHotEncoder() \n",
        "  \n",
        "# Encode labels in column 'species'. \n",
        "clean_data_DL['Assignment group OneHotEncoded']= onehot_encoder.fit_transform(clean_data_DL['Assignment group']) \n",
        "  \n",
        "clean_data_DL['Assignment group OneHotEncoded'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwiIGk4ar7gA"
      },
      "source": [
        "onehot_encoded_dict = dict(zip(clean_data_DL['Assignment group'].unique(), clean_data_DL['Assignment group LabelEncoded'].unique()))\n",
        "len(onehot_encoded_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibiTTlGLQWrM"
      },
      "source": [
        "# Splitting Train Test \n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(clean_data_DL['Final_Text'], clean_data_DL['Assignment group OneHotEncoded'], test_size=0.3, random_state = 0, stratify=clean_data_DL['Assignment group OneHotEncoded'])\n",
        "print('\\033[1mShape of the training set:\\033[0m', X_train.shape, X_test.shape)\n",
        "print('\\033[1mShape of the test set:\\033[0m', y_train.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GOVYYV9QWrM"
      },
      "source": [
        "### Create checkpoints function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RG6wVQXQWrN"
      },
      "source": [
        "#Path where you want to save the weights, model and checkpoints\n",
        "model_path = \"Weights/\"\n",
        "%mkdir Weights\n",
        "\n",
        "# Define model callbacks\n",
        "def call_backs(name):\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.005, patience=100)\n",
        "    model_checkpoint =  ModelCheckpoint(model_path + name + '_epoch{epoch:02d}_loss{val_loss:.4f}.h5',\n",
        "                                                               monitor='val_loss',\n",
        "                                                               verbose=1,\n",
        "                                                               save_best_only=True,\n",
        "                                                               save_weights_only=False,\n",
        "                                                               mode='min',\n",
        "                                                               period=1)\n",
        "    return [model_checkpoint, early_stopping]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAC5r5UcQWrN"
      },
      "source": [
        "# Function to build Neural Network\n",
        "def Build_Model_DNN_Text(shape, nClasses, dropout=0.3):\n",
        "    \"\"\"\n",
        "    buildModel_DNN_Tex(shape, nClasses,dropout)\n",
        "    Build Deep neural networks Model for text classification\n",
        "    Shape is input feature space\n",
        "    nClasses is number of classes\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "    node = 512 # number of nodes\n",
        "    nLayers = 4 # number of  hidden layer\n",
        "    model.add(Dense(node,input_dim=shape,activation='relu'))\n",
        "    model.add(Dropout(dropout))\n",
        "    for i in range(0,nLayers):\n",
        "        model.add(Dense(node,input_dim=node,activation='relu'))\n",
        "        model.add(Dropout(dropout))\n",
        "        model.add(BatchNormalization())\n",
        "    model.add(Dense(nClasses, activation='softmax'))\n",
        "    model.compile(loss='sparse_categorical_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "    print(model.summary())\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdB5_mvzQWrN"
      },
      "source": [
        "Tfidf_vect = TfidfVectorizer(max_features=2000)\n",
        "Tfidf_vect.fit(clean_data_DL.Final_Text.astype(str))\n",
        "X_train_tfidf = Tfidf_vect.transform(X_train)\n",
        "X_test_tfidf = Tfidf_vect.transform(X_test)\n",
        "\n",
        "# Instantiate the network\n",
        "model_DNN = Build_Model_DNN_Text(X_train_tfidf.shape[1], 62)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6aC4UqOQWrN"
      },
      "source": [
        "run_classification(model_DNN, X_train_tfidf, X_test_tfidf, y_train, y_test,pipelineRequired = False,isDeepModel=True, arch_name='DNN')\n",
        "\n",
        "'''model_DNN.fit(X_train_tfidf, y_train,\n",
        "                              validation_data=(X_test_tfidf, y_test),\n",
        "                              callbacks=call_backs(\"NN\"),\n",
        "                              epochs=10,\n",
        "                              batch_size=128,\n",
        "                              verbose=2)\n",
        "predicted = model_DNN.predict(X_test_tfidf)'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaCQnFBRQWrO"
      },
      "source": [
        "### Extract Glove Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMA3MFYUQWrO"
      },
      "source": [
        "#download the glove embedding zip file from http://nlp.stanford.edu/data/wordvecs/glove.6B.zip\n",
        "from zipfile import ZipFile\n",
        "# Check if it is already extracted else Open the zipped file as readonly\n",
        "if not os.path.isfile('glove.6B/glove.6B.200d.txt'):\n",
        "    #glove_embeddings = 'glove.6B.zip'\n",
        "    glove_embeddings = '/content/drive/MyDrive/Capstone/glove.6B.zip'\n",
        "    with ZipFile(glove_embeddings, 'r') as archive:\n",
        "        archive.extractall('glove.6B')\n",
        "\n",
        "# List the files under extracted folder\n",
        "os.listdir('glove.6B')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2Xtecr3QWrO"
      },
      "source": [
        "## Convolutional Neural Networks (CNN)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghIAygEfQWrO"
      },
      "source": [
        "#gloveFileName = 'glove.6B/glove.6B.200d.txt'\n",
        "gloveFileName = '/content/glove.6B/glove.6B.200d.txt'\n",
        "MAX_SEQUENCE_LENGTH = 500\n",
        "EMBEDDING_DIM=200\n",
        "MAX_NB_WORDS=75000\n",
        "\n",
        "# Function to generate Embedding\n",
        "def loadData_Tokenizer(X_train, X_test,filename):\n",
        "    np.random.seed(7)\n",
        "    text = np.concatenate((X_train, X_test), axis=0)\n",
        "    text = np.array(text)\n",
        "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
        "    tokenizer.fit_on_texts(text)\n",
        "    sequences = tokenizer.texts_to_sequences(text)\n",
        "    word_index = tokenizer.word_index\n",
        "    text = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "    print('Found %s unique tokens.' % len(word_index))\n",
        "    indices = np.arange(text.shape[0])\n",
        "    # np.random.shuffle(indices)\n",
        "    text = text[indices]\n",
        "    print(text.shape)\n",
        "    X_train = text[0:len(X_train), ]\n",
        "    X_test = text[len(X_train):, ]\n",
        "    embeddings_index = {}\n",
        "    f = open(filename, encoding=\"utf8\")\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        try:\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "        except:\n",
        "            pass\n",
        "        embeddings_index[word] = coefs\n",
        "    f.close()\n",
        "    print('Total %s word vectors.' % len(embeddings_index))\n",
        "    return (X_train, X_test, word_index,embeddings_index)\n",
        "\n",
        "\n",
        "embedding_matrix = []\n",
        "\n",
        "def buildEmbed_matrices(word_index,embedding_dim):\n",
        "    embedding_matrix = np.random.random((len(word_index) + 1, embedding_dim))\n",
        "    for word, i in word_index.items():\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            # words not found in embedding index will be all-zeros.\n",
        "            if len(embedding_matrix[i]) !=len(embedding_vector):\n",
        "                print(\"could not broadcast input array from shape\",str(len(embedding_matrix[i])), \"into shape\",str(len(embedding_vector)),\n",
        "                      \" Please make sure your\"\" EMBEDDING_DIM is equal to embedding_vector file ,GloVe,\")\n",
        "                exit(1)\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "    return embedding_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DljFFfKxQWrO"
      },
      "source": [
        "# Generate Glove embedded datasets\n",
        "X_train_Glove, X_test_Glove, word_index, embeddings_index = loadData_Tokenizer(X_train,X_test,gloveFileName)\n",
        "embedding_matrix = buildEmbed_matrices(word_index,EMBEDDING_DIM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URiIMsVEQWrO"
      },
      "source": [
        "def Build_Model_CNN_Text(word_index, embeddings_matrix, nclasses,dropout=0.5):\n",
        "    \"\"\"\n",
        "        def buildModel_CNN(word_index, embeddings_index, nclasses, MAX_SEQUENCE_LENGTH=500, EMBEDDING_DIM=50, dropout=0.5):\n",
        "        word_index in word index ,\n",
        "        embeddings_index is embeddings index, look at data_helper.py\n",
        "        nClasses is number of classes,\n",
        "        MAX_SEQUENCE_LENGTH is maximum lenght of text sequences,\n",
        "        EMBEDDING_DIM is an int value for dimention of word embedding look at data_helper.py\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "    embedding_layer = Embedding(len(word_index) + 1,\n",
        "                                EMBEDDING_DIM,\n",
        "                                weights=[embeddings_matrix],\n",
        "                                input_length=MAX_SEQUENCE_LENGTH,\n",
        "                                trainable=True)\n",
        "    # applying a more complex convolutional approach\n",
        "    convs = []\n",
        "    filter_sizes = []\n",
        "    layer = 5\n",
        "    print(\"Filter  \",layer)\n",
        "    for fl in range(0,layer):\n",
        "        filter_sizes.append((fl+2))\n",
        "    node = 128\n",
        "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "    embedded_sequences = embedding_layer(sequence_input)\n",
        "    for fsz in filter_sizes:\n",
        "        l_conv = Conv1D(node, kernel_size=fsz, activation='relu')(embedded_sequences)\n",
        "        l_pool = MaxPooling1D(5)(l_conv)\n",
        "        #l_pool = Dropout(0.25)(l_pool)\n",
        "        convs.append(l_pool)\n",
        "    l_merge = Concatenate(axis=1)(convs)\n",
        "    l_cov1 = Conv1D(node, 5, activation='relu')(l_merge)\n",
        "    l_cov1 = Dropout(dropout)(l_cov1)\n",
        "    l_batch1 = BatchNormalization()(l_cov1)\n",
        "    l_pool1 = MaxPooling1D(5)(l_batch1)\n",
        "    l_cov2 = Conv1D(node, 5, activation='relu')(l_pool1)\n",
        "    l_cov2 = Dropout(dropout)(l_cov2)\n",
        "    l_batch2 = BatchNormalization()(l_cov2)\n",
        "    l_pool2 = MaxPooling1D(30)(l_batch2)\n",
        "    l_flat = Flatten()(l_pool2)\n",
        "    l_dense = Dense(1024, activation='relu')(l_flat)\n",
        "    l_dense = Dropout(dropout)(l_dense)\n",
        "    l_dense = Dense(512, activation='relu')(l_dense)\n",
        "    l_dense = Dropout(dropout)(l_dense)\n",
        "    preds = Dense(nclasses, activation='softmax')(l_dense)\n",
        "    model = Model(sequence_input, preds)\n",
        "    model.compile(loss='sparse_categorical_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    print(model.summary())\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxdeJLw9QWrP"
      },
      "source": [
        "# Train the network and run classification\n",
        "model_CNN = Build_Model_CNN_Text(word_index,embedding_matrix, 62)\n",
        "run_classification(model_CNN, X_train_Glove, X_test_Glove, y_train, y_test,pipelineRequired = False,isDeepModel=True, arch_name='CNN')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMu2ONzLQWrP"
      },
      "source": [
        "## Recurrent Neural Networks (RNN) --> Gated Recurrent Unit (GRU)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grFaHdizQWrP"
      },
      "source": [
        "def Build_Model_RNN_Text(word_index, embeddings_matrix, nclasses,dropout=0.5):\n",
        "    \"\"\"\n",
        "    def buildModel_RNN(word_index, embeddings_matrix, nclasses,  MAX_SEQUENCE_LENGTH=500, EMBEDDING_DIM=100, dropout=0.5):\n",
        "    word_index in word index ,\n",
        "    embeddings_matrix is embeddings_matrix, look at data_helper.py\n",
        "    nClasses is number of classes,\n",
        "    MAX_SEQUENCE_LENGTH is maximum lenght of text sequences\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "    hidden_layer = 3\n",
        "    gru_node = 32\n",
        "    \n",
        "    model.add(Embedding(len(word_index) + 1,\n",
        "                                EMBEDDING_DIM,\n",
        "                                weights=[embeddings_matrix],\n",
        "                                input_length=MAX_SEQUENCE_LENGTH,\n",
        "                                trainable=True))\n",
        "    print(gru_node)\n",
        "    for i in range(0,hidden_layer):\n",
        "        model.add(GRU(gru_node,return_sequences=True, recurrent_dropout=0.2))\n",
        "        model.add(Dropout(dropout))\n",
        "        model.add(BatchNormalization())\n",
        "    model.add(GRU(gru_node, recurrent_dropout=0.2))\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dense(nclasses, activation='softmax'))\n",
        "    model.compile(loss='sparse_categorical_crossentropy',\n",
        "                      optimizer='sgd',\n",
        "                      metrics=['accuracy'])\n",
        "    \n",
        "    print(model.summary())\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlNeKO-3QWrP"
      },
      "source": [
        "# Train the network and run classification\n",
        "model_RNN = Build_Model_RNN_Text(word_index,embedding_matrix, 62)\n",
        "run_classification(model_RNN, X_train_Glove, X_test_Glove, y_train, y_test,pipelineRequired = False,isDeepModel=True, arch_name='RNN')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVAZo1u8QWrP"
      },
      "source": [
        "## RNN with LSTM networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtFjAJ4oQWrQ"
      },
      "source": [
        "EMBEDDING_DIM = 200\n",
        "#gloveFileName = 'glove.6B/glove.6B.100d.txt'\n",
        "gloveFileName = '/content/glove.6B/glove.6B.200d.txt'\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, TimeDistributed, Activation\n",
        "from keras.layers import Flatten, Permute, merge, Input\n",
        "from keras.layers import Embedding\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, multiply, concatenate, Dropout\n",
        "from keras.layers import GRU, Bidirectional\n",
        "\n",
        "\n",
        "def Build_Model_LTSM_Text(word_index, embeddings_matrix, nclasses):\n",
        "    kernel_size = 2\n",
        "    filters = 256\n",
        "    pool_size = 2\n",
        "    gru_node = 256\n",
        "    \n",
        "    model = Sequential()\n",
        "    model.add(Embedding(len(word_index) + 1,\n",
        "                                EMBEDDING_DIM,\n",
        "                                weights=[embeddings_matrix],\n",
        "                                input_length=MAX_SEQUENCE_LENGTH,\n",
        "                                trainable=True))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Conv1D(filters, kernel_size, activation='relu'))\n",
        "    model.add(MaxPooling1D(pool_size=pool_size))\n",
        "    model.add(Conv1D(filters, kernel_size, activation='relu'))\n",
        "    model.add(MaxPooling1D(pool_size=pool_size))\n",
        "    model.add(Conv1D(filters, kernel_size, activation='relu'))\n",
        "    model.add(MaxPooling1D(pool_size=pool_size))\n",
        "    model.add(Conv1D(filters, kernel_size, activation='relu'))\n",
        "    model.add(MaxPooling1D(pool_size=pool_size))\n",
        "    model.add(Bidirectional(LSTM(gru_node, return_sequences=True, recurrent_dropout=0.2)))\n",
        "    model.add(Bidirectional(LSTM(gru_node, return_sequences=True, recurrent_dropout=0.2)))\n",
        "    model.add(Bidirectional(LSTM(gru_node, return_sequences=True, recurrent_dropout=0.2)))\n",
        "    model.add(Bidirectional(LSTM(gru_node, recurrent_dropout=0.2)))\n",
        "    model.add(Dense(1024,activation='relu'))\n",
        "    model.add(Dense(nclasses))\n",
        "    model.add(Activation('softmax'))\n",
        "    model.compile(loss='sparse_categorical_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    print(model.summary())\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0973kvsQWrQ"
      },
      "source": [
        "X_train_Glove,X_test_Glove, word_index,embeddings_index = loadData_Tokenizer(X_train,X_test,gloveFileName)\n",
        "embedding_matrix = buildEmbed_matrices(word_index,EMBEDDING_DIM)\n",
        "\n",
        "model_LTSM = Build_Model_LTSM_Text(word_index,embedding_matrix, 62)\n",
        "run_classification(model_LTSM, X_train_Glove, X_test_Glove, y_train, y_test,pipelineRequired = False,isDeepModel=True, arch_name='LSTM')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}